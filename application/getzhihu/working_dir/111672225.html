<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 110】Reward-Free Exploration - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning),深度学习（Deep Learning）,机器学习"/><meta data-react-helmet="true" name="description" content="这篇文章提出了一个比较新的设定，先在状态空间探索，然后求解 MDP 问题。文章给出了在这种设定下的 provably efficient 算法和相应的 lower bound。原文传送门Jin, Chi, et al. &amp;#34;Reward-Free Exploration for…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 110】Reward-Free Exploration"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/111672225"/><meta data-react-helmet="true" property="og:description" content="这篇文章提出了一个比较新的设定，先在状态空间探索，然后求解 MDP 问题。文章给出了在这种设定下的 provably efficient 算法和相应的 lower bound。原文传送门Jin, Chi, et al. &amp;#34;Reward-Free Exploration for…"/><meta data-react-helmet="true" property="og:image" content="https://pic2.zhimg.com/v2-1383ee23dba94f9a378c6115294d0d57_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:111672225,&quot;title&quot;:&quot;【强化学习 110】Reward-Free Exploration&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic2.zhimg.com/v2-1383ee23dba94f9a378c6115294d0d57_1200x500.jpg" alt="【强化学习 110】Reward-Free Exploration"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 110】Reward-Free Exploration</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">39 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>这篇文章提出了一个比较新的设定，先在状态空间探索，然后求解 MDP 问题。文章给出了在这种设定下的 provably efficient 算法和相应的 lower bound。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.02794.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Jin, Chi, et al. &#34;Reward-Free Exploration for Reinforcement Learning.&#34; arXiv preprint arXiv:2002.02794 (2020).</a></p><h2>特色</h2><p>从 model-based 的角度来看，强化学习问题中最困难的不在于估计 reward，而在于估计 transition function。并且，如果想要以较高的精度估计到 transition matrix 中的每一个格子的数值（e.g., in terms of total variation）是几乎不可能的。从 model-free 的角度来看，这个问题就是要找一个好的具有探索性的策略，而这里的探索性指的是在状态空间（或者状态-行动空间）上的探索。</p><p>这篇文章提出了解决强化学习问题的一个范式：先进行探索（exploration phase），再进行规划求解（planning phase）。在探索的阶段不接受奖励信息，只是在状态空间上纯探索，以此得到一个探索性的策略，并且执行该策略得到一个数据集 D。在规划阶段，对于任意一个给定的奖励函数 r，利用数据集 D 估计出来的 transition function，应用标准的强化学习方法求解到好的策略。本文结合一些之前的强化学习算法的理论保证，给出了一个算法使得该算法在这个设定上 probably efficient。</p><h2>过程</h2><h3>1、背景和相关文献</h3><p>首先，这种强化学习的范式在某些方面具有显著的优势：通过探索得到这样一个数据集 D 之后，我们就可以在不对 MDP 进行任何的采样的情况下，针对任意的奖励函数都找到接近最优的策略。这在某些情况下特别有用，比如，我们有一个期望的智能体的 behavior，但是需要设计一个奖励函数使得智能体达到这样的 behavior。（还可以参考 meta learning 等设定）</p><p>其次，在技术上，该问题的实质是强化学习里面最为困难的一个问题：对于状态空间的探索问题。而且在该范式下，直接分离出来该问题，在 exploration phase 中解决。</p><p>在相关的文献上：RMax [Brafman and Tennenholtz, 2002] 可以被改造成 reward-free 的形式，但是样本效率比较低；其他针对特定 reward 的算法得到的 PAC 算法，肯定不一定对于任意的 reward 最优；有一些适用 function approximation 的探索方法 [Du et al 2019, Misra et al 2019]，这两篇我还不太熟；还有一篇比较类似的，[Hazan et al 2019]（ICML 19&#39;），也是一个 reward-free 的常见（不太记得专栏有没有写了），证明了他们的算法能够达到一个 max-entropy 的策略（即，在状态空间上是 max-entropy），但是没有说明探索得到的“成果”如何转化为相对于任意奖励函数的最优策略。当时读这一篇的时候就觉得肯定得填上这一环，但是主要的技术原因在于 max-entropy  并不保证“每个”状态都被较好地访问到；而这篇文章观察到，有些实在怎么着都访问不太到的状态其实也不是很影响最后的 value function，因此把不去过多地访问它们也没事。</p><h3>2、Reward-free setting</h3><p>文章提出的这个范式，在第一个探索阶段只做 reward-free 的探索，这个交互和标准的 RL 交互的区别就在于环境不返回奖励。相比于标准 RL，其他方面都一样，比如都具有一个固定的初始状态分布，并且要从该分布出发根据 transition dynamics 来访问各个状态。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-7cb351307a603627f48630be6ab32a84_b.jpg" data-caption="" data-size="normal" data-rawwidth="1224" data-rawheight="327" class="origin_image zh-lightbox-thumb" width="1224" data-original="https://pic1.zhimg.com/v2-7cb351307a603627f48630be6ab32a84_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1224&#39; height=&#39;327&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1224" data-rawheight="327" class="origin_image zh-lightbox-thumb lazy" width="1224" data-original="https://pic1.zhimg.com/v2-7cb351307a603627f48630be6ab32a84_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-7cb351307a603627f48630be6ab32a84_b.jpg"/></figure><h3>3、Overview</h3><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-2887dfcbf3130a957473ebb130133ba7_b.jpg" data-caption="" data-size="normal" data-rawwidth="1171" data-rawheight="310" class="origin_image zh-lightbox-thumb" width="1171" data-original="https://pic4.zhimg.com/v2-2887dfcbf3130a957473ebb130133ba7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1171&#39; height=&#39;310&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1171" data-rawheight="310" class="origin_image zh-lightbox-thumb lazy" width="1171" data-original="https://pic4.zhimg.com/v2-2887dfcbf3130a957473ebb130133ba7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-2887dfcbf3130a957473ebb130133ba7_b.jpg"/></figure><p>其中，第 1、2 步就是我们前面讲到的 exploration phase；而 3、4 步就是 planning phase。</p><p>最后文章证明了该算法只需要在 exploration phase 做这么多个轨迹的探索，就能在 planning phase 对于任意的奖励函数都找到 epsilon-optimal policy。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-1900ecc67dac98f107d6231e44c0605e_b.jpg" data-caption="" data-size="normal" data-rawwidth="1140" data-rawheight="257" class="origin_image zh-lightbox-thumb" width="1140" data-original="https://pic3.zhimg.com/v2-1900ecc67dac98f107d6231e44c0605e_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1140&#39; height=&#39;257&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1140" data-rawheight="257" class="origin_image zh-lightbox-thumb lazy" width="1140" data-original="https://pic3.zhimg.com/v2-1900ecc67dac98f107d6231e44c0605e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-1900ecc67dac98f107d6231e44c0605e_b.jpg"/></figure><h3>4、Exploration phase</h3><p>这个阶段的通过智能体和环境的 reward-free 交互，找到一个具有探索性的策略，然后输出一个使用该策略得到的采样数据集。算法如下图所示。其中 3-7 行是在学习得到一个探索性的策略集合，具体的方式是对于每一个状态，都构造一个只在该状态上有奖励的奖励函数，然后使用已有的 RL 算法来得到能够把智能体带到该状态的策略集合。（注意到带到第 h 层的状态 s 之后，再从该状态出发，选择什么 action 都无所谓了，因此第 6 行把 Euler 得到的策略稍微又改了一下）接下来第 8-11 行则执行执行该策略族，得到一个数据集 D。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-623f9853b4048891eff9ce80345f48d9_b.jpg" data-caption="" data-size="normal" data-rawwidth="1136" data-rawheight="461" class="origin_image zh-lightbox-thumb" width="1136" data-original="https://pic2.zhimg.com/v2-623f9853b4048891eff9ce80345f48d9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1136&#39; height=&#39;461&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1136" data-rawheight="461" class="origin_image zh-lightbox-thumb lazy" width="1136" data-original="https://pic2.zhimg.com/v2-623f9853b4048891eff9ce80345f48d9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-623f9853b4048891eff9ce80345f48d9_b.jpg"/></figure><p>我们前面提到，对于有些实在访问不到的状态（无论什么策略都访问不到），其实可以直接把它们放弃掉。因为，即使这些状态上的奖励函数很高，但是我们怎么控制也达不到这些状态，因此最后的最优策略也不可能特别地去访问这些状态。我们记这部分状态为 insignificant，具体定义如下：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-3a47ce7e00c31fc8b5ece408cd89349e_b.png" data-caption="" data-size="normal" data-rawwidth="1142" data-rawheight="152" class="origin_image zh-lightbox-thumb" width="1142" data-original="https://pic3.zhimg.com/v2-3a47ce7e00c31fc8b5ece408cd89349e_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1142&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1142" data-rawheight="152" class="origin_image zh-lightbox-thumb lazy" width="1142" data-original="https://pic3.zhimg.com/v2-3a47ce7e00c31fc8b5ece408cd89349e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-3a47ce7e00c31fc8b5ece408cd89349e_b.png"/></figure><p>文章证明思路的关键就在于，通过该 exploration 得到的数据集对于任意 significant 的状态，都能够以比较大的概率访问到。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-6590afbd5d0348de09b3989d397e7bc0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1151" data-rawheight="200" class="origin_image zh-lightbox-thumb" width="1151" data-original="https://pic1.zhimg.com/v2-6590afbd5d0348de09b3989d397e7bc0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1151&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1151" data-rawheight="200" class="origin_image zh-lightbox-thumb lazy" width="1151" data-original="https://pic1.zhimg.com/v2-6590afbd5d0348de09b3989d397e7bc0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-6590afbd5d0348de09b3989d397e7bc0_b.jpg"/></figure><p>文章之所以要用 Euler 是因为该算法得到的策略集的性能和最优价值函数的大小有关，即，如果这个最优价值函数本身就不特别好（比如，即使最优策略也访问不到那些 reward 比较大但是 insignificant 的状态），那么得到的策略和最优价值函数的差距也不会特别大。这一点对于本文的证明比较关键。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-f771f1d27238d65167dd191ab42492ff_b.jpg" data-caption="" data-size="normal" data-rawwidth="1136" data-rawheight="301" class="origin_image zh-lightbox-thumb" width="1136" data-original="https://pic4.zhimg.com/v2-f771f1d27238d65167dd191ab42492ff_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1136&#39; height=&#39;301&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1136" data-rawheight="301" class="origin_image zh-lightbox-thumb lazy" width="1136" data-original="https://pic4.zhimg.com/v2-f771f1d27238d65167dd191ab42492ff_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-f771f1d27238d65167dd191ab42492ff_b.jpg"/></figure><h3>5、Planning phase</h3><p>规划阶段的做法就比较就简单了，就是从数据集里面估计 transition matrix 然后使用该 transition matrix 来求解。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-346876f631718751787990930ca89df4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1140" data-rawheight="320" class="origin_image zh-lightbox-thumb" width="1140" data-original="https://pic1.zhimg.com/v2-346876f631718751787990930ca89df4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1140&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1140" data-rawheight="320" class="origin_image zh-lightbox-thumb lazy" width="1140" data-original="https://pic1.zhimg.com/v2-346876f631718751787990930ca89df4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-346876f631718751787990930ca89df4_b.jpg"/></figure><p>证明上主要的技术就是把两方面的误差叠加起来：估计的 transition matrix 和真实 transition matrix 的误差，在估计的 transition matrix 上求解出来的策略和这上面最优策略之间的误差。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-ae4c13a59ba86a95a02da7ced3e7c8b5_b.jpg" data-caption="" data-size="normal" data-rawwidth="1119" data-rawheight="225" class="origin_image zh-lightbox-thumb" width="1119" data-original="https://pic2.zhimg.com/v2-ae4c13a59ba86a95a02da7ced3e7c8b5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1119&#39; height=&#39;225&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1119" data-rawheight="225" class="origin_image zh-lightbox-thumb lazy" width="1119" data-original="https://pic2.zhimg.com/v2-ae4c13a59ba86a95a02da7ced3e7c8b5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ae4c13a59ba86a95a02da7ced3e7c8b5_b.jpg"/></figure><p>其中， <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BP%7D_1" alt="\mathbb{P}_1" eeimg="1"/> 表示初始状态分布，带 hat 的价值函数表示在估计的 MDP 上得到的价值函数，带 hat 的策略表示通过 APPROXIMATE-MDP-SOLVER 得到的策略。最后，可以得到如下定理</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-0b81e12c479373e262093be9c529dde4_b.png" data-caption="" data-size="normal" data-rawwidth="1147" data-rawheight="77" class="origin_image zh-lightbox-thumb" width="1147" data-original="https://pic1.zhimg.com/v2-0b81e12c479373e262093be9c529dde4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1147&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1147" data-rawheight="77" class="origin_image zh-lightbox-thumb lazy" width="1147" data-original="https://pic1.zhimg.com/v2-0b81e12c479373e262093be9c529dde4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0b81e12c479373e262093be9c529dde4_b.png"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_b.png" data-caption="" data-size="normal" data-rawwidth="1158" data-rawheight="109" class="origin_image zh-lightbox-thumb" width="1158" data-original="https://pic1.zhimg.com/v2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1158&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1158" data-rawheight="109" class="origin_image zh-lightbox-thumb lazy" width="1158" data-original="https://pic1.zhimg.com/v2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_b.png"/></figure><p>文章再使用 NPG [Agarwal et al 2019] （专栏前面有讲过）关于 sample complexity 的结论，就可以得到最后的定理。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-32bd32a6b0546c3e905b50fea23b41ed_b.jpg" data-caption="" data-size="normal" data-rawwidth="1174" data-rawheight="445" class="origin_image zh-lightbox-thumb" width="1174" data-original="https://pic2.zhimg.com/v2-32bd32a6b0546c3e905b50fea23b41ed_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1174&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1174" data-rawheight="445" class="origin_image zh-lightbox-thumb lazy" width="1174" data-original="https://pic2.zhimg.com/v2-32bd32a6b0546c3e905b50fea23b41ed_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-32bd32a6b0546c3e905b50fea23b41ed_b.jpg"/></figure><h3>6、Lower bound</h3><p>这一部分没太看明白，但是暂时不关心，就只贴一下结论。大致上来说，lower bound 通过构造法来证明，本文的构造主要考虑一个情况，即各个状态都能差不多概率被访问到（都 significant），但是奖励是可以随机地只出现在任意一个 state-action 上面，因此在探索阶段需要把这些状态都照顾到，因此至少需要一些样本才可能达到最后的结果。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-98a6eb7145bff3feecb88e2ab8c81f1a_b.png" data-caption="" data-size="normal" data-rawwidth="1163" data-rawheight="188" class="origin_image zh-lightbox-thumb" width="1163" data-original="https://pic3.zhimg.com/v2-98a6eb7145bff3feecb88e2ab8c81f1a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1163&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1163" data-rawheight="188" class="origin_image zh-lightbox-thumb lazy" width="1163" data-original="https://pic3.zhimg.com/v2-98a6eb7145bff3feecb88e2ab8c81f1a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-98a6eb7145bff3feecb88e2ab8c81f1a_b.png"/></figure><h3>7、ZeroMax</h3><p>本文中的算法可以看做是根据 Euler 算法得到的；文章附录中还给出了一种根据 RMax 算法得到的适用于 reward-free exploration的 ZeroMax 算法。该算法做法上也很直观，建立一个集合 K 表示见到过次数比较多的状态。在每次迭代中，先构建一个 MDP 的估计，接着在该估计的 MDP 上求解策略，最后运行该策略进行采样。该估计的 MDP 分配奖励给访问次数较少的状态，从而激励策略能够尽可能访问到所有能访问到的状态。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-6916ed062071b1c7ab2eff449a65e1a7_b.png" data-caption="" data-size="normal" data-rawwidth="1129" data-rawheight="50" class="origin_image zh-lightbox-thumb" width="1129" data-original="https://pic4.zhimg.com/v2-6916ed062071b1c7ab2eff449a65e1a7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1129&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1129" data-rawheight="50" class="origin_image zh-lightbox-thumb lazy" width="1129" data-original="https://pic4.zhimg.com/v2-6916ed062071b1c7ab2eff449a65e1a7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-6916ed062071b1c7ab2eff449a65e1a7_b.png"/></figure><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-854664a7aba8c0e1afe242758b69f9e6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1149" data-rawheight="427" class="origin_image zh-lightbox-thumb" width="1149" data-original="https://pic3.zhimg.com/v2-854664a7aba8c0e1afe242758b69f9e6_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1149&#39; height=&#39;427&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1149" data-rawheight="427" class="origin_image zh-lightbox-thumb lazy" width="1149" data-original="https://pic3.zhimg.com/v2-854664a7aba8c0e1afe242758b69f9e6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-854664a7aba8c0e1afe242758b69f9e6_b.jpg"/></figure><p>由于该算法的目标是希望能够 uniformly 地访问到各个状态，但是这其实浪费了很多精力在 insignificant 的状态上，因此最后的效率不如文章正文中提到的算法效率高。</p><h3>8、MaxEnt exploration</h3><p>针对 [Hazan et al 2019]（ICML 19&#39;）提出的 MaxEnt 算法，这篇文章在附录中也把该算法最后推导到了本文 exploration + planning 的设定上，得到了相应的 sample complexity。最后结果表明，因为和 ZeroMax 类似的原因，该算法效率不高。</p><hr/><p>Remark: The key is to construct a exploration policy such that it can visit every significant state with a visitation probability that not differs too much from maximum possible visitation probability. Next, we can apply the theorems for NPG to obtain a sample complexity bound. Notice that there is a distribution mismatch coefficient in the bound, which is exactly what we ensured in the exploration phase.</p></div></div><div class="ContentItem-time">编辑于 2020-03-08</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19813032" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">深度学习（Deep Learning）</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">机器学习</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 39 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 39</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>4 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="0754dac3-30a7-4c24-a5e5-6dcd14580276" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="0754dac3-30a7-4c24-a5e5-6dcd14580276">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"111672225":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":111672225,"title":"【强化学习 110】Reward-Free Exploration","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F111672225","imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1383ee23dba94f9a378c6115294d0d57_b.jpg","titleImage":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1383ee23dba94f9a378c6115294d0d57_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2dbac362440dbbdfe5a57fca8274ca10_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"327\" data-watermark=\"watermark\" data-original-src=\"v2-2dbac362440dbbdfe5a57fca8274ca10\" data-watermark-src=\"v2-7cb351307a603627f48630be6ab32a84\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2dbac362440dbbdfe5a57fca8274ca10_r.png\"\u002F\u003E这篇文章提出了一个比较新的设定，先在状态空间探索，然后求解 MDP 问题。文章给出了在这种设定下的 provably efficient 算法和相应的 lower bound。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2002.02794.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EJin, Chi, et al. &#34;Reward-Free Exploration for Reinforcement Learning.&#34; arXiv preprint arX…\u003C\u002Fa\u003E","created":1583639759,"updated":1583640807,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1106,"imageHeight":551,"content":"\u003Cp\u003E这篇文章提出了一个比较新的设定，先在状态空间探索，然后求解 MDP 问题。文章给出了在这种设定下的 provably efficient 算法和相应的 lower bound。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2002.02794.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EJin, Chi, et al. &#34;Reward-Free Exploration for Reinforcement Learning.&#34; arXiv preprint arXiv:2002.02794 (2020).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E从 model-based 的角度来看，强化学习问题中最困难的不在于估计 reward，而在于估计 transition function。并且，如果想要以较高的精度估计到 transition matrix 中的每一个格子的数值（e.g., in terms of total variation）是几乎不可能的。从 model-free 的角度来看，这个问题就是要找一个好的具有探索性的策略，而这里的探索性指的是在状态空间（或者状态-行动空间）上的探索。\u003C\u002Fp\u003E\u003Cp\u003E这篇文章提出了解决强化学习问题的一个范式：先进行探索（exploration phase），再进行规划求解（planning phase）。在探索的阶段不接受奖励信息，只是在状态空间上纯探索，以此得到一个探索性的策略，并且执行该策略得到一个数据集 D。在规划阶段，对于任意一个给定的奖励函数 r，利用数据集 D 估计出来的 transition function，应用标准的强化学习方法求解到好的策略。本文结合一些之前的强化学习算法的理论保证，给出了一个算法使得该算法在这个设定上 probably efficient。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1、背景和相关文献\u003C\u002Fh3\u003E\u003Cp\u003E首先，这种强化学习的范式在某些方面具有显著的优势：通过探索得到这样一个数据集 D 之后，我们就可以在不对 MDP 进行任何的采样的情况下，针对任意的奖励函数都找到接近最优的策略。这在某些情况下特别有用，比如，我们有一个期望的智能体的 behavior，但是需要设计一个奖励函数使得智能体达到这样的 behavior。（还可以参考 meta learning 等设定）\u003C\u002Fp\u003E\u003Cp\u003E其次，在技术上，该问题的实质是强化学习里面最为困难的一个问题：对于状态空间的探索问题。而且在该范式下，直接分离出来该问题，在 exploration phase 中解决。\u003C\u002Fp\u003E\u003Cp\u003E在相关的文献上：RMax [Brafman and Tennenholtz, 2002] 可以被改造成 reward-free 的形式，但是样本效率比较低；其他针对特定 reward 的算法得到的 PAC 算法，肯定不一定对于任意的 reward 最优；有一些适用 function approximation 的探索方法 [Du et al 2019, Misra et al 2019]，这两篇我还不太熟；还有一篇比较类似的，[Hazan et al 2019]（ICML 19&#39;），也是一个 reward-free 的常见（不太记得专栏有没有写了），证明了他们的算法能够达到一个 max-entropy 的策略（即，在状态空间上是 max-entropy），但是没有说明探索得到的“成果”如何转化为相对于任意奖励函数的最优策略。当时读这一篇的时候就觉得肯定得填上这一环，但是主要的技术原因在于 max-entropy  并不保证“每个”状态都被较好地访问到；而这篇文章观察到，有些实在怎么着都访问不太到的状态其实也不是很影响最后的 value function，因此把不去过多地访问它们也没事。\u003C\u002Fp\u003E\u003Ch3\u003E2、Reward-free setting\u003C\u002Fh3\u003E\u003Cp\u003E文章提出的这个范式，在第一个探索阶段只做 reward-free 的探索，这个交互和标准的 RL 交互的区别就在于环境不返回奖励。相比于标准 RL，其他方面都一样，比如都具有一个固定的初始状态分布，并且要从该分布出发根据 transition dynamics 来访问各个状态。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7cb351307a603627f48630be6ab32a84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb\" width=\"1224\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7cb351307a603627f48630be6ab32a84_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1224&#39; height=&#39;327&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1224\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7cb351307a603627f48630be6ab32a84_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7cb351307a603627f48630be6ab32a84_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E3、Overview\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2887dfcbf3130a957473ebb130133ba7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1171\" data-rawheight=\"310\" class=\"origin_image zh-lightbox-thumb\" width=\"1171\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2887dfcbf3130a957473ebb130133ba7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1171&#39; height=&#39;310&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1171\" data-rawheight=\"310\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1171\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2887dfcbf3130a957473ebb130133ba7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2887dfcbf3130a957473ebb130133ba7_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中，第 1、2 步就是我们前面讲到的 exploration phase；而 3、4 步就是 planning phase。\u003C\u002Fp\u003E\u003Cp\u003E最后文章证明了该算法只需要在 exploration phase 做这么多个轨迹的探索，就能在 planning phase 对于任意的奖励函数都找到 epsilon-optimal policy。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1900ecc67dac98f107d6231e44c0605e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1140\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb\" width=\"1140\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1900ecc67dac98f107d6231e44c0605e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1140&#39; height=&#39;257&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1140\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1140\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1900ecc67dac98f107d6231e44c0605e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1900ecc67dac98f107d6231e44c0605e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E4、Exploration phase\u003C\u002Fh3\u003E\u003Cp\u003E这个阶段的通过智能体和环境的 reward-free 交互，找到一个具有探索性的策略，然后输出一个使用该策略得到的采样数据集。算法如下图所示。其中 3-7 行是在学习得到一个探索性的策略集合，具体的方式是对于每一个状态，都构造一个只在该状态上有奖励的奖励函数，然后使用已有的 RL 算法来得到能够把智能体带到该状态的策略集合。（注意到带到第 h 层的状态 s 之后，再从该状态出发，选择什么 action 都无所谓了，因此第 6 行把 Euler 得到的策略稍微又改了一下）接下来第 8-11 行则执行执行该策略族，得到一个数据集 D。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-623f9853b4048891eff9ce80345f48d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"461\" class=\"origin_image zh-lightbox-thumb\" width=\"1136\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-623f9853b4048891eff9ce80345f48d9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1136&#39; height=&#39;461&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"461\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1136\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-623f9853b4048891eff9ce80345f48d9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-623f9853b4048891eff9ce80345f48d9_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E我们前面提到，对于有些实在访问不到的状态（无论什么策略都访问不到），其实可以直接把它们放弃掉。因为，即使这些状态上的奖励函数很高，但是我们怎么控制也达不到这些状态，因此最后的最优策略也不可能特别地去访问这些状态。我们记这部分状态为 insignificant，具体定义如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a47ce7e00c31fc8b5ece408cd89349e_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1142\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1142\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a47ce7e00c31fc8b5ece408cd89349e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1142&#39; height=&#39;152&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1142\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1142\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a47ce7e00c31fc8b5ece408cd89349e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a47ce7e00c31fc8b5ece408cd89349e_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E文章证明思路的关键就在于，通过该 exploration 得到的数据集对于任意 significant 的状态，都能够以比较大的概率访问到。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6590afbd5d0348de09b3989d397e7bc0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"1151\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6590afbd5d0348de09b3989d397e7bc0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1151&#39; height=&#39;200&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1151\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6590afbd5d0348de09b3989d397e7bc0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6590afbd5d0348de09b3989d397e7bc0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E文章之所以要用 Euler 是因为该算法得到的策略集的性能和最优价值函数的大小有关，即，如果这个最优价值函数本身就不特别好（比如，即使最优策略也访问不到那些 reward 比较大但是 insignificant 的状态），那么得到的策略和最优价值函数的差距也不会特别大。这一点对于本文的证明比较关键。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f771f1d27238d65167dd191ab42492ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb\" width=\"1136\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f771f1d27238d65167dd191ab42492ff_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1136&#39; height=&#39;301&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1136\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f771f1d27238d65167dd191ab42492ff_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f771f1d27238d65167dd191ab42492ff_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E5、Planning phase\u003C\u002Fh3\u003E\u003Cp\u003E规划阶段的做法就比较就简单了，就是从数据集里面估计 transition matrix 然后使用该 transition matrix 来求解。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-346876f631718751787990930ca89df4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1140\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb\" width=\"1140\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-346876f631718751787990930ca89df4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1140&#39; height=&#39;320&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1140\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1140\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-346876f631718751787990930ca89df4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-346876f631718751787990930ca89df4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E证明上主要的技术就是把两方面的误差叠加起来：估计的 transition matrix 和真实 transition matrix 的误差，在估计的 transition matrix 上求解出来的策略和这上面最优策略之间的误差。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ae4c13a59ba86a95a02da7ced3e7c8b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1119\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb\" width=\"1119\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ae4c13a59ba86a95a02da7ced3e7c8b5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1119&#39; height=&#39;225&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1119\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1119\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ae4c13a59ba86a95a02da7ced3e7c8b5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ae4c13a59ba86a95a02da7ced3e7c8b5_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BP%7D_1\" alt=\"\\mathbb{P}_1\" eeimg=\"1\"\u002F\u003E 表示初始状态分布，带 hat 的价值函数表示在估计的 MDP 上得到的价值函数，带 hat 的策略表示通过 APPROXIMATE-MDP-SOLVER 得到的策略。最后，可以得到如下定理\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0b81e12c479373e262093be9c529dde4_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1147\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb\" width=\"1147\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0b81e12c479373e262093be9c529dde4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1147&#39; height=&#39;77&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1147\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1147\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0b81e12c479373e262093be9c529dde4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0b81e12c479373e262093be9c529dde4_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb\" width=\"1158\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1158&#39; height=&#39;109&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1158\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2b4cd74bf24ecd0e7f9d7b9ba9d6ae8_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E文章再使用 NPG [Agarwal et al 2019] （专栏前面有讲过）关于 sample complexity 的结论，就可以得到最后的定理。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-32bd32a6b0546c3e905b50fea23b41ed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1174\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb\" width=\"1174\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-32bd32a6b0546c3e905b50fea23b41ed_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1174&#39; height=&#39;445&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1174\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1174\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-32bd32a6b0546c3e905b50fea23b41ed_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-32bd32a6b0546c3e905b50fea23b41ed_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E6、Lower bound\u003C\u002Fh3\u003E\u003Cp\u003E这一部分没太看明白，但是暂时不关心，就只贴一下结论。大致上来说，lower bound 通过构造法来证明，本文的构造主要考虑一个情况，即各个状态都能差不多概率被访问到（都 significant），但是奖励是可以随机地只出现在任意一个 state-action 上面，因此在探索阶段需要把这些状态都照顾到，因此至少需要一些样本才可能达到最后的结果。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98a6eb7145bff3feecb88e2ab8c81f1a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1163\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb\" width=\"1163\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98a6eb7145bff3feecb88e2ab8c81f1a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1163&#39; height=&#39;188&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1163\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1163\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98a6eb7145bff3feecb88e2ab8c81f1a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98a6eb7145bff3feecb88e2ab8c81f1a_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E7、ZeroMax\u003C\u002Fh3\u003E\u003Cp\u003E本文中的算法可以看做是根据 Euler 算法得到的；文章附录中还给出了一种根据 RMax 算法得到的适用于 reward-free exploration的 ZeroMax 算法。该算法做法上也很直观，建立一个集合 K 表示见到过次数比较多的状态。在每次迭代中，先构建一个 MDP 的估计，接着在该估计的 MDP 上求解策略，最后运行该策略进行采样。该估计的 MDP 分配奖励给访问次数较少的状态，从而激励策略能够尽可能访问到所有能访问到的状态。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6916ed062071b1c7ab2eff449a65e1a7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1129\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb\" width=\"1129\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6916ed062071b1c7ab2eff449a65e1a7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1129&#39; height=&#39;50&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1129\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1129\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6916ed062071b1c7ab2eff449a65e1a7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6916ed062071b1c7ab2eff449a65e1a7_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-854664a7aba8c0e1afe242758b69f9e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1149\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb\" width=\"1149\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-854664a7aba8c0e1afe242758b69f9e6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1149&#39; height=&#39;427&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1149\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1149\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-854664a7aba8c0e1afe242758b69f9e6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-854664a7aba8c0e1afe242758b69f9e6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E由于该算法的目标是希望能够 uniformly 地访问到各个状态，但是这其实浪费了很多精力在 insignificant 的状态上，因此最后的效率不如文章正文中提到的算法效率高。\u003C\u002Fp\u003E\u003Ch3\u003E8、MaxEnt exploration\u003C\u002Fh3\u003E\u003Cp\u003E针对 [Hazan et al 2019]（ICML 19&#39;）提出的 MaxEnt 算法，这篇文章在附录中也把该算法最后推导到了本文 exploration + planning 的设定上，得到了相应的 sample complexity。最后结果表明，因为和 ZeroMax 类似的原因，该算法效率不高。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp\u003ERemark: The key is to construct a exploration policy such that it can visit every significant state with a visitation probability that not differs too much from maximum possible visitation probability. Next, we can apply the theorems for NPG to obtain a sample complexity bound. Notice that there is a distribution mismatch coefficient in the bound, which is exactly what we ensured in the exploration phase.\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","type":"topic","id":"19813032","name":"深度学习（Deep Learning）"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","type":"topic","id":"19559450","name":"机器学习"}],"voteupCount":39,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":4,"contributions":[{"id":23223829,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 110】Reward-Free Exploration - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F111672225 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F111672225","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F111672225","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>