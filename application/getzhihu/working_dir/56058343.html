<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习入门 2】强化学习策略迭代类方法 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="目录初识时间差分方法 —— TD(0)从少量状态到数不清的状态 —— 函数逼近技术从TD(0)到MC的过渡 —— TD(\lambda)用强化学习来玩游戏 —— DQN引言强化学习的目标是找到能够最大化收益的策略，其中一类重要的方法…"/><meta data-react-helmet="true" property="og:title" content="【强化学习入门 2】强化学习策略迭代类方法"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/56058343"/><meta data-react-helmet="true" property="og:description" content="目录初识时间差分方法 —— TD(0)从少量状态到数不清的状态 —— 函数逼近技术从TD(0)到MC的过渡 —— TD(\lambda)用强化学习来玩游戏 —— DQN引言强化学习的目标是找到能够最大化收益的策略，其中一类重要的方法…"/><meta data-react-helmet="true" property="og:image" content=""/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:56058343,&quot;title&quot;:&quot;【强化学习入门 2】强化学习策略迭代类方法&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习入门 2】强化学习策略迭代类方法</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">15 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><h2><b>目录</b></h2><ul><li>初识时间差分方法 —— TD(0)</li><li>从少量状态到数不清的状态 —— 函数逼近技术</li><li>从TD(0)到MC的过渡 —— TD(<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/>)</li><li>用强化学习来玩游戏 —— DQN</li></ul><h2><b>引言</b></h2><p>强化学习的目标是找到能够最大化收益的策略，其中一类重要的方法是策略迭代类的方法。这类方法的的主要特征是采用了Sutton的强化学习书中提到的GPI（generalized policy iteration）框架。这类方法都包含两个步骤的交替进行，在策略改进步（policy improvment）中，根据学到的价值函数产生这个价值函数下较好的策略；在策略评价步（policy evaluation）中，估计新产生策略对应的价值函数。前一讲中的Modified Policy Iteration算法是这类方法的一个草图。</p><p>在这一讲中，主要介绍<b><i>时间差分方法</i></b>（temporal-different method, TD），它结合了MC方法model-free的特点和DP方法bootstrap的特点。我们先介绍on-policy和off-policy情形下的TD(0)算法。接下来我们会拓展到状态空间、行动空间连续（或者指数级多）的情形，在这种情形下每个状态的价值函数不再是单独储存而是用函数去对其拟合。我们还将介绍 TD(<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/>)算法，通过它可以自然地从 <img src="https://www.zhihu.com/equation?tex=TD%280%29" alt="TD(0)" eeimg="1"/> 过渡到MC方法。最后我们介绍策略迭代类方法里面十分具有影响力的一篇工作DQN。</p><h2><b>初识时间差分方法 —— TD(0)</b></h2><p>回顾前面的两种方法，DP方法需要直接使用对于环境的建模（即，需要使用 <img src="https://www.zhihu.com/equation?tex=p%28s%27%2Cr%7Cs%2Ca%29" alt="p(s&#39;,r|s,a)" eeimg="1"/> ），而MC方法可以直接从与环境的交互经历中间来学习，它不需要知道对于环境的建模或者估计一个关于环境的模型（即，不需要使用或者估计 <img src="https://www.zhihu.com/equation?tex=p%28s%27%2Cr%7Cs%2Ca%29" alt="p(s&#39;,r|s,a)" eeimg="1"/> ）。对于类似DP这样需要对环境建模的方法我们称之为<b><i>model-based</i></b>方法；对于类似MC这样不需要对环境建模的方法我们称之为<b><i>model-free</i></b>方法。同时，我们还注意到，在DP方法中，对于某个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 的价值函数的估计需要使用到对于其后续状态 <img src="https://www.zhihu.com/equation?tex=s%27" alt="s&#39;" eeimg="1"/> 的价值函数估计值；而在MC方法中，对于某个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 的价值函数的估计来自于一整条抵达终止状态的链的实际收益，它不依赖其他状态的价值函数估计值。对于像DP这种对于一个状态价值函数估计的更新需要依赖其他状态价值函数估计的性质，我们称之为<b><i>bootstrap</i></b>。这里介绍的TD方法既像MC一样有model-free的特点，也像DP一样有bootstrap的性质。</p><p>回顾DP方法，DP方法在更新某个状态 <img src="https://www.zhihu.com/equation?tex=S_t" alt="S_t" eeimg="1"/> 的价值函数估计值的时候需要根据状态 <img src="https://www.zhihu.com/equation?tex=S_t" alt="S_t" eeimg="1"/> 后续所有可能状态 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D" alt="S_{t+1}" eeimg="1"/> 的价值函数估计值来更新，这种方式叫做<b><i>full backup</i></b>。MC方法在更新某个状态 <img src="https://www.zhihu.com/equation?tex=S_t" alt="S_t" eeimg="1"/> 的价值函数估计值的时候，只需要对于后续状态进行采样，使用一个后续状态 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D" alt="S_{t+1}" eeimg="1"/> 的价值函数估计值来更新即可，这种方式叫做<b><i>sample backup</i></b>。考虑知道了当前的状态 <img src="https://www.zhihu.com/equation?tex=S_t" alt="S_t" eeimg="1"/> 和目前该状态的价值函数估计值 <img src="https://www.zhihu.com/equation?tex=V%28S_t%29" alt="V(S_t)" eeimg="1"/> ，在通过策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 行动之后到达了下一个状态 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D" alt="S_{t+1}" eeimg="1"/> ，同时取得了短期的奖励 <img src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D" alt="R_{t+1}" eeimg="1"/> 。不难推出，相比于 <img src="https://www.zhihu.com/equation?tex=V%28S_t%29" alt="V(S_t)" eeimg="1"/> ， <img src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29" alt="R_{t+1} + \gamma V(S_{t+1})" eeimg="1"/> 是一个更好的估计值，因此在每一步中，我们都把估计值 <img src="https://www.zhihu.com/equation?tex=V%28S_t%29" alt="V(S_t)" eeimg="1"/> 往这个更好的方向更新一点，因此我们可以得到<b><i>TD误差</i></b>（TD error）和价值函数更新公式。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29+-+V%28S_t%29" alt="\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)" eeimg="1"/> </p><p><img src="https://www.zhihu.com/equation?tex=V%28S_t%29+%5Cleftarrow+V%28S_t%29+%2B+%5Calpha+%5Cdelta_t" alt="V(S_t) \leftarrow V(S_t) + \alpha \delta_t" eeimg="1"/> </p><p>与MC方法类似，TD方法也有on-policy和off-policy的版本，这里先给出他们各自的算法框图。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%B8%80%EF%BC%9ASARSA%3A+on-policy+TD+Method%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+Q%28%5Ctext%7Bterminal-state%7D%2C+%5Ccdot%29+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Ctext%7Brepeat+for+each+episode%3A%7D+%5C%5C+5%5Cquad+%26+%5Cquad+%5Ctext%7Binitialize+%7D+S+%5C%5C+6%5Cquad+%26+%5Cquad+%5Ctext%7Bchoose+%7DA%5Ctext%7B+from+%7DS%5Ctext%7B+using+policy+derived+from+%7DQ%5Ctext%7B+%28e.g.+%7D%5Cepsilon%5Ctext%7B-greedy%29%7D+%5C%5C+7%5Cquad+%26+%5Cquad+%5Ctext%7Brepeat+%28for+each+step+of+episode%29%3A%7D+%5C%5C+8%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Btack+action+%7DA%5Ctext%7B+observe+%7DR%2C+S%27+%5C%5C+9%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bchoose+%7DA%27%5Ctext%7B+from+%7D+S%27+%5Ctext%7B+using+policy+derived+from+%7D+Q%5Ctext%7B+%28e.g.+%7D%5Cepsilon%5Ctext%7B-greedy%29%7D%5C%5C+10%5Cquad+%26+%5Cquad+%5Cquad+Q%28S%2C+A%29+%5Cleftarrow+Q%28S%2C+A%29+%2B+%5Calpha+%5BR+%2B+%5Cgamma+Q%28S%27%2C+A%27%29+-+Q%28S%2C+A%29%5D+%5C%5C+11%5Cquad+%26+%5Cquad+%5Cquad+S+%5Cleftarrow+S%27%2C+A+%5Cleftarrow+A%27+%5C%5C+12%5Cquad+%26+%5Cquad+%5Ctext%7Buntil+%7D+S+%5Ctext%7B+is+terminated%7D+%5C%5C+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法一：SARSA: on-policy TD Method} \\ 1\quad &amp; \text{initialize } \forall s \in \mathcal{S}, a \in \mathcal{A}(s)\\ 2\quad &amp; \quad Q(s, a) \leftarrow \text{arbitrary}  \\ 3\quad &amp; \quad Q(\text{terminal-state}, \cdot) \leftarrow 0 \\ 4\quad &amp; \text{repeat for each episode:} \\ 5\quad &amp; \quad \text{initialize } S \\ 6\quad &amp; \quad \text{choose }A\text{ from }S\text{ using policy derived from }Q\text{ (e.g. }\epsilon\text{-greedy)} \\ 7\quad &amp; \quad \text{repeat (for each step of episode):} \\ 8\quad &amp; \quad \quad \text{tack action }A\text{ observe }R, S&#39; \\ 9\quad &amp; \quad \quad \text{choose }A&#39;\text{ from } S&#39; \text{ using policy derived from } Q\text{ (e.g. }\epsilon\text{-greedy)}\\ 10\quad &amp; \quad \quad Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S&#39;, A&#39;) - Q(S, A)] \\ 11\quad &amp; \quad \quad S \leftarrow S&#39;, A \leftarrow A&#39; \\ 12\quad &amp; \quad \text{until } S \text{ is terminated} \\ \end{aligned}" eeimg="1"/> </p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%BA%8C%EF%BC%9AQ-learning%3A+off-policy+TD+Method%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+Q%28%5Ctext%7Bterminal-state%7D%2C+%5Ccdot%29+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Ctext%7Brepeat+for+each+episode%3A%7D+%5C%5C+5%5Cquad+%26+%5Cquad+%5Ctext%7Binitialize+%7D+S+%5C%5C++6%5Cquad+%26+%5Cquad+%5Ctext%7Brepeat+%28for+each+step+of+episode%29%3A%7D+%5C%5C+7%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bchoose+%7DA%5Ctext%7B+from+%7DS%5Ctext%7B+using+policy+derived+from+%7DQ%5Ctext%7B+%28e.g.+%7D%5Cepsilon%5Ctext%7B-greedy%29%7D+%5C%5C+8%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Btack+action+%7DA%5Ctext%7B+observe+%7DR%2C+S%27+%5C%5C+9%5Cquad+%26+%5Cquad+%5Cquad+Q%28S%2C+A%29+%5Cleftarrow+Q%28S%2C+A%29+%2B+%5Calpha+%5BR+%2B+%5Cgamma+%5Cmax_a+Q%28S%27%2C+a%29+-+Q%28S%2C+A%29%5D+%5C%5C+10+%5Cquad+%26+%5Cquad+%5Cquad+S+%5Cleftarrow+S%27+%5C%5C+11%5Cquad+%26+%5Cquad+%5Ctext%7Buntil+%7D+S+%5Ctext%7B+is+terminated%7D+%5C%5C+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法二：Q-learning: off-policy TD Method} \\ 1\quad &amp; \text{initialize } \forall s \in \mathcal{S}, a \in \mathcal{A}(s)\\ 2\quad &amp; \quad Q(s, a) \leftarrow \text{arbitrary}  \\ 3\quad &amp; \quad Q(\text{terminal-state}, \cdot) \leftarrow 0 \\ 4\quad &amp; \text{repeat for each episode:} \\ 5\quad &amp; \quad \text{initialize } S \\  6\quad &amp; \quad \text{repeat (for each step of episode):} \\ 7\quad &amp; \quad \quad \text{choose }A\text{ from }S\text{ using policy derived from }Q\text{ (e.g. }\epsilon\text{-greedy)} \\ 8\quad &amp; \quad \quad \text{tack action }A\text{ observe }R, S&#39; \\ 9\quad &amp; \quad \quad Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_a Q(S&#39;, a) - Q(S, A)] \\ 10 \quad &amp; \quad \quad S \leftarrow S&#39; \\ 11\quad &amp; \quad \text{until } S \text{ is terminated} \\ \end{aligned}" eeimg="1"/> </p><p>可以看到，在TD方法中，个体和环境的每产生一次交互，程序都会更新相应的价值函数。在SARSA（on-policy方法）中，目标策略和行动策略都是关于行动价值函数 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy策略；在Q-learning（off-policy方法）中，行动策略是关于 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy策略，而目标策略是关于 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 的greedy策略。</p><p>这里可以总结出TD方法的几个优势。首先，如同MC一样，它不需要关于环境的建模，是一种model-free的方法。其次，它在和环境的每一步交互过后都进行相应的计算，是一种完全的增量形式；与之相反的是MC方法，它需要在一个回合结束过后集中进行运算，在很多应用场景中，这是一个比较低效的方式。另外，对于有限状态的情形下（状态数目不多，以至于能够使用离散的 <img src="https://www.zhihu.com/equation?tex=V%28S%29" alt="V(S)" eeimg="1"/> 或者 <img src="https://www.zhihu.com/equation?tex=Q%28S%2CA%29" alt="Q(S,A)" eeimg="1"/> 存储），可以证明算法的收敛性。最后，在经验上TD算法的收敛速度在很多情况下比MC和DP方法更快。</p><p>为了区别于后面会讲到的另一类TD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )算法，我们将这种只往后看一步的方法称作TD(0)。</p><p>最后引用一下Sutton书中的一句话来说明TD算法思想在强化学习领域的重要性。</p><blockquote>If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.</blockquote><h2><b>从少量状态到数不清的状态 —— 函数逼近技术</b></h2><p>至此，我们对于状态价值函数和行动价值函数的表示都是针对不同的状态分别储存一个数值，并且把它们当做不同的情形来更新的。这种方式我们称之为<b><i>表格方法</i></b>（tabular solution method），因为我们对于每个状态都创建了一个价值函数，在价值函数这个表格的不同格子上更新相应的价值函数。但是在很多情形下，状态可能是连续分布的，这就导致状态数目可能是无穷多的，比如一条线段上的的位置；在另外一些情形下，状态可能是通过排列组合产生的，比如100个不同颜色小球的排列，这样的情形可能产生巨大的状态数目。当我们对每个状态都采用一个格子来储存他们的价值函数时，要么遍历一遍所有的状态会消耗巨大的计算资源（比如，DP算法中需要对于某个状态的后续所有可能的状态遍历），要么在很长的计算时间内，总有大量的状态一次都不能被更新到（比如，MC或者TD算法中虽然并不要求每次迭代遍历所有的状态，但是有些状态一次都不能被访问到会导致算法性能大幅下降）。</p><p>当我们遇到无穷或者庞大的状态集的时候，我们需要采取<b><i>近似方法</i></b>（approximate solution method）。我们通常使用一组参数来控制相应的状态价值函数（如果是行动状态函数也是类似的处理方法），我们把状态价值函数 <img src="https://www.zhihu.com/equation?tex=v%28s%29" alt="v(s)" eeimg="1"/> 写成 <img src="https://www.zhihu.com/equation?tex=v+%28s%2C+%5Cmathbf%7Bw%7D%29" alt="v (s, \mathbf{w})" eeimg="1"/> 的形式，它的具体形式可以根据不同任务而不同，比如最简单的也是我们下面讨论中默认的一种形式——线性近似（linear approximation） <img src="https://www.zhihu.com/equation?tex=v%28s%2C+%5Cmathbf%7Bw%7D%29+%3D+%5Cmathbf%7Bw%7D%5ET+%5Cmathbf%7Bx%7D%28s%29" alt="v(s, \mathbf{w}) = \mathbf{w}^T \mathbf{x}(s)" eeimg="1"/> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bs%7D" alt="\mathbf{s}" eeimg="1"/> 是维度相同的向量，最常见的 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%28s%29" alt="\mathbf{x}(s)" eeimg="1"/> 可以使one-hot形式的向量，表示该状态处于某个状态组中。因此当我们要更新状态价值函数的估计值时，我们不再对于某一个格子上的数值直接进行更新，而是对于参数 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 进行更新。参数的更新公式可以通过对 <img src="https://www.zhihu.com/equation?tex=%28G_t+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%29%5E2" alt="(G_t - v(S_t, \mathbf{w}_t))^2" eeimg="1"/> 求导得到，其中 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> 是得到的 <img src="https://www.zhihu.com/equation?tex=v%28S_t%29" alt="v(S_t)" eeimg="1"/> 的目标值。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BG_t+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_w+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [G_t - v(S_t, \mathbf{w}_t)] \nabla_w v(S_t, \mathbf{w}_t)" eeimg="1"/> </p><p>我们可以使用采样方法得到 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> ，此时 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> 是无偏估计，因此上式的后一项是真实的梯度；我们还可以使用bootstrap的方法来得到 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> ，比如 <img src="https://www.zhihu.com/equation?tex=G_t+%3D+r_t+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w_t%29" alt="G_t = r_t + \gamma v(S_{t+1}, w_t)" eeimg="1"/> ，但由于这个目标还依赖其他的价值函数值，而这些价值函数的值又同时依赖参数 <img src="https://www.zhihu.com/equation?tex=w_t" alt="w_t" eeimg="1"/> ，为了计算简便我们不再考虑目标值中关于参数的梯度，这样的近似产生<b><i>半梯度方法</i></b>（semi-gradient method）。</p><p>由此我们能够写出在半梯度方法下TD(0)的更新公式。可以证明它在线性近似和on-policy的情形下是可以收敛的。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_%7B%5Cmathbf%7Bw%7D%7D+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [R_{t+1} + \gamma v(S_{t+1}, \mathbf{w}_t) - v(S_t, \mathbf{w}_t)] \nabla_{\mathbf{w}} v(S_t, \mathbf{w}_t)" eeimg="1"/> </p><p>当然，这样的近似价值函数还可以使用更为复杂的形式来表示，比如使用决策树或者神经网络等。不过值得注意的是，在表格方法中很多算法都能够有较好的收敛保证，但是在函数逼近方法下很多算法都十分不稳定，尤其是当算法中同时包含<b><i>函数逼近技术、bootstrap和off-policy</i></b>时，算法在理论和实践上都很难保证稳定。而这三者都十分的重要，函数逼近技术让我们能够解决状态空间复杂的问题；bootstrap大幅提高的算法的效率；而off-policy让我们能够更好地控制算法的探索，这对于学习到一个好的策略十分重要。而这三者的结合却很容易导致算法的发散，Sutton在书中称它们为<b><i>The Deadly Triad</i></b>。</p><p>算法的收敛性保证、实践上算法的稳定性以及算法的效率是应用到函数逼近技术强化学习算法的核心问题。off-policy下算法的不稳定主要是由于1）行动策略得到目标和目标策略所需目标的不一致，这个问题可以通过前面提到的重要性采样率来解决；2）行动策略产生数据的分布和算法所需要的分布不一致，这个问题是目前比较困难的问题。为了应对在off-policy上不稳定的问题大致上有两类思路：第一类思路是去设计一种真实的梯度方法，而不是前面提到的半梯度方法，因为半梯度方法的收敛性依赖于数据的on-policy分布；第二类思路是得到行动策略采集到的数据之后，对其进行加权，使其分布看起来像一个on-policy策略得到的数据分布。有如下几种尝试解决的方法：</p><ol><li><b><i>MC方法</i></b>：使用蒙特卡洛方法来更新公式 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BG_t+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_w+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [G_t - v(S_t, \mathbf{w}_t)] \nabla_w v(S_t, \mathbf{w}_t)" eeimg="1"/> 里面的 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> ，所得到的梯度是真实的梯度，能够在off-policy情形下保证稳定。但是这种做法的缺点在于效率低。</li><li><b><i>Naive Residual Gradient Algorithm</i></b>：直接对Mean Squared TD Error  <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTDE%7D" alt="\overline{TDE}" eeimg="1"/> 求梯度，并且做梯度下降，其更新公式为 <img src="https://www.zhihu.com/equation?tex=+%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%28%5Cnabla_w+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+-%5Cgamma+%5Cnabla_w+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29%29" alt=" \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t (\nabla_w v(S_t, \mathbf{w}_t) -\gamma \nabla_w v(S_{t+1}, \mathbf{w}_t))" eeimg="1"/> ，它相比于Semi-gradient TD(0)方法唯一的不同就是后面多出了减去的一项，之前的半梯度方法没有能够考虑到 <img src="https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29" alt="G_t = R_{t+1} + \gamma v(S_{t+1}, \mathbf{w}_t)" eeimg="1"/> 里面后一步估计产生的梯度，而这里就将这一部分梯度补上了，使其成为了一个真实的梯度。这个方法能够稳健的收敛，但是其问题在于最小化 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTDE%7D" alt="\overline{TDE}" eeimg="1"/> 之后收敛到的并不是正确的价值函数；换句话说，价值函数在其真实取值的时候对应的 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTDE%7D" alt="\overline{TDE}" eeimg="1"/> 并非最小。</li><li><b><i>Residual Gradient Algorithm</i></b>：当价值函数在真实取值的时候，对应的Bellman error为零，<i>Bellman error是TD error的期望</i>。因此我们对Bellman Error  <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BBE%7D" alt="\overline{BE}" eeimg="1"/> 来求梯度，并且做梯度下降，这样得到的方法就是Residual Gradient Algorithm，其更新公式为 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cleft%5B+%5Cmathbb%7BE%7D_b%5B%5Crho_t+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29%29%5D+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%29+%5Cright%5D+%5Cleft%5B+%5Cnabla+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+-%5Cgamma+%5Cmathbb%7BE%7D_b%5B%5Crho_t+%5Cnabla+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cright%5D" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[ \mathbb{E}_b[\rho_t (R_{t+1} + \gamma v(S_{t+1}, \mathbf{w}_t))] - v(S_t, \mathbf{w}_t)) \right] \left[ \nabla v(S_t, \mathbf{w}_t) -\gamma \mathbb{E}_b[\rho_t \nabla v(S_{t+1}, \mathbf{w}_t)] \right]" eeimg="1"/> 。需要注意的是这里面有两个期望，这两个期望里面的采样需要是独立的。这个方法的缺点在于：1）慢；2）仍然收敛到的可能不是正确的价值函数；3）最重要的在于 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BBE%7D" alt="\overline{BE}" eeimg="1"/> 不是<b><i>可学习</i></b>（learnable）的。不同的MDP可以产生相同的数据（包括state、action、reward序列），但是它们却对应不同的 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BBE%7D" alt="\overline{BE}" eeimg="1"/> ，因此我们无法单独从数据中学习到 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BBE%7D" alt="\overline{BE}" eeimg="1"/> ，这种情况我们称之为不可学习的。</li><li><b><i>Gradient-TD Method</i></b>：前面给我们的启示就是我们需要找一个可学习的目标，然后对其优化。函数逼近技术中权值 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 的维度比状态数目一般要小，因此某种函数逼近方法只是在价值函数空间中的一个子空间，把Bellman Error投影到这个子空间上，得到的就是Projected Bellman Error <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BPBE%7D" alt="\overline{PBE}" eeimg="1"/> ，它是可学习的。对 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BPBE%7D" alt="\overline{PBE}" eeimg="1"/> 做梯度，可以得到权值的更新公式（线性近似） <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Crho_t+%28%5Cmathbf%7Bx%7D_t+-+%5Cgamma+%5Cmathbf%7Bx%7D_%7Bt%2B1%7D%29+%5Cmathbf%7Bx%7D_t%5ET+%5Cmathbf%7Bv%7D_t" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \rho_t (\mathbf{x}_t - \gamma \mathbf{x}_{t+1}) \mathbf{x}_t^T \mathbf{v}_t" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bv%7D_t+%2B+%5Cbeta+%5Crho_t+%28%5Cdelta_t+-+%5Cmathbf%7Bv%7D_t%5ET+%5Cmathbf%7Bx%7D_t%29%5Cmathbf%7Bx%7D_t" alt="\mathbf{v}_{t+1} = \mathbf{v}_t + \beta \rho_t (\delta_t - \mathbf{v}_t^T \mathbf{x}_t)\mathbf{x}_t" eeimg="1"/> ，增加了一个变量 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D_t" alt="\mathbf{v}_t" eeimg="1"/> 是把公式中的一个Least Mean Square的解的形式换成了梯度下降形式。这一类方法是目前使用最为广泛的稳定off-policy方法。</li><li><b><i>Emphatic-TD Method</i></b> [2,3]：前面的思路都是寻找一个好的优化目标并且求出其正确的梯度，这个方法使用了另一种思路，即使用行动策略进行采样之后，对于不同的状态进行不同的加权，使其看起来像是on-policy的分布。其权值更新公式为 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+M_t+%5Crho_t+%5Cdelta_t+%5Cnabla+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha M_t \rho_t \delta_t \nabla v(S_t, \mathbf{w}_t)" eeimg="1"/> ，这里的 <img src="https://www.zhihu.com/equation?tex=M_t" alt="M_t" eeimg="1"/> 就是动态更新的对于某个状态的权重，把样本的等效分布调整为和on-policy分布一致。</li></ol><h2><b>从TD(0)到MC的过渡 —— TD(</b> <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> <b>)</b></h2><p>回顾前面的TD和MC方法，在TD方法中，每次都把当前的状态价值函数都往一个更为合理的状态价值函数估计值$ <img src="https://www.zhihu.com/equation?tex=G_t%5E%7B%280%29%7D+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7B%5Ctheta%7D%29" alt="G_t^{(0)} = R_{t+1} + \gamma v(S_{t+1}, \mathbf{\theta})" eeimg="1"/> 上更新；在MC方法中，我们每次实际上是将状态价值函数估计值往 <img src="https://www.zhihu.com/equation?tex=G_t%5E%7B%28T-t%29%7D+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%2B+%5Ccdots+%2B+%5Cgamma%5E%7BT-t%7D+R_T" alt="G_t^{(T-t)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t} R_T" eeimg="1"/> 上更新。我们其实可以定义 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/> 步的TD算法，使得其目标收益估计为</p><p><img src="https://www.zhihu.com/equation?tex=G_t%5E%7B%28n%29%7D+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%2B+%5Ccdots+%2B+%5Cgamma%5En+v%28S_%7Bt%2Bn%7D%2C+%5Cmathbf%7B%5Ctheta%7D%29%2C+0%5Cle+t%5Cle+T-n" alt="G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^n v(S_{t+n}, \mathbf{\theta}), 0\le t\le T-n" eeimg="1"/> </p><p>实际上对于 <img src="https://www.zhihu.com/equation?tex=G_t%5E%7B%28n%29%7D" alt="G_t^{(n)}" eeimg="1"/> 的任意归一化的加权组合都能够形成可行复合收益估计，比如 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B4%7DG_t%5E%7B%281%29%7D+%2B+%5Cfrac%7B3%7D%7B4%7DG_t%5E%7B%282%29%7D" alt="\frac{1}{4}G_t^{(1)} + \frac{3}{4}G_t^{(2)}" eeimg="1"/> 。这里我们定义一种特殊的复合收益估计，称之为<b><i><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益</i></b>（<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/>-return）</p><p><img src="https://www.zhihu.com/equation?tex=G_t%5E%5Clambda+%3D+%281-%5Clambda%29+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Clambda%5E%7Bn-1%7D+G_t%5E%7B%28n%29%7D" alt="G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=0%5Cle%5Clambda%5Cle+1" alt="0\le\lambda\le 1" eeimg="1"/> ，根据 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> 取值的不同，<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益对应的实际上是从MC方法到前述TD方法（也称TD(0)方法）的过渡。当 <img src="https://www.zhihu.com/equation?tex=%5Clambda+%3D+0" alt="\lambda = 0" eeimg="1"/> 时，<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益对应的就是一步的TD方法；当 <img src="https://www.zhihu.com/equation?tex=%5Clambda+%3D+1" alt="\lambda = 1" eeimg="1"/> 时，<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益对应的就是MC方法。</p><p>至此我们已经看到了 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益是TD(0)方法到MC方法的过渡，不过更为精彩的地方在于，<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益的这种定义方式让我们能够更加方便地对于涉及到未来多步奖励的目标进行权重更新。考虑一串连续的动作 <img src="https://www.zhihu.com/equation?tex=S_t%2C+A_t%2C+R_%7Bt%2B1%7D%2C+S_%7Bt%2B1%7D%2C+A_%7Bt%2B1%7D%2C+%5Ccdots" alt="S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \cdots" eeimg="1"/> ，如果我们希望对于未来 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/> 步的奖励进行统计，然后更新权重，我们就必须等到 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/> 步之后才能得到完整的奖励序列之后再进行更新，而在此之前将无事可做；更进一步，如果 <img src="https://www.zhihu.com/equation?tex=n%5Cto+%5Cinfty" alt="n\to \infty" eeimg="1"/> ，那么我们只能像MC方法一样，等到回合结束之后再回过头来更新权重。那有没有什么方法让我们每一步都做尽可能的更新呢？这样权重每一步都在更新，并且最新的信息都能被及时利用。这就要引出eligibility trace技术了。</p><p>这种技术可以将运算量从集中地在每次回合结束时进行变为分散在每一步之中进行，大致思想是将“往后看”变为“往前看”，<i>不像之前那样认为更新这一步需要后 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/> 步的奖励，现在认为这一步刚刚得到的奖励马上可以用于更新前 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/> 步内所有的权重</i>。定义eligibility trace  <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Be%7D" alt="\mathbf{e}" eeimg="1"/> ，在函数逼近技术下，它是与权值 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 维度相同的向量，它记录了历史中每一步的所经历的状态的累积，通过更新它，可以使我们能够先记录走过的路径，在最后得到相应的奖励之后，再往正确的方向更新。换句话说，权重 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 是对于到此为止所有经历的长期记忆，而 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Be%7D" alt="\mathbf{e}" eeimg="1"/> 是对于最近几步的短期记忆。</p><p>我们这里举一个最为简单的例子来说明eligibility trace技术是如何应用的。<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Be%7D" alt="\mathbf{e}" eeimg="1"/> 初值为0，每次都会累积一个当前状态下价值函数相对于权重的梯度，即</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Cmathbf%7Be%7D_%7B-1%7D+%3D+%5Cmathbf%7B0%7D+%5C%5C+%26+%5Cmathbf%7Be%7D_t+%3D+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bv%7D+%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \mathbf{e}_{-1} = \mathbf{0} \\ &amp; \mathbf{e}_t = \gamma \lambda \mathbf{e}_{t-1} + \nabla \widehat{v} (S_t, \mathbf{w}_t) \end{aligned}" eeimg="1"/> </p><p>每一步最新的TD误差为</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bv%7D%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bv%7D%28S_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\delta_t = R_{t+1} + \gamma \widehat{v}(S_{t+1}, \mathbf{w}_t) - \widehat{v}(S_t, \mathbf{w}_t)" eeimg="1"/> </p><p>权重的更新就累加TD误差和eligibility trace的乘积，</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%5Cmathbf%7Be%7D_t" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \mathbf{e}_t" eeimg="1"/> </p><p>这样得到是的<b><i>semi-gradient version TD(</i></b> <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> <b><i>)算法</i></b></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Cmathbf%7Be%7D_%7B-1%7D+%3D+%5Cmathbf%7B0%7D+%5C%5C+%26+%5Cmathbf%7Be%7D_t+%3D+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bv%7D+%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5Cquad+%5Ctext%7B%28accumulating+trace%29%7D%5C%5C+%26+%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bv%7D%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bv%7D%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5C%5C+%26+%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%5Cmathbf%7Be%7D_t+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \mathbf{e}_{-1} = \mathbf{0} \\ &amp; \mathbf{e}_t = \gamma \lambda \mathbf{e}_{t-1} + \nabla \widehat{v} (S_t, \mathbf{w}_t) \quad \text{(accumulating trace)}\\ &amp; \delta_t = R_{t+1} + \gamma \widehat{v}(S_{t+1}, \mathbf{w}_t) - \widehat{v}(S_t, \mathbf{w}_t) \\ &amp; \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \mathbf{e}_t \end{aligned}" eeimg="1"/> </p><p>这个算法是一个online的算法，但是这个算法对于offline版本 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益的模拟不是很好，另外一种更逼近offline版本 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> -收益的算法是<b><i>True Online TD( </i></b><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> <b><i>)算法</i></b>（with linear function approximation），这里的“true online”指的是它是对于一个offline目标的online实现。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Cmathbf%7Be%7D_%7B-1%7D+%3D+%5Cmathbf%7B0%7D+%5C%5C+%26+%5Cmathbf%7Be%7D_t+%3D+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%281-%5Calpha+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D%5ET+%5Cmathbf%7Bx%7D_t%29+%5Cmathbf%7Bx%7D_t+%5Cquad+%5Ctext%7B%28Dutch+trace%29%7D%5C%5C+%26+%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bv%7D%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bv%7D%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5C%5C+%26+%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%5Cmathbf%7Be%7D_t+%2B+%5Calpha+%28%5Cmathbf%7Bw%7D_t%5ET+%5Cmathbf%7Bx%7D_t+-+%5Cmathbf%7Bw%7D_%7Bt-1%7D%5ET+%5Cmathbf%7Bx%7D_t%29+%28%5Cmathbf%7Be%7D_t+-+%5Cmathbf%7Bx%7D_t%29+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \mathbf{e}_{-1} = \mathbf{0} \\ &amp; \mathbf{e}_t = \gamma \lambda \mathbf{e}_{t-1} + (1-\alpha \gamma \lambda \mathbf{e}_{t-1}^T \mathbf{x}_t) \mathbf{x}_t \quad \text{(Dutch trace)}\\ &amp; \delta_t = R_{t+1} + \gamma \widehat{v}(S_{t+1}, \mathbf{w}_t) - \widehat{v}(S_t, \mathbf{w}_t) \\ &amp; \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \mathbf{e}_t + \alpha (\mathbf{w}_t^T \mathbf{x}_t - \mathbf{w}_{t-1}^T \mathbf{x}_t) (\mathbf{e}_t - \mathbf{x}_t) \end{aligned}" eeimg="1"/> </p><p>总结一下Eligibility Trace是一种可以附加在很多算法上面的技术，它不像传统“往后看”的算法那样，必须要等到收益信号完整得到之后才对权值进行更新，它在拿到最新的数据之后，就会尽可能地对权重进行更新。这样做的好处是能够把运算量尽可能分摊到每一步中，而不是集中再完整的信号全部拿到之后；同时尽早地把信息反馈到模型中也可以使模型学习地更快。Sutton的书上面提到的相关的应用包括</p><ul><li><b><i>使用了Eligibility Trace的一个MC版本</i></b>（linear function approximation, single reward at the end of episode）：该算法在回合中的每一步中都更新两个变量，一个变量 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Be%7D" alt="\mathbf{e}" eeimg="1"/> 记录历史轨迹中状态特征向量的叠加，一个变量 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Ba%7D" alt="\mathbf{a}" eeimg="1"/> 记录轨迹上权重的叠加和“遗忘”；在回合结束拿到真实的收益之后，就可以一步对权重进行更新。</li><li><b><i>SARSA(</i></b> <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> <b><i>)和True Online SARSA(</i></b> <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> <b><i>)</i></b>：前面的TD(<img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )是应用在了状态价值函数 <img src="https://www.zhihu.com/equation?tex=v%28S%2C%5Cmathbf%7Bw%7D%29" alt="v(S,\mathbf{w})" eeimg="1"/> 上面，相应的应用在行动价值函数 <img src="https://www.zhihu.com/equation?tex=q%28S%2C+A%2C+%5Cmathbf%7Bw%7D%29" alt="q(S, A, \mathbf{w})" eeimg="1"/> 上的版本就是这个，公式是对应的。</li><li><b><i>Off-Policy版本的TD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )和SARSA( </i></b><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/><b><i>)</i></b>：由于是off-policy，增加了相应的importance ratio，相应的公式变为了 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Be%7D_t+%3D+%5Crho_t+%28%5Cgamma_t+%5Clambda_t+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bv%7D+%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%29" alt="\mathbf{e}_t = \rho_t (\gamma_t \lambda_t \mathbf{e}_{t-1} + \nabla \widehat{v} (S_t, \mathbf{w}_t) )" eeimg="1"/>  和 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Be%7D_t+%3D+%5Crho_t+%5Cgamma_t+%5Clambda_t+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bq%7D+%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{e}_t = \rho_t \gamma_t \lambda_t \mathbf{e}_{t-1} + \nabla \widehat{q} (S_t, A_t, \mathbf{w}_t)" eeimg="1"/> 。</li><li><b><i>Tree-Backup( </i></b><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/><b><i> )</i></b>：它是off-policy的方法，但是不涉及使用importance ratio，它对每个样本调整了权重，使其更新的目标正确，其更新公式为 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Be%7D_t+%3D+%5Cgamma_t+%5Clambda_t+%5Cpi%28A_t+%7C+S_t%29+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bq%7D+%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{e}_t = \gamma_t \lambda_t \pi(A_t | S_t) \mathbf{e}_{t-1} + \nabla \widehat{q} (S_t, A_t, \mathbf{w}_t)" eeimg="1"/> ，使用到的TD误差为 <img src="https://www.zhihu.com/equation?tex=%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma_%7Bt%2B1%7D+%5Cbar%7BQ%7D_%7Bt%2B1%7D+-+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\delta_t = R_{t+1} + \gamma_{t+1} \bar{Q}_{t+1} - \widehat{q}(S_t, A_t, \mathbf{w}_t)" eeimg="1"/> 。</li><li><b><i>GTD( </i></b><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/><b><i> )、GQ( </i></b><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/><b><i> )、HTD( </i></b><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/><b><i> )和Emphatic TD( </i></b><img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/><b><i> )</i></b>：这四个是前面提到的Gradient TD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )以及Emphatic-TD Method的Eligibility Trace的实现版本。GTD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )是Gradient-TD的直接实现；GQ( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )是相应的action value function的版本；HTD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )结合了GTD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )和TD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )，它在目标策略和行动策略相同的情况下和TD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )完全等价；Emphatic TD( <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> )是前面Emphatic-TD的Eligibility Trace的实现版本。</li></ul><h2><b>用强化学习来玩游戏 —— DQN</b></h2><p>下面我们就来讲一下DeepMind的这一篇工作Deep Q-Network（DQN），可以说强化学习如今的火热和这篇工作的成功是分不开的，这篇工作在2013年发表在NIPS上[4]之后，又以封面文章的形式发表在了2015年的Nature杂志上[5]。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-ed8b1bb0a0679660b5881e4b3944c779_b.jpg" data-caption="" data-size="normal" data-rawwidth="506" data-rawheight="665" class="origin_image zh-lightbox-thumb" width="506" data-original="https://pic2.zhimg.com/v2-ed8b1bb0a0679660b5881e4b3944c779_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;506&#39; height=&#39;665&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="506" data-rawheight="665" class="origin_image zh-lightbox-thumb lazy" width="506" data-original="https://pic2.zhimg.com/v2-ed8b1bb0a0679660b5881e4b3944c779_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ed8b1bb0a0679660b5881e4b3944c779_b.jpg"/></figure><p>我们先回顾一下之前讲到的Q-Learning，在表格方法中，Q-Learning主要是做如下更新 <img src="https://www.zhihu.com/equation?tex=Q%28S%2C+A%29+%5Cleftarrow+Q%28S%2C+A%29+%2B+%5Calpha+%5BR+%2B+%5Cgamma+%5Cmax_a+Q%28S%27%2C+a%29+-+Q%28S%2C+A%29%5D" alt="Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_a Q(S&#39;, a) - Q(S, A)]" eeimg="1"/> ；这里的Deep指的是使用了深度神经网络来做function approximation，在function approximation下，仿照我们之前的推导，对于Mean Squared TD Error  <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTDE%7D+%3D+%5Cmathbb%7BE%7D_%7Bs%2Ca%5Csim+b%7D%5B%28G+-+%5Cwidehat%7Bq%7D%28s%2Ca%2C+%5Cmathbf%7Bw%7D%29%5E2%5D" alt="\overline{TDE} = \mathbb{E}_{s,a\sim b}[(G - \widehat{q}(s,a, \mathbf{w})^2]" eeimg="1"/> 求梯度（其中 <img src="https://www.zhihu.com/equation?tex=G+%3D+%5Cmathbb%7BE%7D_%7Bs%27%7D%5Br+%2B+%5Cgamma+max_%7Ba%27%7D+%5Cwidehat%7Bq%7D%28s%27%2C+a%27%2C+%5Cmathbf%7Bw%7D%29+%7C+s%2C+a%5D" alt="G = \mathbb{E}_{s&#39;}[r + \gamma max_{a&#39;} \widehat{q}(s&#39;, a&#39;, \mathbf{w}) | s, a]" eeimg="1"/> ），从而得到相应的半梯度方法的权值更新公式</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_%7B%5Cmathbf%7Bw%7D%7D+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [R_{t+1} + \gamma \max_a \widehat{q}(S_{t+1}, A_t, \mathbf{w}_t) - \widehat{q}(S_t, A_t, \mathbf{w}_t)] \nabla_{\mathbf{w}} \widehat{q}(S_t, A_t, \mathbf{w}_t)" eeimg="1"/> </p><p>我们注意到，这里的 <img src="https://www.zhihu.com/equation?tex=%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\widehat{q}(S_t, A_t, \mathbf{w}_t)" eeimg="1"/> 是由一个神经网络表示的，它对于 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTDE%7D" alt="\overline{TDE}" eeimg="1"/> 做梯度下降和神经网络中普通地使用BP来更新权重无异。我们可以把 <img src="https://www.zhihu.com/equation?tex=%28S_t%2C+A_t%29" alt="(S_t, A_t)" eeimg="1"/> 当做训练样本的输入，把相应的 <img src="https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="G_t = R_{t+1} + \gamma \max_a \widehat{q}(S_{t+1}, A_t, \mathbf{w}_t)" eeimg="1"/> 当做训练样本的输出，这样就转化成了一个标准的基于神经网络的有监督学习问题了。（该文章中的神经网络结构是用 <img src="https://www.zhihu.com/equation?tex=S_t" alt="S_t" eeimg="1"/> 做输入，用 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C" alt="|\mathcal{A}|" eeimg="1"/> 维的向量做输出代表不同行动的价值函数）。但是DQN中和有监督学习存在以下三点不同，增加了其相对于有监督学习的难度：</p><ol><li>有监督学习样本的目标是不变，而DQN样本的目标是基于该神经网络现在的权值的；</li><li>有监督学习样本之间是相互独立的，但是DQN中的样本是从一连串的MDP的序列中得到的，相邻的样本之间不相互独立；</li><li>有监督学习假设了样本分布的稳定性，但是DQN中随着学习到不同的策略，样本的分布也在逐渐发生变化；</li></ol><p>这个方法集齐了前面提到的The Deadly Triad，即使用了off-policy、function approximation和bootstrap，并且所使用的也是半梯度方法，按理来说算法的收敛是十分困难的。这篇文章的主要贡献就在于，对于这样一个理论上很难稳定的算法，它能够使用一些工程上的技巧使其稳定，正如原文中所说</p><blockquote>This suggests that, despite lacking any theoretical convergence guarantees, our method is able to train large neural networks using a reinforcement learning signal and stochastic gradient descent in a stable manner.</blockquote><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-76a87cbe3c5d5d5f1b145d01ca643abe_b.jpg" data-caption="" data-size="normal" data-rawwidth="1688" data-rawheight="822" class="origin_image zh-lightbox-thumb" width="1688" data-original="https://pic3.zhimg.com/v2-76a87cbe3c5d5d5f1b145d01ca643abe_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1688&#39; height=&#39;822&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1688" data-rawheight="822" class="origin_image zh-lightbox-thumb lazy" width="1688" data-original="https://pic3.zhimg.com/v2-76a87cbe3c5d5d5f1b145d01ca643abe_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-76a87cbe3c5d5d5f1b145d01ca643abe_b.jpg"/></figure><p>上图显示的就是DQN的算法，倒数第三行所叙述的更新公式就是我们前面列出来的semi-gradient Q-learning的更新公式，我们主要就其几点特征来叙述</p><ul><li>Experience Replay: 这是DQN算法中最为关键的技术，正是这个技术的应用才能够使得DQN在工程上能够稳定收敛。其主要做法是在agent有了一段经历之后，把这个经历 <img src="https://www.zhihu.com/equation?tex=e_t+%3D+%28s_t%2C+a_t%2C+r_t%2C+s_%7Bt%2B1%7D%29" alt="e_t = (s_t, a_t, r_t, s_{t+1})" eeimg="1"/> 存入<b><i>经验池</i></b>（replay memory）中，在每次需要更新权值的时候，从经验池中选取一个mini-batch来对其做梯度下降更新权重。这样的做法有以下几个特征：</li><ul><li>每个经历在经验池中都可以被多次取出来使用，这样提高了数据的使用效率；</li><li>由一个连续动作序列产生的经验相互之间具有很大的相关性，一个相关性很高的mini-batch会增大更新的方差，甚至会导致算法不稳定，而经验池的使用使得算法更加稳定；</li><li>由于在经验池中采样一个mini-batch再来更新权重，这样是的其样本上的损失函数 <img src="https://www.zhihu.com/equation?tex=%5Coverline%7BTDE%7D" alt="\overline{TDE}" eeimg="1"/>更加的平滑，有利于减少方差；</li></ul><li>Off-policy: 由于使用了经验池，在更新当前权重的时候，其样本并不是基于当前权重的greedy策略（目标策略），因此，就必然地需要使用off-policy的方法，比如这里使用的Q-learning；同时off-policy的探索更强，能够避免on-policy方法里面陷入较差的极小值点的问题。</li><li>Stacked Frames: 很多游戏里面有些元素的出现是“一闪一闪”的，如果只是传入最新一帧的图片，这样的信息就无法传递给agent，因此，在实际操作的过程中，每个状态都传递了最近四帧的图片给Q-network；从数学的角度上来说，这样的做法让状态的表示更具有马可夫性。</li><li>Frame-skipping: 在训练的时候每一帧都去判断要采取什么动作相对来讲频率比较高，这会降低agent采样到不同状态的效率，因此，在DQN的工作中会每隔4帧才传递给agent去判断要采取什么动作。</li></ul><p>自从DQN发表以来，激发了深度强化学习领域的热潮，基于DQN做的各种改进也是层出不穷，下面一幅图就显示了大家基于DQN所做的大量的工作（摘自<a href="https://zhuanlan.zhihu.com/p/39999667" class="internal">HYQ：强化学习路在何方？</a>）</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-63fefc2066a4c21a896a7af671a71fc9_b.jpg" data-size="normal" data-rawwidth="865" data-rawheight="845" class="origin_image zh-lightbox-thumb" width="865" data-original="https://pic2.zhimg.com/v2-63fefc2066a4c21a896a7af671a71fc9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;845&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="865" data-rawheight="845" class="origin_image zh-lightbox-thumb lazy" width="865" data-original="https://pic2.zhimg.com/v2-63fefc2066a4c21a896a7af671a71fc9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-63fefc2066a4c21a896a7af671a71fc9_b.jpg"/><figcaption>图：DQN 和其变种们</figcaption></figure><p>以下简单介绍几个重要的改进</p><p><b>Target Q-Network</b></p><p>算法不稳定的一个重要的来源是每次更新的目标和原本的网络具有较强的相关性，这里的思路是建立两个价值函数网络：一个网络还是原来那样用于直到行动策略进行 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy的采样，并且使用BP方法更新权重；另一个网络专门用于目标的计算。这样更新的公式就变为了</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%5E-%29+-+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_%7B%5Cmathbf%7Bw%7D%7D+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [R_{t+1} + \gamma \max_a \widehat{q}(S_{t+1}, A_t, \mathbf{w}_t^-) - \widehat{q}(S_t, A_t, \mathbf{w}_t)] \nabla_{\mathbf{w}} \widehat{q}(S_t, A_t, \mathbf{w}_t)" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cwidehat%7Bq%7D%28S%2C+A%2C+%5Cmathbf%7Bw%7D%5E-%29" alt="\widehat{q}(S, A, \mathbf{w}^-)" eeimg="1"/> 就是新增加的这个target network，而权值 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D%5E-" alt="\mathbf{w}^-" eeimg="1"/> 是每隔 <img src="https://www.zhihu.com/equation?tex=C" alt="C" eeimg="1"/> 步从 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 拷贝过来的。通过这种方法能够再进一步地使得网络的训练更加稳定，该方法是在DQN文章的Nature版本中加入的（Nature DQN）[5]。</p><p><b>Double Q-Learning</b></p><p>DQN的目标为 <img src="https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29" alt="G_t = R_{t+1} + \gamma \max_a \widehat{q}(S_{t+1}, A_t, \mathbf{w}_t)" eeimg="1"/> ，这里面有一个 <img src="https://www.zhihu.com/equation?tex=%5Cmax" alt="\max" eeimg="1"/> 的操作，这个操作会带来价值函数值的高估。举一个简单的例子帮助理解， <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5B%5Cmax%28X_1%2C+X_2%29%5D+%5Cge+%5Cmax%28%5Cmathbb%7BE%7D%5BX_1%5D%2C+%5Cmathbb%7BE%7D%5BX_1%5D%29" alt="\mathbb{E}[\max(X_1, X_2)] \ge \max(\mathbb{E}[X_1], \mathbb{E}[X_1])" eeimg="1"/> ，你可以想象两个随机变量都是标准正态分布，那么式子左边肯定是大于零的，而式子右边应该等于零。根据这一的道理，由于每个Q值都是基于其他Q值的 <img src="https://www.zhihu.com/equation?tex=%5Cmax" alt="\max" eeimg="1"/> 操作得到的，这样Q值就会被高估。</p><p>不过，这样的高估是否存在于实际的DQN算法中呢？如果存在，这样的高估是否会损害DQN的性能呢？首先，如果随着算法的迭代，如果Q值的估计越来越准确，即Q值本身方差就比较小，在做 <img src="https://www.zhihu.com/equation?tex=%5Cmax" alt="\max" eeimg="1"/> 操作之后的高估也会越来越小，这样的高估就可能在算法的迭代中消失。其次，如果所有的Q值估算出来都比真是的值均匀地高一些，其实也不影响最后形成的策略；更进一步，适当的随机高估一些状态，其实能够起到更好的探索的效果（参见UCB算法的思想）。</p><p>不过这篇研究工作[6]发现 <img src="https://www.zhihu.com/equation?tex=%5Cmax" alt="\max" eeimg="1"/> 操作不仅在实验上能够观察到Q值明显的高估，而且得到的高估的Q值会损害DQN的性能。为什么高估的现象会一直存在？因为实验中存在一些不可避免的噪声，比如由于随机性环境带来的噪声、由于函数逼近带来的不确定性和算法不稳定性带来的不确定性，这些都很难避免，因此Q值估计上的噪声不可消灭，因此这样的高估会存在最后的结果中。为什么这样的高估会损害算法的性能？因为Q值的高估分布并不是均匀的，Q值的计算是bootstrap的，某个Q值由于随机性的高估，会导致后续很多步骤都基于此，由此形成正反馈导致不均匀的高估。为什么高估不会带来更好的探索呢？因为一旦形成一系列的高估，个体更可能反复去探索这个被高估的行动序列，反而不会去探索那些访问更少的行动。</p><p>解决方法很好理解， <img src="https://www.zhihu.com/equation?tex=%5Cmax" alt="\max" eeimg="1"/> 操作其实可以分为两步，一步是选取一个数值最大的元素，另一步是取出这个元素的数值，如果这两步都是用了同样的estimator，就会产生高估；解决方法就是把这两步分开并且使用不一样的estimators。反映到公式里面就是把目标换为了</p><p><img src="https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+arg+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+a%2C+%5Cmathbf%7Bw%7D_t%5E-%29+%2C+%5Cmathbf%7Bw%7D_t%29" alt="G_t = R_{t+1} + \gamma \widehat{q}(S_{t+1}, arg \max_a \widehat{q}(S_{t+1}, a, \mathbf{w}_t^-) , \mathbf{w}_t)" eeimg="1"/> </p><p>而在迭代中，参数 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_t%5E-" alt="\mathbf{w}_t^-" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_t" alt="\mathbf{w}_t" eeimg="1"/> 隔一段时间就交替位置，轮流更新；这样就产生了两个训练方法相同，但是不完全相同的Q-network。故此把这种方法叫做double Q-learning。</p><p><b>Prioritized Experience Replay</b></p><p>在最原始的DQN中，对于经验池中经验的采样是均匀分布的，这样的做法显然不是最优的，如果我们能够更多地采样那些能够帮助我们更好、更多地学习的样本，就能够提高数据利用的效率，加快学习的速度。因此，DeepMind这篇文章[7]的大致思路就是维护一个优先队列，每次把经验加入经验池中的时候都对这个经验有一个评分，从经验池中选取数据的时候根据这个评分来更多地选取评分高的经验。当然，梯度的更新是基于采样出来的这个mini-batch的，如果采样不是均匀的，可能导致得到梯度的期望不再是准确的梯度，因此，在此基础上还需要对采样出来的每个经验进行importance ratio的补偿。下面我们就分别对这三个部分进行说明。</p><p>1. 如何对不同的经验进行评分？大体的思想是，如果一个经验能够使得agent学习到更多的东西，那么我们就希望给该经验更高的评分。那么究竟如何量化呢？在这篇文章中使用了TD误差的绝对值来作为每个经验是否更有用的衡量标准，用TD误差的好处是在Q-Learning里面这个误差本来就有，容易获取，同时TD误差的绝对值越大，越表明这个经验是出乎意料的，权重在该经验上面的更新就会越大，即agent学习到的量就会更多。当然，也可以使用其他的衡量标准。比如使用权重更新前后TD误差的变化，或者该经验造成的权重更新的大小；使用这两种衡量标准的动机是有些经验虽然有很大的TD误差，但是在特定的函数逼近下，这个误差没法被消除，如果总是强调这样的样本，反而浪费了模型学习的时间。另外的衡量标准还包括赋予正TD误差比负TD误差更多的重视、对于表现好的轨迹上所有的经验都更加重视等。</p><p>2. 如何从经验池中选择经验进行梯度的估计？一个简单的想法就是，既然有了评分，就选择评分最高的经验。但是这样的做法有两方面的问题，一方面来说TD误差的衡量方式并不能完全代表该经验的价值，随着权值的改变，一开始TD误差小的样本可能现在当前权值下变得很大，或者由于环境的随机性该经验本身可能有学习的价值，但是存进来的时候偶然地TD误差较小；另一方面来说，权值的更新是较慢的，可能TD误差大的样本经过若干次更新其TD误差还是较大，最后可能是翻来覆去在学习同一拨经验。为了避免这样的问题，该文采取了随机采样的方式来从经验池中选择经验，即根据每个经验的TD误差，计算一个其采样的概率，然后依照此概率进行采样。有两种概率形式，分别是</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++P%28i%29+%26+%3D+%5Cdfrac%7Bp_i%5E%5Calpha%7D%7B%5Csum_k+p_k%5E%5Calpha%7D+%5Ctext%7B++where+%7D+p_i+%3D+%7C%5Cdelta_i%7C+%2B+%5Cepsilon+%5C%5C+P%28i%29+%26%3D+%5Cdfrac%7B1%7D%7Brank%28i%29%7D+%5Cend%7Baligned%7D" alt="\begin{aligned}  P(i) &amp; = \dfrac{p_i^\alpha}{\sum_k p_k^\alpha} \text{  where } p_i = |\delta_i| + \epsilon \\ P(i) &amp;= \dfrac{1}{rank(i)} \end{aligned}" eeimg="1"/> </p><p>虽然后一种在分布上更加长尾，探索可能更好，但是由于它忽略了绝对数值关系，两者的实验效果差不多。</p><p>3. 有侧重的采样之后如何使得最后的梯度无偏？我们还需要对每个采样出来的样本乘上importance ratio用来补偿不同样本被采样频率的不同，补偿的方法就是乘上 <img src="https://www.zhihu.com/equation?tex=w_i+%3D+%5Cleft%28%5Cdfrac%7B1%7D%7BN%7D%5Cdfrac%7B1%7D%7Bp%28i%29%7D%5Cright%29%5E%5Cbeta" alt="w_i = \left(\dfrac{1}{N}\dfrac{1}{p(i)}\right)^\beta" eeimg="1"/> ，当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3D1" alt="\beta=1" eeimg="1"/> 的时候就可以完全补偿，产生无偏的梯度。做importance ratio另外一个顺带的好处就是，TD误差大的原本会在权值空间上产生更大的步长，这容易导致算法的不稳定；而importance ratio使得这些TD误差大的样本每次对权值的更新步长更小了；这样“小步多次”的更新方式使得算法更加稳定。</p><p><b>Dueling Network Architecture</b></p><p>在原始的DQN中，使用的是在深度学习中常见的网络结构来作为Q值的函数逼近，其输入端是状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 的特征表示，对应 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C" alt="|\mathcal{A}|" eeimg="1"/> 个输出端，每个输出端代表的是相应行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 的Q值 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 。网络是从输入到输出的CNN神经网络。那么有没有其他的神经网络结构更适合用于Q值的函数逼近呢？</p><p>该篇文章[8]就对此进行了探索，提出了下图所示的神经网络结构。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-16994bf377c62a584dff513fa34669fb_b.jpg" data-caption="" data-size="normal" data-rawwidth="850" data-rawheight="632" class="origin_image zh-lightbox-thumb" width="850" data-original="https://pic4.zhimg.com/v2-16994bf377c62a584dff513fa34669fb_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;850&#39; height=&#39;632&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="850" data-rawheight="632" class="origin_image zh-lightbox-thumb lazy" width="850" data-original="https://pic4.zhimg.com/v2-16994bf377c62a584dff513fa34669fb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-16994bf377c62a584dff513fa34669fb_b.jpg"/></figure><p>该图上方显示的是在DQN中使用的网络结构，原始的状态特征 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> （Atari游戏的视频截图）经过若干层CNN之后通过Flatten层，最后通过一层全连接层Dense变为长度为 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C" alt="|\mathcal{A}|" eeimg="1"/> 的向量。其中每个位置的数值就代表行动价值函数 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 。该图下方是该文提出的dueling网络结构。该网络结构主要利用了行动状态函数 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 和状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 之间的一个常用的关系，把行动价值函数 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 拆分为了状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 和Advantage <img src="https://www.zhihu.com/equation?tex=A%28s%2Ca%29" alt="A(s,a)" eeimg="1"/> 的和。即， <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29+%3D+V%28s%29+%2B+A%28s%2Ca%29" alt="Q(s,a) = V(s) + A(s,a)" eeimg="1"/> 。</p><p>我们先介绍其做法，再来描述这样做的优势。</p><p>这样网络的结构在前面还是跟传统的一样，但是在Flatten层之后，该层向量被复制分别送到了上下两路，上面一路通过全连接层变为了一个标量，该标量代表相应的状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> ，下面一路通过另外一个全连接层变成了长度为 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C" alt="|\mathcal{A}|" eeimg="1"/> 的向量，其中每一位的数值代表Advantage <img src="https://www.zhihu.com/equation?tex=A%28s%2Ca%29" alt="A(s,a)" eeimg="1"/>，最后 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 和相应的 <img src="https://www.zhihu.com/equation?tex=A%28s%2Ca%29" alt="A(s,a)" eeimg="1"/> 相加就可以得到最后的 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 。但是很快就会发现一个问题，学习的目标是 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> ，把 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 上面加上一个常数，再从 <img src="https://www.zhihu.com/equation?tex=A%28s%2Ca%29" alt="A(s,a)" eeimg="1"/> 上面减去相同的一个常数并不影响结果，在训练的过程中，我们并没法保证这个常数如何在状态价值函数和Advantage之间分配。换个角度来说，一个 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C" alt="|\mathcal{A}|" eeimg="1"/> 维的目标 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 并不能完全决定一个 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C%2B1" alt="|\mathcal{A}|+1" eeimg="1"/> 维的系统。因此，我们还必须找出他们之间的一个约束关系。</p><p>在定义上，有如下关系</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bfor+stochastic+policy+%7D+%5Cbegin%7Bcases%7D+V%5E%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%28s%29%7D%5BQ%5E%5Cpi%28s%2Ca%29%5D+%26+%5C%5C+%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%28s%29%7D%5BA%5E%5Cpi+%28s%2Ca%29%5D+%3D+0+%26+%5C%5C+%5Cend%7Bcases%7D+%5Ctext%7Bfor+deterministic+policy+%7D+%5Cbegin%7Bcases%7D+V%28s%29+%3D+Q%28s%2Ca%5E%2A%29+%26+%5C%5C+A+%28s%2Ca%5E%2A%29+%3D+0+%26+%5C%5C+%5Cend%7Bcases%7D" alt="\text{for stochastic policy } \begin{cases} V^\pi(s) = \mathbb{E}_{a\sim \pi(s)}[Q^\pi(s,a)] &amp; \\ \mathbb{E}_{a\sim \pi(s)}[A^\pi (s,a)] = 0 &amp; \\ \end{cases} \text{for deterministic policy } \begin{cases} V(s) = Q(s,a^*) &amp; \\ A (s,a^*) = 0 &amp; \\ \end{cases}" eeimg="1"/> </p><p>由此我们可以写出一个它们之间的约束关系</p><p><img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%3B%5Ctheta%2C%5Calpha%2C%5Cbeta%29+%3D+V%28s%3B%5Ctheta%2C%5Cbeta%29+%2B+%28A%28s%2Ca%3B%5Ctheta%2C%5Calpha%29+-+%5Cmax_%7Ba%27%5Cin+%7C%5Cmathcal%7BA%7D%7C%7D+A%28s%2Ca%27%3B%5Ctheta%2C%5Calpha%29" alt="Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + (A(s,a;\theta,\alpha) - \max_{a&#39;\in |\mathcal{A}|} A(s,a&#39;;\theta,\alpha)" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 是网络共有部分的参数， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"/> 是Advantage部分的网络参数， <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta" eeimg="1"/> 是状态价值函数部分的网络参数。我们把表达式中右边括号里面的数值作为Advantage部分的目标数值，剩下的部分就是状态价值函数的目标数值。</p><p>但是一般来说，含有 <img src="https://www.zhihu.com/equation?tex=%5Cmax" alt="\max" eeimg="1"/> 常常会导致算法不稳定，一个常见的想法就是换成softmax，本文的作者也这么做了，但是最后还是觉得不如直接换成减去一个均值。其坏处是这样得到的<img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 和原本的定义就会有一个gap了，不过我们关心的是Advantage部分的相对大小，所以这个gap也不是很影响。于是有</p><p><img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%3B%5Ctheta%2C%5Calpha%2C%5Cbeta%29+%3D+V%28s%3B%5Ctheta%2C%5Cbeta%29+%2B+%28A%28s%2Ca%3B%5Ctheta%2C%5Calpha%29+-+%5Cdfrac%7B1%7D%7B%7C%5Cmathcal%7BA%7D%7C%7D+%5Csum_%7Ba%27%7D+A%28s%2Ca%27%3B%5Ctheta%2C%5Calpha%29" alt="Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + (A(s,a;\theta,\alpha) - \dfrac{1}{|\mathcal{A}|} \sum_{a&#39;} A(s,a&#39;;\theta,\alpha)" eeimg="1"/> </p><p>有了这个之后，网络的训练还是照常进行，只不过原来是一个 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C" alt="|\mathcal{A}|" eeimg="1"/> 维的目标，现在变成了 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BA%7D%7C%2B1" alt="|\mathcal{A}|+1" eeimg="1"/> 维的目标，仍然用BP来训练网络。</p><p>了解完了其做法，那么自然会有一个疑问，为什么这样做会更好？这样做是为了解决什么样的问题？这样做有如下几个优势：</p><ol><li>在某些状态上，采取任何一种行动的效果都差不多，我们不需要对于每个状态-行动对都去努力学习Q值。文中举了一个例子，在开赛车的游戏里面，如果前路笔直宽阔并且没有障碍物，那么left, right, no-op这些操作基本上没什么差异，这时候如果有一个V值统一的描述一下该状态就可以了。</li><li>在传统的训练中，我们把一个 <img src="https://www.zhihu.com/equation?tex=%28s%2Ca%29" alt="(s,a)" eeimg="1"/> 对看做一个样本，该样本来了只考虑该样本 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 带给网络参数的影响；而现在的这种结构下，任何一个 <img src="https://www.zhihu.com/equation?tex=%28s%2C%5Ccdot%29" alt="(s,\cdot)" eeimg="1"/> 来了都能促成对 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 的学习，从而影响别的 <img src="https://www.zhihu.com/equation?tex=%28s%2Ca%27%29" alt="(s,a&#39;)" eeimg="1"/> 上的数值；这样对于数据的利用就更为充分。</li><li>在训练中观察的很多情况下，同一个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 下不同行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 对应的Q值相互之间的差值相对于其绝对数值来讲小很多，这样，一点点的噪声将会对各个行动之间的排序造成很大的影响，进而影响到策略；把Q值中相同的部分扣除掉之后，会大大减轻这种问题。</li></ol><h2><b>总结</b></h2><p>我们这一讲引入了强化学习最为精髓的思想——时间差分算法，在此基础上我们对其进行了拓展。在状态空间和行动空间变得较大的时候，我们就需要开始使用函数逼近技术；考虑到和环境的交互，我们更希望是“边做边思考”，为了这样的计算便捷性的目标我们可以使用Eligibility Trace的技术；同时为了应对更加庞大而复杂的状态空间，我们考虑引入神经网络来对价值函数进行拟合，这就形成了目前一个非常强大的算法DQN；DQN发明以来就引起了强化学习社区的极大关注，对于其的改进层出不穷，我们选取了其中最为重要的几个改进加以介绍。</p><p>简单总结一下。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-5796f1635f8e77ab7ac440b52c905c6b_b.png" data-caption="" data-size="normal" data-rawwidth="885" data-rawheight="123" class="origin_image zh-lightbox-thumb" width="885" data-original="https://pic4.zhimg.com/v2-5796f1635f8e77ab7ac440b52c905c6b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;885&#39; height=&#39;123&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="885" data-rawheight="123" class="origin_image zh-lightbox-thumb lazy" width="885" data-original="https://pic4.zhimg.com/v2-5796f1635f8e77ab7ac440b52c905c6b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-5796f1635f8e77ab7ac440b52c905c6b_b.png"/></figure><h2><b>参考文献</b></h2><p>[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. Vol. 1. No. 1. Cambridge: MIT press, 1998. </p><p>[2] Sutton, Richard S. &#34;True Online Emphatic TD ($\lambda $): Quick Reference and Implementation Guide.&#34; arXiv preprint arXiv:1507.07147 (2015). </p><p>[3] Mahmood, A. Rupam, et al. &#34;Emphatic temporal-difference learning.&#34; arXiv preprint arXiv:1507.01569 (2015).</p><p>[4] Mnih, Volodymyr, et al. &#34;Playing atari with deep reinforcement learning.&#34; arXiv preprint arXiv:1312.5602 (2013).</p><p>[5] Mnih, Volodymyr, et al. &#34;Human-level control through deep reinforcement learning.&#34; Nature 518.7540 (2015): 529.</p><p>[6] Van Hasselt, Hado, Arthur Guez, and David Silver. &#34;Deep Reinforcement Learning with Double Q-Learning.&#34; AAAI. Vol. 2. 2016.</p><p>[7] Schaul, Tom, et al. &#34;Prioritized experience replay.&#34; arXiv preprint arXiv:1511.05952 (2015).</p><p>[8] Wang, Ziyu, et al. &#34;Dueling network architectures for deep reinforcement learning.&#34; arXiv preprint arXiv:1511.06581 (2015).</p><p></p></div></div><div class="ContentItem-time">编辑于 2019-04-03</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 15 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 15</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>3 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="7685eec7-279c-4b09-84ae-6982137c5bdc" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="7685eec7-279c-4b09-84ae-6982137c5bdc">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"56058343":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":56058343,"title":"【强化学习入门 2】强化学习策略迭代类方法","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56058343","imageUrl":"","titleImage":"","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-03f51b4f4dccc4461207a89df8900838_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"665\" data-watermark=\"watermark\" data-original-src=\"v2-03f51b4f4dccc4461207a89df8900838\" data-watermark-src=\"v2-ed8b1bb0a0679660b5881e4b3944c779\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-03f51b4f4dccc4461207a89df8900838_r.jpg\"\u002F\u003E\u003Cb\u003E目录\u003C\u002Fb\u003E初识时间差分方法 —— TD(0)从少量状态到数不清的状态 —— 函数逼近技术从TD(0)到MC的过渡 —— TD(\\lambda)用强化学习来玩游戏 —— DQN\u003Cb\u003E引言\u003C\u002Fb\u003E强化学习的目标是找到能够最大化收益的策略，其中一类重要的方法是策略迭代类的方法。这类方法的的主要特征…","created":1548868652,"updated":1554301309,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":0,"imageHeight":0,"content":"\u003Ch2\u003E\u003Cb\u003E目录\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli\u003E初识时间差分方法 —— TD(0)\u003C\u002Fli\u003E\u003Cli\u003E从少量状态到数不清的状态 —— 函数逼近技术\u003C\u002Fli\u003E\u003Cli\u003E从TD(0)到MC的过渡 —— TD(\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E)\u003C\u002Fli\u003E\u003Cli\u003E用强化学习来玩游戏 —— DQN\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E引言\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E强化学习的目标是找到能够最大化收益的策略，其中一类重要的方法是策略迭代类的方法。这类方法的的主要特征是采用了Sutton的强化学习书中提到的GPI（generalized policy iteration）框架。这类方法都包含两个步骤的交替进行，在策略改进步（policy improvment）中，根据学到的价值函数产生这个价值函数下较好的策略；在策略评价步（policy evaluation）中，估计新产生策略对应的价值函数。前一讲中的Modified Policy Iteration算法是这类方法的一个草图。\u003C\u002Fp\u003E\u003Cp\u003E在这一讲中，主要介绍\u003Cb\u003E\u003Ci\u003E时间差分方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（temporal-different method, TD），它结合了MC方法model-free的特点和DP方法bootstrap的特点。我们先介绍on-policy和off-policy情形下的TD(0)算法。接下来我们会拓展到状态空间、行动空间连续（或者指数级多）的情形，在这种情形下每个状态的价值函数不再是单独储存而是用函数去对其拟合。我们还将介绍 TD(\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E)算法，通过它可以自然地从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=TD%280%29\" alt=\"TD(0)\" eeimg=\"1\"\u002F\u003E 过渡到MC方法。最后我们介绍策略迭代类方法里面十分具有影响力的一篇工作DQN。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E初识时间差分方法 —— TD(0)\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E回顾前面的两种方法，DP方法需要直接使用对于环境的建模（即，需要使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s%27%2Cr%7Cs%2Ca%29\" alt=\"p(s&#39;,r|s,a)\" eeimg=\"1\"\u002F\u003E ），而MC方法可以直接从与环境的交互经历中间来学习，它不需要知道对于环境的建模或者估计一个关于环境的模型（即，不需要使用或者估计 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s%27%2Cr%7Cs%2Ca%29\" alt=\"p(s&#39;,r|s,a)\" eeimg=\"1\"\u002F\u003E ）。对于类似DP这样需要对环境建模的方法我们称之为\u003Cb\u003E\u003Ci\u003Emodel-based\u003C\u002Fi\u003E\u003C\u002Fb\u003E方法；对于类似MC这样不需要对环境建模的方法我们称之为\u003Cb\u003E\u003Ci\u003Emodel-free\u003C\u002Fi\u003E\u003C\u002Fb\u003E方法。同时，我们还注意到，在DP方法中，对于某个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 的价值函数的估计需要使用到对于其后续状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s%27\" alt=\"s&#39;\" eeimg=\"1\"\u002F\u003E 的价值函数估计值；而在MC方法中，对于某个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 的价值函数的估计来自于一整条抵达终止状态的链的实际收益，它不依赖其他状态的价值函数估计值。对于像DP这种对于一个状态价值函数估计的更新需要依赖其他状态价值函数估计的性质，我们称之为\u003Cb\u003E\u003Ci\u003Ebootstrap\u003C\u002Fi\u003E\u003C\u002Fb\u003E。这里介绍的TD方法既像MC一样有model-free的特点，也像DP一样有bootstrap的性质。\u003C\u002Fp\u003E\u003Cp\u003E回顾DP方法，DP方法在更新某个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"\u002F\u003E 的价值函数估计值的时候需要根据状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"\u002F\u003E 后续所有可能状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_%7Bt%2B1%7D\" alt=\"S_{t+1}\" eeimg=\"1\"\u002F\u003E 的价值函数估计值来更新，这种方式叫做\u003Cb\u003E\u003Ci\u003Efull backup\u003C\u002Fi\u003E\u003C\u002Fb\u003E。MC方法在更新某个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"\u002F\u003E 的价值函数估计值的时候，只需要对于后续状态进行采样，使用一个后续状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_%7Bt%2B1%7D\" alt=\"S_{t+1}\" eeimg=\"1\"\u002F\u003E 的价值函数估计值来更新即可，这种方式叫做\u003Cb\u003E\u003Ci\u003Esample backup\u003C\u002Fi\u003E\u003C\u002Fb\u003E。考虑知道了当前的状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"\u002F\u003E 和目前该状态的价值函数估计值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28S_t%29\" alt=\"V(S_t)\" eeimg=\"1\"\u002F\u003E ，在通过策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 行动之后到达了下一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_%7Bt%2B1%7D\" alt=\"S_{t+1}\" eeimg=\"1\"\u002F\u003E ，同时取得了短期的奖励 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_%7Bt%2B1%7D\" alt=\"R_{t+1}\" eeimg=\"1\"\u002F\u003E 。不难推出，相比于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28S_t%29\" alt=\"V(S_t)\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29\" alt=\"R_{t+1} + \\gamma V(S_{t+1})\" eeimg=\"1\"\u002F\u003E 是一个更好的估计值，因此在每一步中，我们都把估计值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28S_t%29\" alt=\"V(S_t)\" eeimg=\"1\"\u002F\u003E 往这个更好的方向更新一点，因此我们可以得到\u003Cb\u003E\u003Ci\u003ETD误差\u003C\u002Fi\u003E\u003C\u002Fb\u003E（TD error）和价值函数更新公式。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29+-+V%28S_t%29\" alt=\"\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28S_t%29+%5Cleftarrow+V%28S_t%29+%2B+%5Calpha+%5Cdelta_t\" alt=\"V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E与MC方法类似，TD方法也有on-policy和off-policy的版本，这里先给出他们各自的算法框图。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%B8%80%EF%BC%9ASARSA%3A+on-policy+TD+Method%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+Q%28%5Ctext%7Bterminal-state%7D%2C+%5Ccdot%29+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Ctext%7Brepeat+for+each+episode%3A%7D+%5C%5C+5%5Cquad+%26+%5Cquad+%5Ctext%7Binitialize+%7D+S+%5C%5C+6%5Cquad+%26+%5Cquad+%5Ctext%7Bchoose+%7DA%5Ctext%7B+from+%7DS%5Ctext%7B+using+policy+derived+from+%7DQ%5Ctext%7B+%28e.g.+%7D%5Cepsilon%5Ctext%7B-greedy%29%7D+%5C%5C+7%5Cquad+%26+%5Cquad+%5Ctext%7Brepeat+%28for+each+step+of+episode%29%3A%7D+%5C%5C+8%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Btack+action+%7DA%5Ctext%7B+observe+%7DR%2C+S%27+%5C%5C+9%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bchoose+%7DA%27%5Ctext%7B+from+%7D+S%27+%5Ctext%7B+using+policy+derived+from+%7D+Q%5Ctext%7B+%28e.g.+%7D%5Cepsilon%5Ctext%7B-greedy%29%7D%5C%5C+10%5Cquad+%26+%5Cquad+%5Cquad+Q%28S%2C+A%29+%5Cleftarrow+Q%28S%2C+A%29+%2B+%5Calpha+%5BR+%2B+%5Cgamma+Q%28S%27%2C+A%27%29+-+Q%28S%2C+A%29%5D+%5C%5C+11%5Cquad+%26+%5Cquad+%5Cquad+S+%5Cleftarrow+S%27%2C+A+%5Cleftarrow+A%27+%5C%5C+12%5Cquad+%26+%5Cquad+%5Ctext%7Buntil+%7D+S+%5Ctext%7B+is+terminated%7D+%5C%5C+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法一：SARSA: on-policy TD Method} \\\\ 1\\quad &amp; \\text{initialize } \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\\\ 2\\quad &amp; \\quad Q(s, a) \\leftarrow \\text{arbitrary}  \\\\ 3\\quad &amp; \\quad Q(\\text{terminal-state}, \\cdot) \\leftarrow 0 \\\\ 4\\quad &amp; \\text{repeat for each episode:} \\\\ 5\\quad &amp; \\quad \\text{initialize } S \\\\ 6\\quad &amp; \\quad \\text{choose }A\\text{ from }S\\text{ using policy derived from }Q\\text{ (e.g. }\\epsilon\\text{-greedy)} \\\\ 7\\quad &amp; \\quad \\text{repeat (for each step of episode):} \\\\ 8\\quad &amp; \\quad \\quad \\text{tack action }A\\text{ observe }R, S&#39; \\\\ 9\\quad &amp; \\quad \\quad \\text{choose }A&#39;\\text{ from } S&#39; \\text{ using policy derived from } Q\\text{ (e.g. }\\epsilon\\text{-greedy)}\\\\ 10\\quad &amp; \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma Q(S&#39;, A&#39;) - Q(S, A)] \\\\ 11\\quad &amp; \\quad \\quad S \\leftarrow S&#39;, A \\leftarrow A&#39; \\\\ 12\\quad &amp; \\quad \\text{until } S \\text{ is terminated} \\\\ \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%BA%8C%EF%BC%9AQ-learning%3A+off-policy+TD+Method%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+Q%28%5Ctext%7Bterminal-state%7D%2C+%5Ccdot%29+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Ctext%7Brepeat+for+each+episode%3A%7D+%5C%5C+5%5Cquad+%26+%5Cquad+%5Ctext%7Binitialize+%7D+S+%5C%5C++6%5Cquad+%26+%5Cquad+%5Ctext%7Brepeat+%28for+each+step+of+episode%29%3A%7D+%5C%5C+7%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bchoose+%7DA%5Ctext%7B+from+%7DS%5Ctext%7B+using+policy+derived+from+%7DQ%5Ctext%7B+%28e.g.+%7D%5Cepsilon%5Ctext%7B-greedy%29%7D+%5C%5C+8%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Btack+action+%7DA%5Ctext%7B+observe+%7DR%2C+S%27+%5C%5C+9%5Cquad+%26+%5Cquad+%5Cquad+Q%28S%2C+A%29+%5Cleftarrow+Q%28S%2C+A%29+%2B+%5Calpha+%5BR+%2B+%5Cgamma+%5Cmax_a+Q%28S%27%2C+a%29+-+Q%28S%2C+A%29%5D+%5C%5C+10+%5Cquad+%26+%5Cquad+%5Cquad+S+%5Cleftarrow+S%27+%5C%5C+11%5Cquad+%26+%5Cquad+%5Ctext%7Buntil+%7D+S+%5Ctext%7B+is+terminated%7D+%5C%5C+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法二：Q-learning: off-policy TD Method} \\\\ 1\\quad &amp; \\text{initialize } \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\\\ 2\\quad &amp; \\quad Q(s, a) \\leftarrow \\text{arbitrary}  \\\\ 3\\quad &amp; \\quad Q(\\text{terminal-state}, \\cdot) \\leftarrow 0 \\\\ 4\\quad &amp; \\text{repeat for each episode:} \\\\ 5\\quad &amp; \\quad \\text{initialize } S \\\\  6\\quad &amp; \\quad \\text{repeat (for each step of episode):} \\\\ 7\\quad &amp; \\quad \\quad \\text{choose }A\\text{ from }S\\text{ using policy derived from }Q\\text{ (e.g. }\\epsilon\\text{-greedy)} \\\\ 8\\quad &amp; \\quad \\quad \\text{tack action }A\\text{ observe }R, S&#39; \\\\ 9\\quad &amp; \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_a Q(S&#39;, a) - Q(S, A)] \\\\ 10 \\quad &amp; \\quad \\quad S \\leftarrow S&#39; \\\\ 11\\quad &amp; \\quad \\text{until } S \\text{ is terminated} \\\\ \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E可以看到，在TD方法中，个体和环境的每产生一次交互，程序都会更新相应的价值函数。在SARSA（on-policy方法）中，目标策略和行动策略都是关于行动价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy策略；在Q-learning（off-policy方法）中，行动策略是关于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy策略，而目标策略是关于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 的greedy策略。\u003C\u002Fp\u003E\u003Cp\u003E这里可以总结出TD方法的几个优势。首先，如同MC一样，它不需要关于环境的建模，是一种model-free的方法。其次，它在和环境的每一步交互过后都进行相应的计算，是一种完全的增量形式；与之相反的是MC方法，它需要在一个回合结束过后集中进行运算，在很多应用场景中，这是一个比较低效的方式。另外，对于有限状态的情形下（状态数目不多，以至于能够使用离散的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28S%29\" alt=\"V(S)\" eeimg=\"1\"\u002F\u003E 或者 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28S%2CA%29\" alt=\"Q(S,A)\" eeimg=\"1\"\u002F\u003E 存储），可以证明算法的收敛性。最后，在经验上TD算法的收敛速度在很多情况下比MC和DP方法更快。\u003C\u002Fp\u003E\u003Cp\u003E为了区别于后面会讲到的另一类TD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )算法，我们将这种只往后看一步的方法称作TD(0)。\u003C\u002Fp\u003E\u003Cp\u003E最后引用一下Sutton书中的一句话来说明TD算法思想在强化学习领域的重要性。\u003C\u002Fp\u003E\u003Cblockquote\u003EIf one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning.\u003C\u002Fblockquote\u003E\u003Ch2\u003E\u003Cb\u003E从少量状态到数不清的状态 —— 函数逼近技术\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E至此，我们对于状态价值函数和行动价值函数的表示都是针对不同的状态分别储存一个数值，并且把它们当做不同的情形来更新的。这种方式我们称之为\u003Cb\u003E\u003Ci\u003E表格方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（tabular solution method），因为我们对于每个状态都创建了一个价值函数，在价值函数这个表格的不同格子上更新相应的价值函数。但是在很多情形下，状态可能是连续分布的，这就导致状态数目可能是无穷多的，比如一条线段上的的位置；在另外一些情形下，状态可能是通过排列组合产生的，比如100个不同颜色小球的排列，这样的情形可能产生巨大的状态数目。当我们对每个状态都采用一个格子来储存他们的价值函数时，要么遍历一遍所有的状态会消耗巨大的计算资源（比如，DP算法中需要对于某个状态的后续所有可能的状态遍历），要么在很长的计算时间内，总有大量的状态一次都不能被更新到（比如，MC或者TD算法中虽然并不要求每次迭代遍历所有的状态，但是有些状态一次都不能被访问到会导致算法性能大幅下降）。\u003C\u002Fp\u003E\u003Cp\u003E当我们遇到无穷或者庞大的状态集的时候，我们需要采取\u003Cb\u003E\u003Ci\u003E近似方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（approximate solution method）。我们通常使用一组参数来控制相应的状态价值函数（如果是行动状态函数也是类似的处理方法），我们把状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v%28s%29\" alt=\"v(s)\" eeimg=\"1\"\u002F\u003E 写成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v+%28s%2C+%5Cmathbf%7Bw%7D%29\" alt=\"v (s, \\mathbf{w})\" eeimg=\"1\"\u002F\u003E 的形式，它的具体形式可以根据不同任务而不同，比如最简单的也是我们下面讨论中默认的一种形式——线性近似（linear approximation） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v%28s%2C+%5Cmathbf%7Bw%7D%29+%3D+%5Cmathbf%7Bw%7D%5ET+%5Cmathbf%7Bx%7D%28s%29\" alt=\"v(s, \\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s)\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D\" alt=\"\\mathbf{w}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bs%7D\" alt=\"\\mathbf{s}\" eeimg=\"1\"\u002F\u003E 是维度相同的向量，最常见的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bx%7D%28s%29\" alt=\"\\mathbf{x}(s)\" eeimg=\"1\"\u002F\u003E 可以使one-hot形式的向量，表示该状态处于某个状态组中。因此当我们要更新状态价值函数的估计值时，我们不再对于某一个格子上的数值直接进行更新，而是对于参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D\" alt=\"\\mathbf{w}\" eeimg=\"1\"\u002F\u003E 进行更新。参数的更新公式可以通过对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28G_t+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%29%5E2\" alt=\"(G_t - v(S_t, \\mathbf{w}_t))^2\" eeimg=\"1\"\u002F\u003E 求导得到，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 是得到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v%28S_t%29\" alt=\"v(S_t)\" eeimg=\"1\"\u002F\u003E 的目标值。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BG_t+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_w+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [G_t - v(S_t, \\mathbf{w}_t)] \\nabla_w v(S_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E我们可以使用采样方法得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E ，此时 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 是无偏估计，因此上式的后一项是真实的梯度；我们还可以使用bootstrap的方法来得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E ，比如 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+r_t+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w_t%29\" alt=\"G_t = r_t + \\gamma v(S_{t+1}, w_t)\" eeimg=\"1\"\u002F\u003E ，但由于这个目标还依赖其他的价值函数值，而这些价值函数的值又同时依赖参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w_t\" alt=\"w_t\" eeimg=\"1\"\u002F\u003E ，为了计算简便我们不再考虑目标值中关于参数的梯度，这样的近似产生\u003Cb\u003E\u003Ci\u003E半梯度方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（semi-gradient method）。\u003C\u002Fp\u003E\u003Cp\u003E由此我们能够写出在半梯度方法下TD(0)的更新公式。可以证明它在线性近似和on-policy的情形下是可以收敛的。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_%7B%5Cmathbf%7Bw%7D%7D+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma v(S_{t+1}, \\mathbf{w}_t) - v(S_t, \\mathbf{w}_t)] \\nabla_{\\mathbf{w}} v(S_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E当然，这样的近似价值函数还可以使用更为复杂的形式来表示，比如使用决策树或者神经网络等。不过值得注意的是，在表格方法中很多算法都能够有较好的收敛保证，但是在函数逼近方法下很多算法都十分不稳定，尤其是当算法中同时包含\u003Cb\u003E\u003Ci\u003E函数逼近技术、bootstrap和off-policy\u003C\u002Fi\u003E\u003C\u002Fb\u003E时，算法在理论和实践上都很难保证稳定。而这三者都十分的重要，函数逼近技术让我们能够解决状态空间复杂的问题；bootstrap大幅提高的算法的效率；而off-policy让我们能够更好地控制算法的探索，这对于学习到一个好的策略十分重要。而这三者的结合却很容易导致算法的发散，Sutton在书中称它们为\u003Cb\u003E\u003Ci\u003EThe Deadly Triad\u003C\u002Fi\u003E\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E算法的收敛性保证、实践上算法的稳定性以及算法的效率是应用到函数逼近技术强化学习算法的核心问题。off-policy下算法的不稳定主要是由于1）行动策略得到目标和目标策略所需目标的不一致，这个问题可以通过前面提到的重要性采样率来解决；2）行动策略产生数据的分布和算法所需要的分布不一致，这个问题是目前比较困难的问题。为了应对在off-policy上不稳定的问题大致上有两类思路：第一类思路是去设计一种真实的梯度方法，而不是前面提到的半梯度方法，因为半梯度方法的收敛性依赖于数据的on-policy分布；第二类思路是得到行动策略采集到的数据之后，对其进行加权，使其分布看起来像一个on-policy策略得到的数据分布。有如下几种尝试解决的方法：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003EMC方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E：使用蒙特卡洛方法来更新公式 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BG_t+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_w+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [G_t - v(S_t, \\mathbf{w}_t)] \\nabla_w v(S_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E 里面的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E ，所得到的梯度是真实的梯度，能够在off-policy情形下保证稳定。但是这种做法的缺点在于效率低。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003ENaive Residual Gradient Algorithm\u003C\u002Fi\u003E\u003C\u002Fb\u003E：直接对Mean Squared TD Error  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BTDE%7D\" alt=\"\\overline{TDE}\" eeimg=\"1\"\u002F\u003E 求梯度，并且做梯度下降，其更新公式为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%28%5Cnabla_w+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+-%5Cgamma+%5Cnabla_w+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29%29\" alt=\" \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\delta_t (\\nabla_w v(S_t, \\mathbf{w}_t) -\\gamma \\nabla_w v(S_{t+1}, \\mathbf{w}_t))\" eeimg=\"1\"\u002F\u003E ，它相比于Semi-gradient TD(0)方法唯一的不同就是后面多出了减去的一项，之前的半梯度方法没有能够考虑到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"G_t = R_{t+1} + \\gamma v(S_{t+1}, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E 里面后一步估计产生的梯度，而这里就将这一部分梯度补上了，使其成为了一个真实的梯度。这个方法能够稳健的收敛，但是其问题在于最小化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BTDE%7D\" alt=\"\\overline{TDE}\" eeimg=\"1\"\u002F\u003E 之后收敛到的并不是正确的价值函数；换句话说，价值函数在其真实取值的时候对应的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BTDE%7D\" alt=\"\\overline{TDE}\" eeimg=\"1\"\u002F\u003E 并非最小。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003EResidual Gradient Algorithm\u003C\u002Fi\u003E\u003C\u002Fb\u003E：当价值函数在真实取值的时候，对应的Bellman error为零，\u003Ci\u003EBellman error是TD error的期望\u003C\u002Fi\u003E。因此我们对Bellman Error  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BBE%7D\" alt=\"\\overline{BE}\" eeimg=\"1\"\u002F\u003E 来求梯度，并且做梯度下降，这样得到的方法就是Residual Gradient Algorithm，其更新公式为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cleft%5B+%5Cmathbb%7BE%7D_b%5B%5Crho_t+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29%29%5D+-+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29%29+%5Cright%5D+%5Cleft%5B+%5Cnabla+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+-%5Cgamma+%5Cmathbb%7BE%7D_b%5B%5Crho_t+%5Cnabla+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cright%5D\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\left[ \\mathbb{E}_b[\\rho_t (R_{t+1} + \\gamma v(S_{t+1}, \\mathbf{w}_t))] - v(S_t, \\mathbf{w}_t)) \\right] \\left[ \\nabla v(S_t, \\mathbf{w}_t) -\\gamma \\mathbb{E}_b[\\rho_t \\nabla v(S_{t+1}, \\mathbf{w}_t)] \\right]\" eeimg=\"1\"\u002F\u003E 。需要注意的是这里面有两个期望，这两个期望里面的采样需要是独立的。这个方法的缺点在于：1）慢；2）仍然收敛到的可能不是正确的价值函数；3）最重要的在于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BBE%7D\" alt=\"\\overline{BE}\" eeimg=\"1\"\u002F\u003E 不是\u003Cb\u003E\u003Ci\u003E可学习\u003C\u002Fi\u003E\u003C\u002Fb\u003E（learnable）的。不同的MDP可以产生相同的数据（包括state、action、reward序列），但是它们却对应不同的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BBE%7D\" alt=\"\\overline{BE}\" eeimg=\"1\"\u002F\u003E ，因此我们无法单独从数据中学习到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BBE%7D\" alt=\"\\overline{BE}\" eeimg=\"1\"\u002F\u003E ，这种情况我们称之为不可学习的。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003EGradient-TD Method\u003C\u002Fi\u003E\u003C\u002Fb\u003E：前面给我们的启示就是我们需要找一个可学习的目标，然后对其优化。函数逼近技术中权值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D\" alt=\"\\mathbf{w}\" eeimg=\"1\"\u002F\u003E 的维度比状态数目一般要小，因此某种函数逼近方法只是在价值函数空间中的一个子空间，把Bellman Error投影到这个子空间上，得到的就是Projected Bellman Error \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BPBE%7D\" alt=\"\\overline{PBE}\" eeimg=\"1\"\u002F\u003E ，它是可学习的。对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BPBE%7D\" alt=\"\\overline{PBE}\" eeimg=\"1\"\u002F\u003E 做梯度，可以得到权值的更新公式（线性近似） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Crho_t+%28%5Cmathbf%7Bx%7D_t+-+%5Cgamma+%5Cmathbf%7Bx%7D_%7Bt%2B1%7D%29+%5Cmathbf%7Bx%7D_t%5ET+%5Cmathbf%7Bv%7D_t\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\rho_t (\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1}) \\mathbf{x}_t^T \\mathbf{v}_t\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bv%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bv%7D_t+%2B+%5Cbeta+%5Crho_t+%28%5Cdelta_t+-+%5Cmathbf%7Bv%7D_t%5ET+%5Cmathbf%7Bx%7D_t%29%5Cmathbf%7Bx%7D_t\" alt=\"\\mathbf{v}_{t+1} = \\mathbf{v}_t + \\beta \\rho_t (\\delta_t - \\mathbf{v}_t^T \\mathbf{x}_t)\\mathbf{x}_t\" eeimg=\"1\"\u002F\u003E ，增加了一个变量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bv%7D_t\" alt=\"\\mathbf{v}_t\" eeimg=\"1\"\u002F\u003E 是把公式中的一个Least Mean Square的解的形式换成了梯度下降形式。这一类方法是目前使用最为广泛的稳定off-policy方法。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003EEmphatic-TD Method\u003C\u002Fi\u003E\u003C\u002Fb\u003E [2,3]：前面的思路都是寻找一个好的优化目标并且求出其正确的梯度，这个方法使用了另一种思路，即使用行动策略进行采样之后，对于不同的状态进行不同的加权，使其看起来像是on-policy的分布。其权值更新公式为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+M_t+%5Crho_t+%5Cdelta_t+%5Cnabla+v%28S_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha M_t \\rho_t \\delta_t \\nabla v(S_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E ，这里的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=M_t\" alt=\"M_t\" eeimg=\"1\"\u002F\u003E 就是动态更新的对于某个状态的权重，把样本的等效分布调整为和on-policy分布一致。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E\u003Cb\u003E从TD(0)到MC的过渡 —— TD(\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E \u003Cb\u003E)\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E回顾前面的TD和MC方法，在TD方法中，每次都把当前的状态价值函数都往一个更为合理的状态价值函数估计值$ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t%5E%7B%280%29%7D+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7B%5Ctheta%7D%29\" alt=\"G_t^{(0)} = R_{t+1} + \\gamma v(S_{t+1}, \\mathbf{\\theta})\" eeimg=\"1\"\u002F\u003E 上更新；在MC方法中，我们每次实际上是将状态价值函数估计值往 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t%5E%7B%28T-t%29%7D+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%2B+%5Ccdots+%2B+%5Cgamma%5E%7BT-t%7D+R_T\" alt=\"G_t^{(T-t)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma^{T-t} R_T\" eeimg=\"1\"\u002F\u003E 上更新。我们其实可以定义 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E 步的TD算法，使得其目标收益估计为\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t%5E%7B%28n%29%7D+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%2B+%5Ccdots+%2B+%5Cgamma%5En+v%28S_%7Bt%2Bn%7D%2C+%5Cmathbf%7B%5Ctheta%7D%29%2C+0%5Cle+t%5Cle+T-n\" alt=\"G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma^n v(S_{t+n}, \\mathbf{\\theta}), 0\\le t\\le T-n\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E实际上对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t%5E%7B%28n%29%7D\" alt=\"G_t^{(n)}\" eeimg=\"1\"\u002F\u003E 的任意归一化的加权组合都能够形成可行复合收益估计，比如 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cfrac%7B1%7D%7B4%7DG_t%5E%7B%281%29%7D+%2B+%5Cfrac%7B3%7D%7B4%7DG_t%5E%7B%282%29%7D\" alt=\"\\frac{1}{4}G_t^{(1)} + \\frac{3}{4}G_t^{(2)}\" eeimg=\"1\"\u002F\u003E 。这里我们定义一种特殊的复合收益估计，称之为\u003Cb\u003E\u003Ci\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益\u003C\u002Fi\u003E\u003C\u002Fb\u003E（\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E-return）\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t%5E%5Clambda+%3D+%281-%5Clambda%29+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Clambda%5E%7Bn-1%7D+G_t%5E%7B%28n%29%7D\" alt=\"G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=0%5Cle%5Clambda%5Cle+1\" alt=\"0\\le\\lambda\\le 1\" eeimg=\"1\"\u002F\u003E ，根据 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E 取值的不同，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益对应的实际上是从MC方法到前述TD方法（也称TD(0)方法）的过渡。当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda+%3D+0\" alt=\"\\lambda = 0\" eeimg=\"1\"\u002F\u003E 时，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益对应的就是一步的TD方法；当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda+%3D+1\" alt=\"\\lambda = 1\" eeimg=\"1\"\u002F\u003E 时，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益对应的就是MC方法。\u003C\u002Fp\u003E\u003Cp\u003E至此我们已经看到了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益是TD(0)方法到MC方法的过渡，不过更为精彩的地方在于，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益的这种定义方式让我们能够更加方便地对于涉及到未来多步奖励的目标进行权重更新。考虑一串连续的动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t%2C+A_t%2C+R_%7Bt%2B1%7D%2C+S_%7Bt%2B1%7D%2C+A_%7Bt%2B1%7D%2C+%5Ccdots\" alt=\"S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\cdots\" eeimg=\"1\"\u002F\u003E ，如果我们希望对于未来 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E 步的奖励进行统计，然后更新权重，我们就必须等到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E 步之后才能得到完整的奖励序列之后再进行更新，而在此之前将无事可做；更进一步，如果 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n%5Cto+%5Cinfty\" alt=\"n\\to \\infty\" eeimg=\"1\"\u002F\u003E ，那么我们只能像MC方法一样，等到回合结束之后再回过头来更新权重。那有没有什么方法让我们每一步都做尽可能的更新呢？这样权重每一步都在更新，并且最新的信息都能被及时利用。这就要引出eligibility trace技术了。\u003C\u002Fp\u003E\u003Cp\u003E这种技术可以将运算量从集中地在每次回合结束时进行变为分散在每一步之中进行，大致思想是将“往后看”变为“往前看”，\u003Ci\u003E不像之前那样认为更新这一步需要后 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E 步的奖励，现在认为这一步刚刚得到的奖励马上可以用于更新前 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E 步内所有的权重\u003C\u002Fi\u003E。定义eligibility trace  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Be%7D\" alt=\"\\mathbf{e}\" eeimg=\"1\"\u002F\u003E ，在函数逼近技术下，它是与权值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D\" alt=\"\\mathbf{w}\" eeimg=\"1\"\u002F\u003E 维度相同的向量，它记录了历史中每一步的所经历的状态的累积，通过更新它，可以使我们能够先记录走过的路径，在最后得到相应的奖励之后，再往正确的方向更新。换句话说，权重 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D\" alt=\"\\mathbf{w}\" eeimg=\"1\"\u002F\u003E 是对于到此为止所有经历的长期记忆，而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Be%7D\" alt=\"\\mathbf{e}\" eeimg=\"1\"\u002F\u003E 是对于最近几步的短期记忆。\u003C\u002Fp\u003E\u003Cp\u003E我们这里举一个最为简单的例子来说明eligibility trace技术是如何应用的。\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Be%7D\" alt=\"\\mathbf{e}\" eeimg=\"1\"\u002F\u003E 初值为0，每次都会累积一个当前状态下价值函数相对于权重的梯度，即\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Cmathbf%7Be%7D_%7B-1%7D+%3D+%5Cmathbf%7B0%7D+%5C%5C+%26+%5Cmathbf%7Be%7D_t+%3D+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bv%7D+%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\mathbf{e}_{-1} = \\mathbf{0} \\\\ &amp; \\mathbf{e}_t = \\gamma \\lambda \\mathbf{e}_{t-1} + \\nabla \\widehat{v} (S_t, \\mathbf{w}_t) \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E每一步最新的TD误差为\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bv%7D%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bv%7D%28S_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\delta_t = R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\mathbf{w}_t) - \\widehat{v}(S_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E权重的更新就累加TD误差和eligibility trace的乘积，\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%5Cmathbf%7Be%7D_t\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{e}_t\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E这样得到是的\u003Cb\u003E\u003Ci\u003Esemi-gradient version TD(\u003C\u002Fi\u003E\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E \u003Cb\u003E\u003Ci\u003E)算法\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Cmathbf%7Be%7D_%7B-1%7D+%3D+%5Cmathbf%7B0%7D+%5C%5C+%26+%5Cmathbf%7Be%7D_t+%3D+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bv%7D+%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5Cquad+%5Ctext%7B%28accumulating+trace%29%7D%5C%5C+%26+%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bv%7D%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bv%7D%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5C%5C+%26+%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%5Cmathbf%7Be%7D_t+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\mathbf{e}_{-1} = \\mathbf{0} \\\\ &amp; \\mathbf{e}_t = \\gamma \\lambda \\mathbf{e}_{t-1} + \\nabla \\widehat{v} (S_t, \\mathbf{w}_t) \\quad \\text{(accumulating trace)}\\\\ &amp; \\delta_t = R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\mathbf{w}_t) - \\widehat{v}(S_t, \\mathbf{w}_t) \\\\ &amp; \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{e}_t \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E这个算法是一个online的算法，但是这个算法对于offline版本 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益的模拟不是很好，另外一种更逼近offline版本 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E -收益的算法是\u003Cb\u003E\u003Ci\u003ETrue Online TD( \u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E \u003Cb\u003E\u003Ci\u003E)算法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（with linear function approximation），这里的“true online”指的是它是对于一个offline目标的online实现。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Cmathbf%7Be%7D_%7B-1%7D+%3D+%5Cmathbf%7B0%7D+%5C%5C+%26+%5Cmathbf%7Be%7D_t+%3D+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%281-%5Calpha+%5Cgamma+%5Clambda+%5Cmathbf%7Be%7D_%7Bt-1%7D%5ET+%5Cmathbf%7Bx%7D_t%29+%5Cmathbf%7Bx%7D_t+%5Cquad+%5Ctext%7B%28Dutch+trace%29%7D%5C%5C+%26+%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bv%7D%28S_%7Bt%2B1%7D%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bv%7D%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%5C%5C+%26+%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5Cdelta_t+%5Cmathbf%7Be%7D_t+%2B+%5Calpha+%28%5Cmathbf%7Bw%7D_t%5ET+%5Cmathbf%7Bx%7D_t+-+%5Cmathbf%7Bw%7D_%7Bt-1%7D%5ET+%5Cmathbf%7Bx%7D_t%29+%28%5Cmathbf%7Be%7D_t+-+%5Cmathbf%7Bx%7D_t%29+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\mathbf{e}_{-1} = \\mathbf{0} \\\\ &amp; \\mathbf{e}_t = \\gamma \\lambda \\mathbf{e}_{t-1} + (1-\\alpha \\gamma \\lambda \\mathbf{e}_{t-1}^T \\mathbf{x}_t) \\mathbf{x}_t \\quad \\text{(Dutch trace)}\\\\ &amp; \\delta_t = R_{t+1} + \\gamma \\widehat{v}(S_{t+1}, \\mathbf{w}_t) - \\widehat{v}(S_t, \\mathbf{w}_t) \\\\ &amp; \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{e}_t + \\alpha (\\mathbf{w}_t^T \\mathbf{x}_t - \\mathbf{w}_{t-1}^T \\mathbf{x}_t) (\\mathbf{e}_t - \\mathbf{x}_t) \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E总结一下Eligibility Trace是一种可以附加在很多算法上面的技术，它不像传统“往后看”的算法那样，必须要等到收益信号完整得到之后才对权值进行更新，它在拿到最新的数据之后，就会尽可能地对权重进行更新。这样做的好处是能够把运算量尽可能分摊到每一步中，而不是集中再完整的信号全部拿到之后；同时尽早地把信息反馈到模型中也可以使模型学习地更快。Sutton的书上面提到的相关的应用包括\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003E使用了Eligibility Trace的一个MC版本\u003C\u002Fi\u003E\u003C\u002Fb\u003E（linear function approximation, single reward at the end of episode）：该算法在回合中的每一步中都更新两个变量，一个变量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Be%7D\" alt=\"\\mathbf{e}\" eeimg=\"1\"\u002F\u003E 记录历史轨迹中状态特征向量的叠加，一个变量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Ba%7D\" alt=\"\\mathbf{a}\" eeimg=\"1\"\u002F\u003E 记录轨迹上权重的叠加和“遗忘”；在回合结束拿到真实的收益之后，就可以一步对权重进行更新。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003ESARSA(\u003C\u002Fi\u003E\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E \u003Cb\u003E\u003Ci\u003E)和True Online SARSA(\u003C\u002Fi\u003E\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E \u003Cb\u003E\u003Ci\u003E)\u003C\u002Fi\u003E\u003C\u002Fb\u003E：前面的TD(\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )是应用在了状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v%28S%2C%5Cmathbf%7Bw%7D%29\" alt=\"v(S,\\mathbf{w})\" eeimg=\"1\"\u002F\u003E 上面，相应的应用在行动价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=q%28S%2C+A%2C+%5Cmathbf%7Bw%7D%29\" alt=\"q(S, A, \\mathbf{w})\" eeimg=\"1\"\u002F\u003E 上的版本就是这个，公式是对应的。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003EOff-Policy版本的TD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )和SARSA( \u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E\u003Ci\u003E)\u003C\u002Fi\u003E\u003C\u002Fb\u003E：由于是off-policy，增加了相应的importance ratio，相应的公式变为了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Be%7D_t+%3D+%5Crho_t+%28%5Cgamma_t+%5Clambda_t+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bv%7D+%28S_t%2C+%5Cmathbf%7Bw%7D_t%29+%29\" alt=\"\\mathbf{e}_t = \\rho_t (\\gamma_t \\lambda_t \\mathbf{e}_{t-1} + \\nabla \\widehat{v} (S_t, \\mathbf{w}_t) )\" eeimg=\"1\"\u002F\u003E  和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Be%7D_t+%3D+%5Crho_t+%5Cgamma_t+%5Clambda_t+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bq%7D+%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{e}_t = \\rho_t \\gamma_t \\lambda_t \\mathbf{e}_{t-1} + \\nabla \\widehat{q} (S_t, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003ETree-Backup( \u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E\u003Ci\u003E )\u003C\u002Fi\u003E\u003C\u002Fb\u003E：它是off-policy的方法，但是不涉及使用importance ratio，它对每个样本调整了权重，使其更新的目标正确，其更新公式为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Be%7D_t+%3D+%5Cgamma_t+%5Clambda_t+%5Cpi%28A_t+%7C+S_t%29+%5Cmathbf%7Be%7D_%7Bt-1%7D+%2B+%5Cnabla+%5Cwidehat%7Bq%7D+%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{e}_t = \\gamma_t \\lambda_t \\pi(A_t | S_t) \\mathbf{e}_{t-1} + \\nabla \\widehat{q} (S_t, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E ，使用到的TD误差为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma_%7Bt%2B1%7D+%5Cbar%7BQ%7D_%7Bt%2B1%7D+-+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\delta_t = R_{t+1} + \\gamma_{t+1} \\bar{Q}_{t+1} - \\widehat{q}(S_t, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003EGTD( \u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E\u003Ci\u003E )、GQ( \u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E\u003Ci\u003E )、HTD( \u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E\u003Ci\u003E )和Emphatic TD( \u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E\u003Ci\u003E )\u003C\u002Fi\u003E\u003C\u002Fb\u003E：这四个是前面提到的Gradient TD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )以及Emphatic-TD Method的Eligibility Trace的实现版本。GTD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )是Gradient-TD的直接实现；GQ( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )是相应的action value function的版本；HTD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )结合了GTD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )和TD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )，它在目标策略和行动策略相同的情况下和TD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )完全等价；Emphatic TD( \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E )是前面Emphatic-TD的Eligibility Trace的实现版本。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E用强化学习来玩游戏 —— DQN\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E下面我们就来讲一下DeepMind的这一篇工作Deep Q-Network（DQN），可以说强化学习如今的火热和这篇工作的成功是分不开的，这篇工作在2013年发表在NIPS上[4]之后，又以封面文章的形式发表在了2015年的Nature杂志上[5]。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ed8b1bb0a0679660b5881e4b3944c779_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"665\" class=\"origin_image zh-lightbox-thumb\" width=\"506\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ed8b1bb0a0679660b5881e4b3944c779_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;506&#39; height=&#39;665&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"665\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"506\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ed8b1bb0a0679660b5881e4b3944c779_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ed8b1bb0a0679660b5881e4b3944c779_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E我们先回顾一下之前讲到的Q-Learning，在表格方法中，Q-Learning主要是做如下更新 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28S%2C+A%29+%5Cleftarrow+Q%28S%2C+A%29+%2B+%5Calpha+%5BR+%2B+%5Cgamma+%5Cmax_a+Q%28S%27%2C+a%29+-+Q%28S%2C+A%29%5D\" alt=\"Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_a Q(S&#39;, a) - Q(S, A)]\" eeimg=\"1\"\u002F\u003E ；这里的Deep指的是使用了深度神经网络来做function approximation，在function approximation下，仿照我们之前的推导，对于Mean Squared TD Error  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BTDE%7D+%3D+%5Cmathbb%7BE%7D_%7Bs%2Ca%5Csim+b%7D%5B%28G+-+%5Cwidehat%7Bq%7D%28s%2Ca%2C+%5Cmathbf%7Bw%7D%29%5E2%5D\" alt=\"\\overline{TDE} = \\mathbb{E}_{s,a\\sim b}[(G - \\widehat{q}(s,a, \\mathbf{w})^2]\" eeimg=\"1\"\u002F\u003E 求梯度（其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G+%3D+%5Cmathbb%7BE%7D_%7Bs%27%7D%5Br+%2B+%5Cgamma+max_%7Ba%27%7D+%5Cwidehat%7Bq%7D%28s%27%2C+a%27%2C+%5Cmathbf%7Bw%7D%29+%7C+s%2C+a%5D\" alt=\"G = \\mathbb{E}_{s&#39;}[r + \\gamma max_{a&#39;} \\widehat{q}(s&#39;, a&#39;, \\mathbf{w}) | s, a]\" eeimg=\"1\"\u002F\u003E ），从而得到相应的半梯度方法的权值更新公式\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29+-+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_%7B%5Cmathbf%7Bw%7D%7D+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\max_a \\widehat{q}(S_{t+1}, A_t, \\mathbf{w}_t) - \\widehat{q}(S_t, A_t, \\mathbf{w}_t)] \\nabla_{\\mathbf{w}} \\widehat{q}(S_t, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E我们注意到，这里的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\widehat{q}(S_t, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E 是由一个神经网络表示的，它对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BTDE%7D\" alt=\"\\overline{TDE}\" eeimg=\"1\"\u002F\u003E 做梯度下降和神经网络中普通地使用BP来更新权重无异。我们可以把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28S_t%2C+A_t%29\" alt=\"(S_t, A_t)\" eeimg=\"1\"\u002F\u003E 当做训练样本的输入，把相应的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"G_t = R_{t+1} + \\gamma \\max_a \\widehat{q}(S_{t+1}, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E 当做训练样本的输出，这样就转化成了一个标准的基于神经网络的有监督学习问题了。（该文章中的神经网络结构是用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"\u002F\u003E 做输入，用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C\" alt=\"|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E 维的向量做输出代表不同行动的价值函数）。但是DQN中和有监督学习存在以下三点不同，增加了其相对于有监督学习的难度：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E有监督学习样本的目标是不变，而DQN样本的目标是基于该神经网络现在的权值的；\u003C\u002Fli\u003E\u003Cli\u003E有监督学习样本之间是相互独立的，但是DQN中的样本是从一连串的MDP的序列中得到的，相邻的样本之间不相互独立；\u003C\u002Fli\u003E\u003Cli\u003E有监督学习假设了样本分布的稳定性，但是DQN中随着学习到不同的策略，样本的分布也在逐渐发生变化；\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E这个方法集齐了前面提到的The Deadly Triad，即使用了off-policy、function approximation和bootstrap，并且所使用的也是半梯度方法，按理来说算法的收敛是十分困难的。这篇文章的主要贡献就在于，对于这样一个理论上很难稳定的算法，它能够使用一些工程上的技巧使其稳定，正如原文中所说\u003C\u002Fp\u003E\u003Cblockquote\u003EThis suggests that, despite lacking any theoretical convergence guarantees, our method is able to train large neural networks using a reinforcement learning signal and stochastic gradient descent in a stable manner.\u003C\u002Fblockquote\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-76a87cbe3c5d5d5f1b145d01ca643abe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1688\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb\" width=\"1688\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-76a87cbe3c5d5d5f1b145d01ca643abe_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1688&#39; height=&#39;822&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1688\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1688\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-76a87cbe3c5d5d5f1b145d01ca643abe_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-76a87cbe3c5d5d5f1b145d01ca643abe_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E上图显示的就是DQN的算法，倒数第三行所叙述的更新公式就是我们前面列出来的semi-gradient Q-learning的更新公式，我们主要就其几点特征来叙述\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EExperience Replay: 这是DQN算法中最为关键的技术，正是这个技术的应用才能够使得DQN在工程上能够稳定收敛。其主要做法是在agent有了一段经历之后，把这个经历 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=e_t+%3D+%28s_t%2C+a_t%2C+r_t%2C+s_%7Bt%2B1%7D%29\" alt=\"e_t = (s_t, a_t, r_t, s_{t+1})\" eeimg=\"1\"\u002F\u003E 存入\u003Cb\u003E\u003Ci\u003E经验池\u003C\u002Fi\u003E\u003C\u002Fb\u003E（replay memory）中，在每次需要更新权值的时候，从经验池中选取一个mini-batch来对其做梯度下降更新权重。这样的做法有以下几个特征：\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli\u003E每个经历在经验池中都可以被多次取出来使用，这样提高了数据的使用效率；\u003C\u002Fli\u003E\u003Cli\u003E由一个连续动作序列产生的经验相互之间具有很大的相关性，一个相关性很高的mini-batch会增大更新的方差，甚至会导致算法不稳定，而经验池的使用使得算法更加稳定；\u003C\u002Fli\u003E\u003Cli\u003E由于在经验池中采样一个mini-batch再来更新权重，这样是的其样本上的损失函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Coverline%7BTDE%7D\" alt=\"\\overline{TDE}\" eeimg=\"1\"\u002F\u003E更加的平滑，有利于减少方差；\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli\u003EOff-policy: 由于使用了经验池，在更新当前权重的时候，其样本并不是基于当前权重的greedy策略（目标策略），因此，就必然地需要使用off-policy的方法，比如这里使用的Q-learning；同时off-policy的探索更强，能够避免on-policy方法里面陷入较差的极小值点的问题。\u003C\u002Fli\u003E\u003Cli\u003EStacked Frames: 很多游戏里面有些元素的出现是“一闪一闪”的，如果只是传入最新一帧的图片，这样的信息就无法传递给agent，因此，在实际操作的过程中，每个状态都传递了最近四帧的图片给Q-network；从数学的角度上来说，这样的做法让状态的表示更具有马可夫性。\u003C\u002Fli\u003E\u003Cli\u003EFrame-skipping: 在训练的时候每一帧都去判断要采取什么动作相对来讲频率比较高，这会降低agent采样到不同状态的效率，因此，在DQN的工作中会每隔4帧才传递给agent去判断要采取什么动作。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E自从DQN发表以来，激发了深度强化学习领域的热潮，基于DQN做的各种改进也是层出不穷，下面一幅图就显示了大家基于DQN所做的大量的工作（摘自\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F39999667\" class=\"internal\"\u003EHYQ：强化学习路在何方？\u003C\u002Fa\u003E）\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-63fefc2066a4c21a896a7af671a71fc9_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"845\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-63fefc2066a4c21a896a7af671a71fc9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;865&#39; height=&#39;845&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"845\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-63fefc2066a4c21a896a7af671a71fc9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-63fefc2066a4c21a896a7af671a71fc9_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：DQN 和其变种们\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E以下简单介绍几个重要的改进\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003ETarget Q-Network\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E算法不稳定的一个重要的来源是每次更新的目标和原本的网络具有较强的相关性，这里的思路是建立两个价值函数网络：一个网络还是原来那样用于直到行动策略进行 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy的采样，并且使用BP方法更新权重；另一个网络专门用于目标的计算。这样更新的公式就变为了\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cmathbf%7Bw%7D_t+%2B+%5Calpha+%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%5E-%29+-+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29%5D+%5Cnabla_%7B%5Cmathbf%7Bw%7D%7D+%5Cwidehat%7Bq%7D%28S_t%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\max_a \\widehat{q}(S_{t+1}, A_t, \\mathbf{w}_t^-) - \\widehat{q}(S_t, A_t, \\mathbf{w}_t)] \\nabla_{\\mathbf{w}} \\widehat{q}(S_t, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cwidehat%7Bq%7D%28S%2C+A%2C+%5Cmathbf%7Bw%7D%5E-%29\" alt=\"\\widehat{q}(S, A, \\mathbf{w}^-)\" eeimg=\"1\"\u002F\u003E 就是新增加的这个target network，而权值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D%5E-\" alt=\"\\mathbf{w}^-\" eeimg=\"1\"\u002F\u003E 是每隔 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=C\" alt=\"C\" eeimg=\"1\"\u002F\u003E 步从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D\" alt=\"\\mathbf{w}\" eeimg=\"1\"\u002F\u003E 拷贝过来的。通过这种方法能够再进一步地使得网络的训练更加稳定，该方法是在DQN文章的Nature版本中加入的（Nature DQN）[5]。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EDouble Q-Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EDQN的目标为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+A_t%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"G_t = R_{t+1} + \\gamma \\max_a \\widehat{q}(S_{t+1}, A_t, \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E ，这里面有一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax\" alt=\"\\max\" eeimg=\"1\"\u002F\u003E 的操作，这个操作会带来价值函数值的高估。举一个简单的例子帮助理解， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BE%7D%5B%5Cmax%28X_1%2C+X_2%29%5D+%5Cge+%5Cmax%28%5Cmathbb%7BE%7D%5BX_1%5D%2C+%5Cmathbb%7BE%7D%5BX_1%5D%29\" alt=\"\\mathbb{E}[\\max(X_1, X_2)] \\ge \\max(\\mathbb{E}[X_1], \\mathbb{E}[X_1])\" eeimg=\"1\"\u002F\u003E ，你可以想象两个随机变量都是标准正态分布，那么式子左边肯定是大于零的，而式子右边应该等于零。根据这一的道理，由于每个Q值都是基于其他Q值的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax\" alt=\"\\max\" eeimg=\"1\"\u002F\u003E 操作得到的，这样Q值就会被高估。\u003C\u002Fp\u003E\u003Cp\u003E不过，这样的高估是否存在于实际的DQN算法中呢？如果存在，这样的高估是否会损害DQN的性能呢？首先，如果随着算法的迭代，如果Q值的估计越来越准确，即Q值本身方差就比较小，在做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax\" alt=\"\\max\" eeimg=\"1\"\u002F\u003E 操作之后的高估也会越来越小，这样的高估就可能在算法的迭代中消失。其次，如果所有的Q值估算出来都比真是的值均匀地高一些，其实也不影响最后形成的策略；更进一步，适当的随机高估一些状态，其实能够起到更好的探索的效果（参见UCB算法的思想）。\u003C\u002Fp\u003E\u003Cp\u003E不过这篇研究工作[6]发现 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax\" alt=\"\\max\" eeimg=\"1\"\u002F\u003E 操作不仅在实验上能够观察到Q值明显的高估，而且得到的高估的Q值会损害DQN的性能。为什么高估的现象会一直存在？因为实验中存在一些不可避免的噪声，比如由于随机性环境带来的噪声、由于函数逼近带来的不确定性和算法不稳定性带来的不确定性，这些都很难避免，因此Q值估计上的噪声不可消灭，因此这样的高估会存在最后的结果中。为什么这样的高估会损害算法的性能？因为Q值的高估分布并不是均匀的，Q值的计算是bootstrap的，某个Q值由于随机性的高估，会导致后续很多步骤都基于此，由此形成正反馈导致不均匀的高估。为什么高估不会带来更好的探索呢？因为一旦形成一系列的高估，个体更可能反复去探索这个被高估的行动序列，反而不会去探索那些访问更少的行动。\u003C\u002Fp\u003E\u003Cp\u003E解决方法很好理解， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax\" alt=\"\\max\" eeimg=\"1\"\u002F\u003E 操作其实可以分为两步，一步是选取一个数值最大的元素，另一步是取出这个元素的数值，如果这两步都是用了同样的estimator，就会产生高估；解决方法就是把这两步分开并且使用不一样的estimators。反映到公式里面就是把目标换为了\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+arg+%5Cmax_a+%5Cwidehat%7Bq%7D%28S_%7Bt%2B1%7D%2C+a%2C+%5Cmathbf%7Bw%7D_t%5E-%29+%2C+%5Cmathbf%7Bw%7D_t%29\" alt=\"G_t = R_{t+1} + \\gamma \\widehat{q}(S_{t+1}, arg \\max_a \\widehat{q}(S_{t+1}, a, \\mathbf{w}_t^-) , \\mathbf{w}_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E而在迭代中，参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_t%5E-\" alt=\"\\mathbf{w}_t^-\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbf%7Bw%7D_t\" alt=\"\\mathbf{w}_t\" eeimg=\"1\"\u002F\u003E 隔一段时间就交替位置，轮流更新；这样就产生了两个训练方法相同，但是不完全相同的Q-network。故此把这种方法叫做double Q-learning。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EPrioritized Experience Replay\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E在最原始的DQN中，对于经验池中经验的采样是均匀分布的，这样的做法显然不是最优的，如果我们能够更多地采样那些能够帮助我们更好、更多地学习的样本，就能够提高数据利用的效率，加快学习的速度。因此，DeepMind这篇文章[7]的大致思路就是维护一个优先队列，每次把经验加入经验池中的时候都对这个经验有一个评分，从经验池中选取数据的时候根据这个评分来更多地选取评分高的经验。当然，梯度的更新是基于采样出来的这个mini-batch的，如果采样不是均匀的，可能导致得到梯度的期望不再是准确的梯度，因此，在此基础上还需要对采样出来的每个经验进行importance ratio的补偿。下面我们就分别对这三个部分进行说明。\u003C\u002Fp\u003E\u003Cp\u003E1. 如何对不同的经验进行评分？大体的思想是，如果一个经验能够使得agent学习到更多的东西，那么我们就希望给该经验更高的评分。那么究竟如何量化呢？在这篇文章中使用了TD误差的绝对值来作为每个经验是否更有用的衡量标准，用TD误差的好处是在Q-Learning里面这个误差本来就有，容易获取，同时TD误差的绝对值越大，越表明这个经验是出乎意料的，权重在该经验上面的更新就会越大，即agent学习到的量就会更多。当然，也可以使用其他的衡量标准。比如使用权重更新前后TD误差的变化，或者该经验造成的权重更新的大小；使用这两种衡量标准的动机是有些经验虽然有很大的TD误差，但是在特定的函数逼近下，这个误差没法被消除，如果总是强调这样的样本，反而浪费了模型学习的时间。另外的衡量标准还包括赋予正TD误差比负TD误差更多的重视、对于表现好的轨迹上所有的经验都更加重视等。\u003C\u002Fp\u003E\u003Cp\u003E2. 如何从经验池中选择经验进行梯度的估计？一个简单的想法就是，既然有了评分，就选择评分最高的经验。但是这样的做法有两方面的问题，一方面来说TD误差的衡量方式并不能完全代表该经验的价值，随着权值的改变，一开始TD误差小的样本可能现在当前权值下变得很大，或者由于环境的随机性该经验本身可能有学习的价值，但是存进来的时候偶然地TD误差较小；另一方面来说，权值的更新是较慢的，可能TD误差大的样本经过若干次更新其TD误差还是较大，最后可能是翻来覆去在学习同一拨经验。为了避免这样的问题，该文采取了随机采样的方式来从经验池中选择经验，即根据每个经验的TD误差，计算一个其采样的概率，然后依照此概率进行采样。有两种概率形式，分别是\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D++P%28i%29+%26+%3D+%5Cdfrac%7Bp_i%5E%5Calpha%7D%7B%5Csum_k+p_k%5E%5Calpha%7D+%5Ctext%7B++where+%7D+p_i+%3D+%7C%5Cdelta_i%7C+%2B+%5Cepsilon+%5C%5C+P%28i%29+%26%3D+%5Cdfrac%7B1%7D%7Brank%28i%29%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned}  P(i) &amp; = \\dfrac{p_i^\\alpha}{\\sum_k p_k^\\alpha} \\text{  where } p_i = |\\delta_i| + \\epsilon \\\\ P(i) &amp;= \\dfrac{1}{rank(i)} \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E虽然后一种在分布上更加长尾，探索可能更好，但是由于它忽略了绝对数值关系，两者的实验效果差不多。\u003C\u002Fp\u003E\u003Cp\u003E3. 有侧重的采样之后如何使得最后的梯度无偏？我们还需要对每个采样出来的样本乘上importance ratio用来补偿不同样本被采样频率的不同，补偿的方法就是乘上 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w_i+%3D+%5Cleft%28%5Cdfrac%7B1%7D%7BN%7D%5Cdfrac%7B1%7D%7Bp%28i%29%7D%5Cright%29%5E%5Cbeta\" alt=\"w_i = \\left(\\dfrac{1}{N}\\dfrac{1}{p(i)}\\right)^\\beta\" eeimg=\"1\"\u002F\u003E ，当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta%3D1\" alt=\"\\beta=1\" eeimg=\"1\"\u002F\u003E 的时候就可以完全补偿，产生无偏的梯度。做importance ratio另外一个顺带的好处就是，TD误差大的原本会在权值空间上产生更大的步长，这容易导致算法的不稳定；而importance ratio使得这些TD误差大的样本每次对权值的更新步长更小了；这样“小步多次”的更新方式使得算法更加稳定。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EDueling Network Architecture\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E在原始的DQN中，使用的是在深度学习中常见的网络结构来作为Q值的函数逼近，其输入端是状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 的特征表示，对应 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C\" alt=\"|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E 个输出端，每个输出端代表的是相应行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 的Q值 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 。网络是从输入到输出的CNN神经网络。那么有没有其他的神经网络结构更适合用于Q值的函数逼近呢？\u003C\u002Fp\u003E\u003Cp\u003E该篇文章[8]就对此进行了探索，提出了下图所示的神经网络结构。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16994bf377c62a584dff513fa34669fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb\" width=\"850\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16994bf377c62a584dff513fa34669fb_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;850&#39; height=&#39;632&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"850\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16994bf377c62a584dff513fa34669fb_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16994bf377c62a584dff513fa34669fb_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E该图上方显示的是在DQN中使用的网络结构，原始的状态特征 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E （Atari游戏的视频截图）经过若干层CNN之后通过Flatten层，最后通过一层全连接层Dense变为长度为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C\" alt=\"|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E 的向量。其中每个位置的数值就代表行动价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 。该图下方是该文提出的dueling网络结构。该网络结构主要利用了行动状态函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 和状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 之间的一个常用的关系，把行动价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 拆分为了状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 和Advantage \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%28s%2Ca%29\" alt=\"A(s,a)\" eeimg=\"1\"\u002F\u003E 的和。即， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29+%3D+V%28s%29+%2B+A%28s%2Ca%29\" alt=\"Q(s,a) = V(s) + A(s,a)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E我们先介绍其做法，再来描述这样做的优势。\u003C\u002Fp\u003E\u003Cp\u003E这样网络的结构在前面还是跟传统的一样，但是在Flatten层之后，该层向量被复制分别送到了上下两路，上面一路通过全连接层变为了一个标量，该标量代表相应的状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E ，下面一路通过另外一个全连接层变成了长度为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C\" alt=\"|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E 的向量，其中每一位的数值代表Advantage \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%28s%2Ca%29\" alt=\"A(s,a)\" eeimg=\"1\"\u002F\u003E，最后 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 和相应的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%28s%2Ca%29\" alt=\"A(s,a)\" eeimg=\"1\"\u002F\u003E 相加就可以得到最后的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 。但是很快就会发现一个问题，学习的目标是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E ，把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 上面加上一个常数，再从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%28s%2Ca%29\" alt=\"A(s,a)\" eeimg=\"1\"\u002F\u003E 上面减去相同的一个常数并不影响结果，在训练的过程中，我们并没法保证这个常数如何在状态价值函数和Advantage之间分配。换个角度来说，一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C\" alt=\"|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E 维的目标 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 并不能完全决定一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C%2B1\" alt=\"|\\mathcal{A}|+1\" eeimg=\"1\"\u002F\u003E 维的系统。因此，我们还必须找出他们之间的一个约束关系。\u003C\u002Fp\u003E\u003Cp\u003E在定义上，有如下关系\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctext%7Bfor+stochastic+policy+%7D+%5Cbegin%7Bcases%7D+V%5E%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%28s%29%7D%5BQ%5E%5Cpi%28s%2Ca%29%5D+%26+%5C%5C+%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%28s%29%7D%5BA%5E%5Cpi+%28s%2Ca%29%5D+%3D+0+%26+%5C%5C+%5Cend%7Bcases%7D+%5Ctext%7Bfor+deterministic+policy+%7D+%5Cbegin%7Bcases%7D+V%28s%29+%3D+Q%28s%2Ca%5E%2A%29+%26+%5C%5C+A+%28s%2Ca%5E%2A%29+%3D+0+%26+%5C%5C+%5Cend%7Bcases%7D\" alt=\"\\text{for stochastic policy } \\begin{cases} V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi(s)}[Q^\\pi(s,a)] &amp; \\\\ \\mathbb{E}_{a\\sim \\pi(s)}[A^\\pi (s,a)] = 0 &amp; \\\\ \\end{cases} \\text{for deterministic policy } \\begin{cases} V(s) = Q(s,a^*) &amp; \\\\ A (s,a^*) = 0 &amp; \\\\ \\end{cases}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E由此我们可以写出一个它们之间的约束关系\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%3B%5Ctheta%2C%5Calpha%2C%5Cbeta%29+%3D+V%28s%3B%5Ctheta%2C%5Cbeta%29+%2B+%28A%28s%2Ca%3B%5Ctheta%2C%5Calpha%29+-+%5Cmax_%7Ba%27%5Cin+%7C%5Cmathcal%7BA%7D%7C%7D+A%28s%2Ca%27%3B%5Ctheta%2C%5Calpha%29\" alt=\"Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + (A(s,a;\\theta,\\alpha) - \\max_{a&#39;\\in |\\mathcal{A}|} A(s,a&#39;;\\theta,\\alpha)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u002F\u003E 是网络共有部分的参数， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u002F\u003E 是Advantage部分的网络参数， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"\u002F\u003E 是状态价值函数部分的网络参数。我们把表达式中右边括号里面的数值作为Advantage部分的目标数值，剩下的部分就是状态价值函数的目标数值。\u003C\u002Fp\u003E\u003Cp\u003E但是一般来说，含有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax\" alt=\"\\max\" eeimg=\"1\"\u002F\u003E 常常会导致算法不稳定，一个常见的想法就是换成softmax，本文的作者也这么做了，但是最后还是觉得不如直接换成减去一个均值。其坏处是这样得到的\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 和原本的定义就会有一个gap了，不过我们关心的是Advantage部分的相对大小，所以这个gap也不是很影响。于是有\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%3B%5Ctheta%2C%5Calpha%2C%5Cbeta%29+%3D+V%28s%3B%5Ctheta%2C%5Cbeta%29+%2B+%28A%28s%2Ca%3B%5Ctheta%2C%5Calpha%29+-+%5Cdfrac%7B1%7D%7B%7C%5Cmathcal%7BA%7D%7C%7D+%5Csum_%7Ba%27%7D+A%28s%2Ca%27%3B%5Ctheta%2C%5Calpha%29\" alt=\"Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + (A(s,a;\\theta,\\alpha) - \\dfrac{1}{|\\mathcal{A}|} \\sum_{a&#39;} A(s,a&#39;;\\theta,\\alpha)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E有了这个之后，网络的训练还是照常进行，只不过原来是一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C\" alt=\"|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E 维的目标，现在变成了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BA%7D%7C%2B1\" alt=\"|\\mathcal{A}|+1\" eeimg=\"1\"\u002F\u003E 维的目标，仍然用BP来训练网络。\u003C\u002Fp\u003E\u003Cp\u003E了解完了其做法，那么自然会有一个疑问，为什么这样做会更好？这样做是为了解决什么样的问题？这样做有如下几个优势：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E在某些状态上，采取任何一种行动的效果都差不多，我们不需要对于每个状态-行动对都去努力学习Q值。文中举了一个例子，在开赛车的游戏里面，如果前路笔直宽阔并且没有障碍物，那么left, right, no-op这些操作基本上没什么差异，这时候如果有一个V值统一的描述一下该状态就可以了。\u003C\u002Fli\u003E\u003Cli\u003E在传统的训练中，我们把一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28s%2Ca%29\" alt=\"(s,a)\" eeimg=\"1\"\u002F\u003E 对看做一个样本，该样本来了只考虑该样本 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 带给网络参数的影响；而现在的这种结构下，任何一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28s%2C%5Ccdot%29\" alt=\"(s,\\cdot)\" eeimg=\"1\"\u002F\u003E 来了都能促成对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 的学习，从而影响别的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28s%2Ca%27%29\" alt=\"(s,a&#39;)\" eeimg=\"1\"\u002F\u003E 上的数值；这样对于数据的利用就更为充分。\u003C\u002Fli\u003E\u003Cli\u003E在训练中观察的很多情况下，同一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下不同行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 对应的Q值相互之间的差值相对于其绝对数值来讲小很多，这样，一点点的噪声将会对各个行动之间的排序造成很大的影响，进而影响到策略；把Q值中相同的部分扣除掉之后，会大大减轻这种问题。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E\u003Cb\u003E总结\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E我们这一讲引入了强化学习最为精髓的思想——时间差分算法，在此基础上我们对其进行了拓展。在状态空间和行动空间变得较大的时候，我们就需要开始使用函数逼近技术；考虑到和环境的交互，我们更希望是“边做边思考”，为了这样的计算便捷性的目标我们可以使用Eligibility Trace的技术；同时为了应对更加庞大而复杂的状态空间，我们考虑引入神经网络来对价值函数进行拟合，这就形成了目前一个非常强大的算法DQN；DQN发明以来就引起了强化学习社区的极大关注，对于其的改进层出不穷，我们选取了其中最为重要的几个改进加以介绍。\u003C\u002Fp\u003E\u003Cp\u003E简单总结一下。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5796f1635f8e77ab7ac440b52c905c6b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"885\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb\" width=\"885\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5796f1635f8e77ab7ac440b52c905c6b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;885&#39; height=&#39;123&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"885\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"885\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5796f1635f8e77ab7ac440b52c905c6b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5796f1635f8e77ab7ac440b52c905c6b_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E参考文献\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. Vol. 1. No. 1. Cambridge: MIT press, 1998. \u003C\u002Fp\u003E\u003Cp\u003E[2] Sutton, Richard S. &#34;True Online Emphatic TD ($\\lambda $): Quick Reference and Implementation Guide.&#34; arXiv preprint arXiv:1507.07147 (2015). \u003C\u002Fp\u003E\u003Cp\u003E[3] Mahmood, A. Rupam, et al. &#34;Emphatic temporal-difference learning.&#34; arXiv preprint arXiv:1507.01569 (2015).\u003C\u002Fp\u003E\u003Cp\u003E[4] Mnih, Volodymyr, et al. &#34;Playing atari with deep reinforcement learning.&#34; arXiv preprint arXiv:1312.5602 (2013).\u003C\u002Fp\u003E\u003Cp\u003E[5] Mnih, Volodymyr, et al. &#34;Human-level control through deep reinforcement learning.&#34; Nature 518.7540 (2015): 529.\u003C\u002Fp\u003E\u003Cp\u003E[6] Van Hasselt, Hado, Arthur Guez, and David Silver. &#34;Deep Reinforcement Learning with Double Q-Learning.&#34; AAAI. Vol. 2. 2016.\u003C\u002Fp\u003E\u003Cp\u003E[7] Schaul, Tom, et al. &#34;Prioritized experience replay.&#34; arXiv preprint arXiv:1511.05952 (2015).\u003C\u002Fp\u003E\u003Cp\u003E[8] Wang, Ziyu, et al. &#34;Dueling network architectures for deep reinforcement learning.&#34; arXiv preprint arXiv:1511.06581 (2015).\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":15,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":3,"contributions":[{"id":20207657,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习入门 2】强化学习策略迭代类方法 - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56058343 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-4","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"1","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F56058343","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56058343","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>