<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 109】Provable Self-play - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="计算机科学,强化学习 (Reinforcement Learning),人工智能"/><meta data-react-helmet="true" name="description" content="针对 self-play 的强化学习问题，基于 UCB 的思想给出 provably efficient 算法。原文传送门Bai, Yu, and Chi Jin. &amp;#34;Provable Self-Play Algorithms for Competitive Reinforcement Learning.&amp;#34; arXiv prep…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 109】Provable Self-play"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/111191211"/><meta data-react-helmet="true" property="og:description" content="针对 self-play 的强化学习问题，基于 UCB 的思想给出 provably efficient 算法。原文传送门Bai, Yu, and Chi Jin. &amp;#34;Provable Self-Play Algorithms for Competitive Reinforcement Learning.&amp;#34; arXiv prep…"/><meta data-react-helmet="true" property="og:image" content="https://pic4.zhimg.com/v2-0517af4d255678cada7334315727ab36_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:111191211,&quot;title&quot;:&quot;【强化学习 109】Provable Self-play&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic4.zhimg.com/v2-0517af4d255678cada7334315727ab36_1200x500.jpg" alt="【强化学习 109】Provable Self-play"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 109】Provable Self-play</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">39 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>针对 self-play 的强化学习问题，基于 UCB 的思想给出 provably efficient 算法。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.04017" class=" wrap external" target="_blank" rel="nofollow noreferrer">Bai, Yu, and Chi Jin. &#34;Provable Self-Play Algorithms for Competitive Reinforcement Learning.&#34; arXiv preprint arXiv:2002.04017 (2020).</a></p><h2>特色</h2><p>在实践中，很多强化学习方法应用到了 self-play 的方法来训练强化学习智能体，取得了很好的效果。比如本专栏前面介绍过的针对 AlphaGo、StarCraft 等任务的算法都用到了这种训练方法。这里把在相互竞争环境中的 self-play 形式化设定为一个 Markov game，然后针对这个问题提出了 provably efficient 的算法。具体来说，给出了</p><ul><li>基于 UCB 思想的 PPAD-complete 复杂度的 regret <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29+" alt="\tilde{O}(\sqrt{T}) " eeimg="1"/> 的算法</li><li>基于 explore-then-exploit 思想的 polynomial 复杂度的 regret <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BO%7D%28T%5E%7B2%2F3%7D%29" alt="\tilde{O}(T^{2/3})" eeimg="1"/> 的算法</li><li>给出了 lower bound，并且以上算法和 lower bound 直接存在一个 gap</li><li>给出了两种简化情形下能够 match lower bound 的算法</li></ul><p>总体上来说，本文是第一个 self-play RL 上的 provably efficient 算法。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-df091787180082a09993a292c3fea1b5_b.jpg" data-caption="" data-size="normal" data-rawwidth="1199" data-rawheight="480" class="origin_image zh-lightbox-thumb" width="1199" data-original="https://pic2.zhimg.com/v2-df091787180082a09993a292c3fea1b5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1199&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1199" data-rawheight="480" class="origin_image zh-lightbox-thumb lazy" width="1199" data-original="https://pic2.zhimg.com/v2-df091787180082a09993a292c3fea1b5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-df091787180082a09993a292c3fea1b5_b.jpg"/></figure><h2>过程</h2><h3>1、相关工作</h3><p><b>单智能体强化学习</b>：tabular episodic case model-based method <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BO%7D%28%5Csqrt%7BH%5E2+SAT%7D%29+" alt="\tilde{O}(\sqrt{H^2 SAT}) " eeimg="1"/> （Azar et al 2017) model-free method <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BO%7D%28%5Csqrt%7BH%5E3SAT%7D%29+" alt="\tilde{O}(\sqrt{H^3SAT}) " eeimg="1"/> (Jin et al 2018)，而相应的lower-bound 为 <img src="https://www.zhihu.com/equation?tex=%5COmega%28%5Csqrt%7BH%5E2+SAT%7D%29+" alt="\Omega(\sqrt{H^2 SAT}) " eeimg="1"/> 。</p><p><b>Markov game（MG）</b>：之前有较多的工作基于 known transition 和 known reward，即不是强化学习的设定。Wei et al 2017, Jia et al 2019, Sidford et al 2019 在强化学习设定上解决该问题，但是需要对于 MG 或者采样的方式做一些假设，从而某种程度上回避最为关键的探索问题。</p><p><b>Adversarial opponents</b>：注意这一类问题和 MG 不一样，在这一类问题中对手可以是一个和智能体作对的另一个玩家，也可以是专门干扰智能体，比如改变智能体拿到的 reward。MG 中的对手对于 reward 有一个明确的目标，最后能达到一个纳什均衡；但是这类问题则不会达到纳什均衡。</p><h3>2、问题设定</h3><p><b><u>Tabular episodic zero-sum Markov game</u></b></p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-aec2590008cb8d841a362d6b3c894d06_b.jpg" data-caption="" data-size="normal" data-rawwidth="1304" data-rawheight="459" class="origin_image zh-lightbox-thumb" width="1304" data-original="https://pic3.zhimg.com/v2-aec2590008cb8d841a362d6b3c894d06_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1304&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1304" data-rawheight="459" class="origin_image zh-lightbox-thumb lazy" width="1304" data-original="https://pic3.zhimg.com/v2-aec2590008cb8d841a362d6b3c894d06_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-aec2590008cb8d841a362d6b3c894d06_b.jpg"/></figure><p>注意到在本文的设定中，每一轮两个玩家同时给出其行动，并且一个玩家目标是最大化奖励，而另一个玩家的目标是最小化这个相同的奖励，因此是一个 zero-sum 博弈。多智能体强化学习里面有很多研究有 decentralized 的设定，这里没有这种，就是两个玩家同步学习。</p><p><b><u>Value function</u></b></p><p>相应地，可以定义价值函数。从 s 出发，两个玩家分别遵循相应的策略，最后获得的奖励和。注意到，对于 max player 来说，这个 value function 数值越大越好；对于 min player 来说，这个 value function 的数值越小越好。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1bd716b068959544e08a9783ab4f45a7_b.png" data-caption="" data-size="normal" data-rawwidth="1126" data-rawheight="88" class="origin_image zh-lightbox-thumb" width="1126" data-original="https://pic4.zhimg.com/v2-1bd716b068959544e08a9783ab4f45a7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1126&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1126" data-rawheight="88" class="origin_image zh-lightbox-thumb lazy" width="1126" data-original="https://pic4.zhimg.com/v2-1bd716b068959544e08a9783ab4f45a7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1bd716b068959544e08a9783ab4f45a7_b.png"/></figure><p>相应有 Q 函数</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-383faf3816a581b86f533ff6bc684ff5_b.png" data-caption="" data-size="normal" data-rawwidth="1133" data-rawheight="103" class="origin_image zh-lightbox-thumb" width="1133" data-original="https://pic2.zhimg.com/v2-383faf3816a581b86f533ff6bc684ff5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1133&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1133" data-rawheight="103" class="origin_image zh-lightbox-thumb lazy" width="1133" data-original="https://pic2.zhimg.com/v2-383faf3816a581b86f533ff6bc684ff5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-383faf3816a581b86f533ff6bc684ff5_b.png"/></figure><p>V 和 Q 之间的关系以及 Bellman 方程</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-48e7485e8204c7e9f817798a9cb8b9a5_b.png" data-caption="" data-size="normal" data-rawwidth="1175" data-rawheight="176" class="origin_image zh-lightbox-thumb" width="1175" data-original="https://pic2.zhimg.com/v2-48e7485e8204c7e9f817798a9cb8b9a5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1175&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1175" data-rawheight="176" class="origin_image zh-lightbox-thumb lazy" width="1175" data-original="https://pic2.zhimg.com/v2-48e7485e8204c7e9f817798a9cb8b9a5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-48e7485e8204c7e9f817798a9cb8b9a5_b.png"/></figure><p><b><u>Best response</u></b></p><p>注意到这里有两个玩家，当一个玩家策略固定的时候，对于另外一个玩家，就存在一种最优的策略。比如对于 max player 的一个固定策略 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> ，那么存在一个 min player 的 best response <img src="https://www.zhihu.com/equation?tex=%5Cnu%5E%5Cdagger%28%5Cmu%29" alt="\nu^\dagger(\mu)" eeimg="1"/> ，满足</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-9f4804d82721627ef3e2722c45576ca0_b.png" data-caption="" data-size="normal" data-rawwidth="910" data-rawheight="41" class="origin_image zh-lightbox-thumb" width="910" data-original="https://pic1.zhimg.com/v2-9f4804d82721627ef3e2722c45576ca0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;910&#39; height=&#39;41&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="910" data-rawheight="41" class="origin_image zh-lightbox-thumb lazy" width="910" data-original="https://pic1.zhimg.com/v2-9f4804d82721627ef3e2722c45576ca0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-9f4804d82721627ef3e2722c45576ca0_b.png"/></figure><p>考虑对手每次都给出相对于自己的 best response，那么自己也能找到一个最优的策略，使得对手是最优策略的时候，自己也能最大化自己的利益。记这种情况下，自己的策略为 <img src="https://www.zhihu.com/equation?tex=%5Cmu%5E%2A%2C+%5Cnu%5E%2A" alt="\mu^*, \nu^*" eeimg="1"/> ：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-44af26d7bda2bf90cf3e506b4776c7d7_b.png" data-caption="" data-size="normal" data-rawwidth="1133" data-rawheight="74" class="origin_image zh-lightbox-thumb" width="1133" data-original="https://pic4.zhimg.com/v2-44af26d7bda2bf90cf3e506b4776c7d7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1133&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1133" data-rawheight="74" class="origin_image zh-lightbox-thumb lazy" width="1133" data-original="https://pic4.zhimg.com/v2-44af26d7bda2bf90cf3e506b4776c7d7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-44af26d7bda2bf90cf3e506b4776c7d7_b.png"/></figure><p>根据 minmax theorem，有</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-d343f2bc1e8bb53ab6999a45b07222c3_b.png" data-caption="" data-size="normal" data-rawwidth="1123" data-rawheight="66" class="origin_image zh-lightbox-thumb" width="1123" data-original="https://pic4.zhimg.com/v2-d343f2bc1e8bb53ab6999a45b07222c3_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1123&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1123" data-rawheight="66" class="origin_image zh-lightbox-thumb lazy" width="1123" data-original="https://pic4.zhimg.com/v2-d343f2bc1e8bb53ab6999a45b07222c3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-d343f2bc1e8bb53ab6999a45b07222c3_b.png"/></figure><p><b><u>Regret</u></b></p><p>下面定义这种问题下的 regret，注意算法设计的目标就是要在给定 K 轮中 minimize regret。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-ac1d12117d1b22cb0f8595b3cfb8975b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1178" data-rawheight="233" class="origin_image zh-lightbox-thumb" width="1178" data-original="https://pic4.zhimg.com/v2-ac1d12117d1b22cb0f8595b3cfb8975b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1178&#39; height=&#39;233&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1178" data-rawheight="233" class="origin_image zh-lightbox-thumb lazy" width="1178" data-original="https://pic4.zhimg.com/v2-ac1d12117d1b22cb0f8595b3cfb8975b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ac1d12117d1b22cb0f8595b3cfb8975b_b.jpg"/></figure><p>直接看这个定义可能不是很能够理解这个 regret 定义的含义，观察下面关系可以发现，该 regret 的含义就是每一轮每个玩家给出的策略的性能相对于该玩家可能达到的最好策略性能之间的损失和。某个玩家的策略性能需要用价值函数来衡量，而价值函数依赖于对手的策略，因此假设对手采取相对于自己策略的 best response。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-c739f62df51a5e52f7515c62ae8d55a0_b.png" data-size="normal" data-rawwidth="1135" data-rawheight="80" class="origin_image zh-lightbox-thumb" width="1135" data-original="https://pic1.zhimg.com/v2-c739f62df51a5e52f7515c62ae8d55a0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1135&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1135" data-rawheight="80" class="origin_image zh-lightbox-thumb lazy" width="1135" data-original="https://pic1.zhimg.com/v2-c739f62df51a5e52f7515c62ae8d55a0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-c739f62df51a5e52f7515c62ae8d55a0_b.png"/><figcaption>注意到，根据 minmax theorem，中间两项相等，就消掉了。</figcaption></figure><p><b><u>Turn-based games</u></b></p><p>这个设定还可以概括 turn-based games。就是轮到玩家一需要操作的时候，认为另外一个玩家的动作空间大小为 1，并且把这个唯一能够操作的动作设置为一个哑变量。</p><h3>3. Value iteration - upper/lower confidence bound (VI-ULCB)</h3><p>本专栏前面讲过 Jin Chi 的 LinearMDP，其算法和这个类似，也是基于 UCB 的；其算法（或者 Azar et al 2017）可以被概括如下：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-dabce07e055c6e22df0074a75894c41a_b.png" data-caption="" data-size="normal" data-rawwidth="1124" data-rawheight="116" class="origin_image zh-lightbox-thumb" width="1124" data-original="https://pic3.zhimg.com/v2-dabce07e055c6e22df0074a75894c41a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1124&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1124" data-rawheight="116" class="origin_image zh-lightbox-thumb lazy" width="1124" data-original="https://pic3.zhimg.com/v2-dabce07e055c6e22df0074a75894c41a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-dabce07e055c6e22df0074a75894c41a_b.png"/></figure><p>而这里有两个玩家，max player 希望价值函数越大越好，min player 希望价值函数越小越好；仿照这类似的思路，可以让 max player 照着 upper confidence bound 探索，让 min player 照着 lower confidence bound 探索，这样自然想到如下算法：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-375699a35043c9a54cf161df149b4eb6_b.png" data-caption="" data-size="normal" data-rawwidth="1141" data-rawheight="158" class="origin_image zh-lightbox-thumb" width="1141" data-original="https://pic3.zhimg.com/v2-375699a35043c9a54cf161df149b4eb6_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1141&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1141" data-rawheight="158" class="origin_image zh-lightbox-thumb lazy" width="1141" data-original="https://pic3.zhimg.com/v2-375699a35043c9a54cf161df149b4eb6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-375699a35043c9a54cf161df149b4eb6_b.png"/></figure><p>但是仔细想来，这个初步的想法有一个问题，因为估计出来的 Q 函数不仅取决于一个玩家的策略，还需要取决于另外一个玩家的策略；因此，我们需要同时确定两个玩家的策略，即找到两个玩家的策略使得它们 jointly greedy。把这两个 Q 函数看做两个玩家的 payoff matrix，然后认为 jointly greedy 就是找到这个 general-sum game 中的纳什均衡：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-0d1d11364ce0b10364e0b4e1cd3879e2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1150" data-rawheight="217" class="origin_image zh-lightbox-thumb" width="1150" data-original="https://pic3.zhimg.com/v2-0d1d11364ce0b10364e0b4e1cd3879e2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1150&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1150" data-rawheight="217" class="origin_image zh-lightbox-thumb lazy" width="1150" data-original="https://pic3.zhimg.com/v2-0d1d11364ce0b10364e0b4e1cd3879e2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-0d1d11364ce0b10364e0b4e1cd3879e2_b.jpg"/></figure><p>因此，该算法最终可以被概括为 </p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-f31e6c104b1318cd2e14a59c54d969cb_b.png" data-caption="" data-size="normal" data-rawwidth="1188" data-rawheight="152" class="origin_image zh-lightbox-thumb" width="1188" data-original="https://pic4.zhimg.com/v2-f31e6c104b1318cd2e14a59c54d969cb_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1188&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1188" data-rawheight="152" class="origin_image zh-lightbox-thumb lazy" width="1188" data-original="https://pic4.zhimg.com/v2-f31e6c104b1318cd2e14a59c54d969cb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-f31e6c104b1318cd2e14a59c54d969cb_b.png"/></figure><p>详细的算法框图如下</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8e1b5f8223687f003518976ec68941c0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1442" data-rawheight="792" class="origin_image zh-lightbox-thumb" width="1442" data-original="https://pic1.zhimg.com/v2-8e1b5f8223687f003518976ec68941c0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1442&#39; height=&#39;792&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1442" data-rawheight="792" class="origin_image zh-lightbox-thumb lazy" width="1442" data-original="https://pic1.zhimg.com/v2-8e1b5f8223687f003518976ec68941c0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8e1b5f8223687f003518976ec68941c0_b.jpg"/></figure><h3>4. 算法分析</h3><p><b><u>Regret bound</u></b></p><p>该算法可以达到如下 regret</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-30ae8ce2beee002f34d21afcd797bdbf_b.jpg" data-caption="" data-size="normal" data-rawwidth="1145" data-rawheight="239" class="origin_image zh-lightbox-thumb" width="1145" data-original="https://pic4.zhimg.com/v2-30ae8ce2beee002f34d21afcd797bdbf_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1145&#39; height=&#39;239&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1145" data-rawheight="239" class="origin_image zh-lightbox-thumb lazy" width="1145" data-original="https://pic4.zhimg.com/v2-30ae8ce2beee002f34d21afcd797bdbf_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-30ae8ce2beee002f34d21afcd797bdbf_b.jpg"/></figure><p>证明思路和 Linear MDP 相似，关键步骤可以概括如下</p><p>A. 在每一层上，估计的 transition 距离真实 transition 的误差随着样本数目的增多而减小（concentration），相应的误差项刚好和给出的 exploration bonus 相匹配。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-9b979d97e78911987e8031bf373ef3e8_b.jpg" data-caption="" data-size="normal" data-rawwidth="1160" data-rawheight="204" class="origin_image zh-lightbox-thumb" width="1160" data-original="https://pic1.zhimg.com/v2-9b979d97e78911987e8031bf373ef3e8_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1160&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1160" data-rawheight="204" class="origin_image zh-lightbox-thumb lazy" width="1160" data-original="https://pic1.zhimg.com/v2-9b979d97e78911987e8031bf373ef3e8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-9b979d97e78911987e8031bf373ef3e8_b.jpg"/></figure><p>B. 在每一层上，由于加上了 exploration bonus <img src="https://www.zhihu.com/equation?tex=%5Cbeta_t" alt="\beta_t" eeimg="1"/> ，估计出来的 upper 和 lower 大概率包含了真实价值函数的范围。其实质就是把前面的 concentration 展开写出来，注意到前一个 concentration 并没有规定价值函数得是实际遇到的价值函数（下面红色写出的），因此这个不等式也可以应用到相应的 sup 和 inf。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-5d605c1b765d1ef00fa95fab58c35170_b.png" data-caption="" data-size="normal" data-rawwidth="1157" data-rawheight="94" class="origin_image zh-lightbox-thumb" width="1157" data-original="https://pic1.zhimg.com/v2-5d605c1b765d1ef00fa95fab58c35170_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1157&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1157" data-rawheight="94" class="origin_image zh-lightbox-thumb lazy" width="1157" data-original="https://pic1.zhimg.com/v2-5d605c1b765d1ef00fa95fab58c35170_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-5d605c1b765d1ef00fa95fab58c35170_b.png"/></figure><p>C. 把 regret 中的每一项放缩为 upper 和 lower；然后从最后一层到第一层逐层递归，计算出地第一层的 regret bound。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-26d1bc6e3dc483e4ca955e1e3cfa67e2_b.png" data-caption="" data-size="normal" data-rawwidth="1130" data-rawheight="105" class="origin_image zh-lightbox-thumb" width="1130" data-original="https://pic3.zhimg.com/v2-26d1bc6e3dc483e4ca955e1e3cfa67e2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1130&#39; height=&#39;105&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1130" data-rawheight="105" class="origin_image zh-lightbox-thumb lazy" width="1130" data-original="https://pic3.zhimg.com/v2-26d1bc6e3dc483e4ca955e1e3cfa67e2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-26d1bc6e3dc483e4ca955e1e3cfa67e2_b.png"/></figure><p><b><u>PAC bound</u></b></p><p>Regret bound 可以被转化为如下 PAC bound</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-f3edaf96f345926c937c1b69b7e79ce3_b.jpg" data-caption="" data-size="normal" data-rawwidth="1173" data-rawheight="245" class="origin_image zh-lightbox-thumb" width="1173" data-original="https://pic4.zhimg.com/v2-f3edaf96f345926c937c1b69b7e79ce3_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1173&#39; height=&#39;245&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1173" data-rawheight="245" class="origin_image zh-lightbox-thumb lazy" width="1173" data-original="https://pic4.zhimg.com/v2-f3edaf96f345926c937c1b69b7e79ce3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-f3edaf96f345926c937c1b69b7e79ce3_b.jpg"/></figure><p><b><u>Runtime</u></b></p><p>该算法由于需要用到一个 Nash_General_Sum 的子程序，而这个子程序不是 polynomial 的，因此该算法不是 polynomial 的。不过该算法在 MG 的一个特殊情形，即 turn-based game 的时候是 polynomial 的。</p><p><b><u>Turn-based game</u></b></p><p>在 MG 情形下，需要求解 Nash_General_Sum，它的输入是两个各自的 payoff 矩阵，需要求解出两个玩家各自的策略，而求解其纳什均衡没有 polynomial 解法</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-17d9cd20c29f96761187e66f468ef103_b.png" data-caption="" data-size="normal" data-rawwidth="1171" data-rawheight="67" class="origin_image zh-lightbox-thumb" width="1171" data-original="https://pic4.zhimg.com/v2-17d9cd20c29f96761187e66f468ef103_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1171&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1171" data-rawheight="67" class="origin_image zh-lightbox-thumb lazy" width="1171" data-original="https://pic4.zhimg.com/v2-17d9cd20c29f96761187e66f468ef103_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-17d9cd20c29f96761187e66f468ef103_b.png"/></figure><p>在 turn-based 情形下，每轮只有一个玩家可以做实质性的操作，因此只需要求解一个玩家的策略，相应的求解问题则变成了一个简单的 LP 问题</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-2d164412c306dbcc75fe2dfba879c157_b.png" data-caption="" data-size="normal" data-rawwidth="1128" data-rawheight="56" class="origin_image zh-lightbox-thumb" width="1128" data-original="https://pic4.zhimg.com/v2-2d164412c306dbcc75fe2dfba879c157_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1128&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1128" data-rawheight="56" class="origin_image zh-lightbox-thumb lazy" width="1128" data-original="https://pic4.zhimg.com/v2-2d164412c306dbcc75fe2dfba879c157_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-2d164412c306dbcc75fe2dfba879c157_b.png"/></figure><p>此时，该算法是 polynomial 的。</p><h3>5. VI-Explore</h3><p>这里算法设计的目的是找出来一个 polynomial time 的算法。我们观察到前面算法不是 polynomial time 的根本原因是要求解一个 general sum 的问题，在该问题中两个玩家有着不一样的 payoff matrix，这导致求解比较困难。在另一类简化的问题中，两个玩家有着一样的 payoff matrix，这时候该问题退化为 zero-sum 问题，该问题的求解有 polynomial time 的有效解法。前面产生两个不同 payoff matrix 的原因是需要估计 upper 和 lower，这里考虑直接估计一个 unbiased 的 transition matrix。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-a6982591b9007219ce5817a9e90ac9e1_b.png" data-caption="" data-size="normal" data-rawwidth="1147" data-rawheight="117" class="origin_image zh-lightbox-thumb" width="1147" data-original="https://pic2.zhimg.com/v2-a6982591b9007219ce5817a9e90ac9e1_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1147&#39; height=&#39;117&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1147" data-rawheight="117" class="origin_image zh-lightbox-thumb lazy" width="1147" data-original="https://pic2.zhimg.com/v2-a6982591b9007219ce5817a9e90ac9e1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-a6982591b9007219ce5817a9e90ac9e1_b.png"/></figure><p>VI-Explore 就是先用 Jin et al 2020 的一篇文章里面的探索方法，把 transition matrix 估计得足够准确（用一个较好的探索方法+concentration），然后直接基于该估计的 transition matrix 来做 value iteration，得到相应的纳什均衡。可以证明只要 transition matrix 估计的足够准确，相应的纳什均衡也会比较准确。显然，这里的探索方法不如 UCB 方法好，因此 regret 也会相应差一些。</p><p>PAC bound 如下：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-350e9388c91cc565a578356fa572093f_b.jpg" data-caption="" data-size="normal" data-rawwidth="1159" data-rawheight="196" class="origin_image zh-lightbox-thumb" width="1159" data-original="https://pic4.zhimg.com/v2-350e9388c91cc565a578356fa572093f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1159&#39; height=&#39;196&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1159" data-rawheight="196" class="origin_image zh-lightbox-thumb lazy" width="1159" data-original="https://pic4.zhimg.com/v2-350e9388c91cc565a578356fa572093f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-350e9388c91cc565a578356fa572093f_b.jpg"/></figure><p>转化为 regret bound 如下：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8fa37526b937eb8c9203841806cb8788_b.jpg" data-caption="" data-size="normal" data-rawwidth="1138" data-rawheight="211" class="origin_image zh-lightbox-thumb" width="1138" data-original="https://pic1.zhimg.com/v2-8fa37526b937eb8c9203841806cb8788_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1138&#39; height=&#39;211&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1138" data-rawheight="211" class="origin_image zh-lightbox-thumb lazy" width="1138" data-original="https://pic1.zhimg.com/v2-8fa37526b937eb8c9203841806cb8788_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8fa37526b937eb8c9203841806cb8788_b.jpg"/></figure><h3>6. Lower bound</h3><p>之前有人给出了该问题的 lower bound</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-6488b3b4dfe207db1337f4bd4293569f_b.png" data-caption="" data-size="normal" data-rawwidth="1158" data-rawheight="86" class="origin_image zh-lightbox-thumb" width="1158" data-original="https://pic4.zhimg.com/v2-6488b3b4dfe207db1337f4bd4293569f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1158&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1158" data-rawheight="86" class="origin_image zh-lightbox-thumb lazy" width="1158" data-original="https://pic4.zhimg.com/v2-6488b3b4dfe207db1337f4bd4293569f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-6488b3b4dfe207db1337f4bd4293569f_b.png"/></figure><p>这里说，在两种简化的情况下，已知的算法是能够达到该 lower bound 的：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-2da6cbfeadbb3c36dbddda533a4e1260_b.jpg" data-caption="" data-size="normal" data-rawwidth="1163" data-rawheight="233" class="origin_image zh-lightbox-thumb" width="1163" data-original="https://pic1.zhimg.com/v2-2da6cbfeadbb3c36dbddda533a4e1260_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1163&#39; height=&#39;233&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1163" data-rawheight="233" class="origin_image zh-lightbox-thumb lazy" width="1163" data-original="https://pic1.zhimg.com/v2-2da6cbfeadbb3c36dbddda533a4e1260_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-2da6cbfeadbb3c36dbddda533a4e1260_b.jpg"/></figure><p>注意到这里用的是 weak regret，并且前面的 lower bound 也适用于该 weak regret。weak regret 的定义如下 </p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-6395ef4ef87c6697edf7fffb3553d50f_b.png" data-caption="" data-size="normal" data-rawwidth="1179" data-rawheight="163" class="origin_image zh-lightbox-thumb" width="1179" data-original="https://pic4.zhimg.com/v2-6395ef4ef87c6697edf7fffb3553d50f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1179&#39; height=&#39;163&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1179" data-rawheight="163" class="origin_image zh-lightbox-thumb lazy" width="1179" data-original="https://pic4.zhimg.com/v2-6395ef4ef87c6697edf7fffb3553d50f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-6395ef4ef87c6697edf7fffb3553d50f_b.png"/></figure><p>注意到它和文章中 regret 的差别在于：weak regret 是 max(sum(·))，而 regret 是 sum(max(·))。显然，regret ≥ weak regret。</p><p></p></div></div><div class="ContentItem-time">发布于 2020-03-06</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19580349" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">计算机科学</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19551275" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">人工智能</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 39 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 39</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>9 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="6f16b37c-2783-41fb-ac71-03566d46c376" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="6f16b37c-2783-41fb-ac71-03566d46c376">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"111191211":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":111191211,"title":"【强化学习 109】Provable Self-play","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F111191211","imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0517af4d255678cada7334315727ab36_b.jpg","titleImage":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0517af4d255678cada7334315727ab36_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-21e3ea69ef9f8856e30a2424ea4f6041_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1199\" data-rawheight=\"480\" data-watermark=\"watermark\" data-original-src=\"v2-21e3ea69ef9f8856e30a2424ea4f6041\" data-watermark-src=\"v2-df091787180082a09993a292c3fea1b5\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-21e3ea69ef9f8856e30a2424ea4f6041_r.png\"\u002F\u003E针对 self-play 的强化学习问题，基于 UCB 的思想给出 provably efficient 算法。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2002.04017\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBai, Yu, and Chi Jin. &#34;Provable Self-Play Algorithms for Competitive Reinforcement Learning.&#34; arXiv preprint arXiv:2002.04017 (2020).\u003C\u002Fa\u003E特色在实践中，很多…","created":1583465987,"updated":1583465987,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1488,"imageHeight":482,"content":"\u003Cp\u003E针对 self-play 的强化学习问题，基于 UCB 的思想给出 provably efficient 算法。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F2002.04017\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBai, Yu, and Chi Jin. &#34;Provable Self-Play Algorithms for Competitive Reinforcement Learning.&#34; arXiv preprint arXiv:2002.04017 (2020).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E在实践中，很多强化学习方法应用到了 self-play 的方法来训练强化学习智能体，取得了很好的效果。比如本专栏前面介绍过的针对 AlphaGo、StarCraft 等任务的算法都用到了这种训练方法。这里把在相互竞争环境中的 self-play 形式化设定为一个 Markov game，然后针对这个问题提出了 provably efficient 的算法。具体来说，给出了\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E基于 UCB 思想的 PPAD-complete 复杂度的 regret \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29+\" alt=\"\\tilde{O}(\\sqrt{T}) \" eeimg=\"1\"\u002F\u003E 的算法\u003C\u002Fli\u003E\u003Cli\u003E基于 explore-then-exploit 思想的 polynomial 复杂度的 regret \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%28T%5E%7B2%2F3%7D%29\" alt=\"\\tilde{O}(T^{2\u002F3})\" eeimg=\"1\"\u002F\u003E 的算法\u003C\u002Fli\u003E\u003Cli\u003E给出了 lower bound，并且以上算法和 lower bound 直接存在一个 gap\u003C\u002Fli\u003E\u003Cli\u003E给出了两种简化情形下能够 match lower bound 的算法\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E总体上来说，本文是第一个 self-play RL 上的 provably efficient 算法。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-df091787180082a09993a292c3fea1b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1199\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"1199\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-df091787180082a09993a292c3fea1b5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1199&#39; height=&#39;480&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1199\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1199\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-df091787180082a09993a292c3fea1b5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-df091787180082a09993a292c3fea1b5_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1、相关工作\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cb\u003E单智能体强化学习\u003C\u002Fb\u003E：tabular episodic case model-based method \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%28%5Csqrt%7BH%5E2+SAT%7D%29+\" alt=\"\\tilde{O}(\\sqrt{H^2 SAT}) \" eeimg=\"1\"\u002F\u003E （Azar et al 2017) model-free method \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%28%5Csqrt%7BH%5E3SAT%7D%29+\" alt=\"\\tilde{O}(\\sqrt{H^3SAT}) \" eeimg=\"1\"\u002F\u003E (Jin et al 2018)，而相应的lower-bound 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5COmega%28%5Csqrt%7BH%5E2+SAT%7D%29+\" alt=\"\\Omega(\\sqrt{H^2 SAT}) \" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EMarkov game（MG）\u003C\u002Fb\u003E：之前有较多的工作基于 known transition 和 known reward，即不是强化学习的设定。Wei et al 2017, Jia et al 2019, Sidford et al 2019 在强化学习设定上解决该问题，但是需要对于 MG 或者采样的方式做一些假设，从而某种程度上回避最为关键的探索问题。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EAdversarial opponents\u003C\u002Fb\u003E：注意这一类问题和 MG 不一样，在这一类问题中对手可以是一个和智能体作对的另一个玩家，也可以是专门干扰智能体，比如改变智能体拿到的 reward。MG 中的对手对于 reward 有一个明确的目标，最后能达到一个纳什均衡；但是这类问题则不会达到纳什均衡。\u003C\u002Fp\u003E\u003Ch3\u003E2、问题设定\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003ETabular episodic zero-sum Markov game\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-aec2590008cb8d841a362d6b3c894d06_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"1304\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-aec2590008cb8d841a362d6b3c894d06_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1304&#39; height=&#39;459&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1304\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-aec2590008cb8d841a362d6b3c894d06_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-aec2590008cb8d841a362d6b3c894d06_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到在本文的设定中，每一轮两个玩家同时给出其行动，并且一个玩家目标是最大化奖励，而另一个玩家的目标是最小化这个相同的奖励，因此是一个 zero-sum 博弈。多智能体强化学习里面有很多研究有 decentralized 的设定，这里没有这种，就是两个玩家同步学习。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003EValue function\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E相应地，可以定义价值函数。从 s 出发，两个玩家分别遵循相应的策略，最后获得的奖励和。注意到，对于 max player 来说，这个 value function 数值越大越好；对于 min player 来说，这个 value function 的数值越小越好。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1bd716b068959544e08a9783ab4f45a7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1126\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb\" width=\"1126\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1bd716b068959544e08a9783ab4f45a7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1126&#39; height=&#39;88&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1126\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1126\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1bd716b068959544e08a9783ab4f45a7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1bd716b068959544e08a9783ab4f45a7_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E相应有 Q 函数\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-383faf3816a581b86f533ff6bc684ff5_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb\" width=\"1133\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-383faf3816a581b86f533ff6bc684ff5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1133&#39; height=&#39;103&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1133\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-383faf3816a581b86f533ff6bc684ff5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-383faf3816a581b86f533ff6bc684ff5_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EV 和 Q 之间的关系以及 Bellman 方程\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-48e7485e8204c7e9f817798a9cb8b9a5_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1175\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb\" width=\"1175\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-48e7485e8204c7e9f817798a9cb8b9a5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1175&#39; height=&#39;176&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1175\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1175\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-48e7485e8204c7e9f817798a9cb8b9a5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-48e7485e8204c7e9f817798a9cb8b9a5_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003EBest response\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E注意到这里有两个玩家，当一个玩家策略固定的时候，对于另外一个玩家，就存在一种最优的策略。比如对于 max player 的一个固定策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E ，那么存在一个 min player 的 best response \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnu%5E%5Cdagger%28%5Cmu%29\" alt=\"\\nu^\\dagger(\\mu)\" eeimg=\"1\"\u002F\u003E ，满足\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f4804d82721627ef3e2722c45576ca0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"41\" class=\"origin_image zh-lightbox-thumb\" width=\"910\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f4804d82721627ef3e2722c45576ca0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;910&#39; height=&#39;41&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"41\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"910\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f4804d82721627ef3e2722c45576ca0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f4804d82721627ef3e2722c45576ca0_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E考虑对手每次都给出相对于自己的 best response，那么自己也能找到一个最优的策略，使得对手是最优策略的时候，自己也能最大化自己的利益。记这种情况下，自己的策略为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu%5E%2A%2C+%5Cnu%5E%2A\" alt=\"\\mu^*, \\nu^*\" eeimg=\"1\"\u002F\u003E ：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-44af26d7bda2bf90cf3e506b4776c7d7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb\" width=\"1133\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-44af26d7bda2bf90cf3e506b4776c7d7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1133&#39; height=&#39;74&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1133\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-44af26d7bda2bf90cf3e506b4776c7d7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-44af26d7bda2bf90cf3e506b4776c7d7_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E根据 minmax theorem，有\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d343f2bc1e8bb53ab6999a45b07222c3_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"1123\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d343f2bc1e8bb53ab6999a45b07222c3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1123&#39; height=&#39;66&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1123\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d343f2bc1e8bb53ab6999a45b07222c3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d343f2bc1e8bb53ab6999a45b07222c3_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003ERegret\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E下面定义这种问题下的 regret，注意算法设计的目标就是要在给定 K 轮中 minimize regret。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ac1d12117d1b22cb0f8595b3cfb8975b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb\" width=\"1178\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ac1d12117d1b22cb0f8595b3cfb8975b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1178&#39; height=&#39;233&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1178\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ac1d12117d1b22cb0f8595b3cfb8975b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ac1d12117d1b22cb0f8595b3cfb8975b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E直接看这个定义可能不是很能够理解这个 regret 定义的含义，观察下面关系可以发现，该 regret 的含义就是每一轮每个玩家给出的策略的性能相对于该玩家可能达到的最好策略性能之间的损失和。某个玩家的策略性能需要用价值函数来衡量，而价值函数依赖于对手的策略，因此假设对手采取相对于自己策略的 best response。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c739f62df51a5e52f7515c62ae8d55a0_b.png\" data-size=\"normal\" data-rawwidth=\"1135\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb\" width=\"1135\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c739f62df51a5e52f7515c62ae8d55a0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1135&#39; height=&#39;80&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1135\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1135\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c739f62df51a5e52f7515c62ae8d55a0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c739f62df51a5e52f7515c62ae8d55a0_b.png\"\u002F\u003E\u003Cfigcaption\u003E注意到，根据 minmax theorem，中间两项相等，就消掉了。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003ETurn-based games\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这个设定还可以概括 turn-based games。就是轮到玩家一需要操作的时候，认为另外一个玩家的动作空间大小为 1，并且把这个唯一能够操作的动作设置为一个哑变量。\u003C\u002Fp\u003E\u003Ch3\u003E3. Value iteration - upper\u002Flower confidence bound (VI-ULCB)\u003C\u002Fh3\u003E\u003Cp\u003E本专栏前面讲过 Jin Chi 的 LinearMDP，其算法和这个类似，也是基于 UCB 的；其算法（或者 Azar et al 2017）可以被概括如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dabce07e055c6e22df0074a75894c41a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1124\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dabce07e055c6e22df0074a75894c41a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1124&#39; height=&#39;116&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1124\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dabce07e055c6e22df0074a75894c41a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dabce07e055c6e22df0074a75894c41a_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E而这里有两个玩家，max player 希望价值函数越大越好，min player 希望价值函数越小越好；仿照这类似的思路，可以让 max player 照着 upper confidence bound 探索，让 min player 照着 lower confidence bound 探索，这样自然想到如下算法：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-375699a35043c9a54cf161df149b4eb6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1141\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"1141\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-375699a35043c9a54cf161df149b4eb6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1141&#39; height=&#39;158&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1141\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1141\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-375699a35043c9a54cf161df149b4eb6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-375699a35043c9a54cf161df149b4eb6_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E但是仔细想来，这个初步的想法有一个问题，因为估计出来的 Q 函数不仅取决于一个玩家的策略，还需要取决于另外一个玩家的策略；因此，我们需要同时确定两个玩家的策略，即找到两个玩家的策略使得它们 jointly greedy。把这两个 Q 函数看做两个玩家的 payoff matrix，然后认为 jointly greedy 就是找到这个 general-sum game 中的纳什均衡：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0d1d11364ce0b10364e0b4e1cd3879e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"1150\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0d1d11364ce0b10364e0b4e1cd3879e2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1150&#39; height=&#39;217&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1150\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0d1d11364ce0b10364e0b4e1cd3879e2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0d1d11364ce0b10364e0b4e1cd3879e2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E因此，该算法最终可以被概括为 \u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f31e6c104b1318cd2e14a59c54d969cb_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1188\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1188\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f31e6c104b1318cd2e14a59c54d969cb_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1188&#39; height=&#39;152&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1188\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1188\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f31e6c104b1318cd2e14a59c54d969cb_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f31e6c104b1318cd2e14a59c54d969cb_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E详细的算法框图如下\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e1b5f8223687f003518976ec68941c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1442\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb\" width=\"1442\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e1b5f8223687f003518976ec68941c0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1442&#39; height=&#39;792&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1442\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1442\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e1b5f8223687f003518976ec68941c0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e1b5f8223687f003518976ec68941c0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E4. 算法分析\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003ERegret bound\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E该算法可以达到如下 regret\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30ae8ce2beee002f34d21afcd797bdbf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1145\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb\" width=\"1145\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30ae8ce2beee002f34d21afcd797bdbf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1145&#39; height=&#39;239&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1145\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1145\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30ae8ce2beee002f34d21afcd797bdbf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30ae8ce2beee002f34d21afcd797bdbf_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E证明思路和 Linear MDP 相似，关键步骤可以概括如下\u003C\u002Fp\u003E\u003Cp\u003EA. 在每一层上，估计的 transition 距离真实 transition 的误差随着样本数目的增多而减小（concentration），相应的误差项刚好和给出的 exploration bonus 相匹配。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b979d97e78911987e8031bf373ef3e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1160\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b979d97e78911987e8031bf373ef3e8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1160&#39; height=&#39;204&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1160\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b979d97e78911987e8031bf373ef3e8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b979d97e78911987e8031bf373ef3e8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EB. 在每一层上，由于加上了 exploration bonus \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_t\" alt=\"\\beta_t\" eeimg=\"1\"\u002F\u003E ，估计出来的 upper 和 lower 大概率包含了真实价值函数的范围。其实质就是把前面的 concentration 展开写出来，注意到前一个 concentration 并没有规定价值函数得是实际遇到的价值函数（下面红色写出的），因此这个不等式也可以应用到相应的 sup 和 inf。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5d605c1b765d1ef00fa95fab58c35170_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1157\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"1157\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5d605c1b765d1ef00fa95fab58c35170_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1157&#39; height=&#39;94&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1157\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1157\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5d605c1b765d1ef00fa95fab58c35170_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5d605c1b765d1ef00fa95fab58c35170_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EC. 把 regret 中的每一项放缩为 upper 和 lower；然后从最后一层到第一层逐层递归，计算出地第一层的 regret bound。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26d1bc6e3dc483e4ca955e1e3cfa67e2_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"105\" class=\"origin_image zh-lightbox-thumb\" width=\"1130\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26d1bc6e3dc483e4ca955e1e3cfa67e2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1130&#39; height=&#39;105&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"105\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1130\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26d1bc6e3dc483e4ca955e1e3cfa67e2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26d1bc6e3dc483e4ca955e1e3cfa67e2_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003EPAC bound\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003ERegret bound 可以被转化为如下 PAC bound\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f3edaf96f345926c937c1b69b7e79ce3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1173\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb\" width=\"1173\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f3edaf96f345926c937c1b69b7e79ce3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1173&#39; height=&#39;245&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1173\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1173\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f3edaf96f345926c937c1b69b7e79ce3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f3edaf96f345926c937c1b69b7e79ce3_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003ERuntime\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E该算法由于需要用到一个 Nash_General_Sum 的子程序，而这个子程序不是 polynomial 的，因此该算法不是 polynomial 的。不过该算法在 MG 的一个特殊情形，即 turn-based game 的时候是 polynomial 的。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Cu\u003ETurn-based game\u003C\u002Fu\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E在 MG 情形下，需要求解 Nash_General_Sum，它的输入是两个各自的 payoff 矩阵，需要求解出两个玩家各自的策略，而求解其纳什均衡没有 polynomial 解法\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-17d9cd20c29f96761187e66f468ef103_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1171\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb\" width=\"1171\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-17d9cd20c29f96761187e66f468ef103_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1171&#39; height=&#39;67&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1171\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1171\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-17d9cd20c29f96761187e66f468ef103_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-17d9cd20c29f96761187e66f468ef103_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在 turn-based 情形下，每轮只有一个玩家可以做实质性的操作，因此只需要求解一个玩家的策略，相应的求解问题则变成了一个简单的 LP 问题\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d164412c306dbcc75fe2dfba879c157_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1128\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb\" width=\"1128\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d164412c306dbcc75fe2dfba879c157_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1128&#39; height=&#39;56&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1128\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1128\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d164412c306dbcc75fe2dfba879c157_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d164412c306dbcc75fe2dfba879c157_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E此时，该算法是 polynomial 的。\u003C\u002Fp\u003E\u003Ch3\u003E5. VI-Explore\u003C\u002Fh3\u003E\u003Cp\u003E这里算法设计的目的是找出来一个 polynomial time 的算法。我们观察到前面算法不是 polynomial time 的根本原因是要求解一个 general sum 的问题，在该问题中两个玩家有着不一样的 payoff matrix，这导致求解比较困难。在另一类简化的问题中，两个玩家有着一样的 payoff matrix，这时候该问题退化为 zero-sum 问题，该问题的求解有 polynomial time 的有效解法。前面产生两个不同 payoff matrix 的原因是需要估计 upper 和 lower，这里考虑直接估计一个 unbiased 的 transition matrix。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a6982591b9007219ce5817a9e90ac9e1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1147\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb\" width=\"1147\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a6982591b9007219ce5817a9e90ac9e1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1147&#39; height=&#39;117&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1147\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1147\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a6982591b9007219ce5817a9e90ac9e1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a6982591b9007219ce5817a9e90ac9e1_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EVI-Explore 就是先用 Jin et al 2020 的一篇文章里面的探索方法，把 transition matrix 估计得足够准确（用一个较好的探索方法+concentration），然后直接基于该估计的 transition matrix 来做 value iteration，得到相应的纳什均衡。可以证明只要 transition matrix 估计的足够准确，相应的纳什均衡也会比较准确。显然，这里的探索方法不如 UCB 方法好，因此 regret 也会相应差一些。\u003C\u002Fp\u003E\u003Cp\u003EPAC bound 如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-350e9388c91cc565a578356fa572093f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1159\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb\" width=\"1159\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-350e9388c91cc565a578356fa572093f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1159&#39; height=&#39;196&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1159\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1159\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-350e9388c91cc565a578356fa572093f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-350e9388c91cc565a578356fa572093f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E转化为 regret bound 如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8fa37526b937eb8c9203841806cb8788_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb\" width=\"1138\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8fa37526b937eb8c9203841806cb8788_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1138&#39; height=&#39;211&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1138\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8fa37526b937eb8c9203841806cb8788_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8fa37526b937eb8c9203841806cb8788_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E6. Lower bound\u003C\u002Fh3\u003E\u003Cp\u003E之前有人给出了该问题的 lower bound\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6488b3b4dfe207db1337f4bd4293569f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"1158\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6488b3b4dfe207db1337f4bd4293569f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1158&#39; height=&#39;86&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1158\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6488b3b4dfe207db1337f4bd4293569f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6488b3b4dfe207db1337f4bd4293569f_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这里说，在两种简化的情况下，已知的算法是能够达到该 lower bound 的：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2da6cbfeadbb3c36dbddda533a4e1260_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1163\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb\" width=\"1163\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2da6cbfeadbb3c36dbddda533a4e1260_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1163&#39; height=&#39;233&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1163\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1163\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2da6cbfeadbb3c36dbddda533a4e1260_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2da6cbfeadbb3c36dbddda533a4e1260_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到这里用的是 weak regret，并且前面的 lower bound 也适用于该 weak regret。weak regret 的定义如下 \u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6395ef4ef87c6697edf7fffb3553d50f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1179\" data-rawheight=\"163\" class=\"origin_image zh-lightbox-thumb\" width=\"1179\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6395ef4ef87c6697edf7fffb3553d50f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1179&#39; height=&#39;163&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1179\" data-rawheight=\"163\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1179\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6395ef4ef87c6697edf7fffb3553d50f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6395ef4ef87c6697edf7fffb3553d50f_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到它和文章中 regret 的差别在于：weak regret 是 max(sum(·))，而 regret 是 sum(max(·))。显然，regret ≥ weak regret。\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19580349","type":"topic","id":"19580349","name":"计算机科学"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19551275","type":"topic","id":"19551275","name":"人工智能"}],"voteupCount":39,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":9,"contributions":[{"id":23203684,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 109】Provable Self-play - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F111191211 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"gw_mweb_launch-1","expPrefix":"gw_mweb_launch","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"1"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F111191211","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F111191211","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>