<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习理论 57】StatisticalRL 1 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="这是UIUC姜楠老师开设的CS598统计强化学习（理论）课程的第一讲（第一部分），由于最近在研究state abstraction相关的东西，因此准备借此机会刷一下这个课程，强化一下自己在RL理论方面的水平。原文传送门CS598 No…"/><meta data-react-helmet="true" property="og:title" content="【强化学习理论 57】StatisticalRL 1"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/65599582"/><meta data-react-helmet="true" property="og:description" content="这是UIUC姜楠老师开设的CS598统计强化学习（理论）课程的第一讲（第一部分），由于最近在研究state abstraction相关的东西，因此准备借此机会刷一下这个课程，强化一下自己在RL理论方面的水平。原文传送门CS598 No…"/><meta data-react-helmet="true" property="og:image" content="https://pic3.zhimg.com/v2-dd88831d08188edc9831170cff17dbe0_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:65599582,&quot;title&quot;:&quot;【强化学习理论 57】StatisticalRL 1&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic3.zhimg.com/v2-dd88831d08188edc9831170cff17dbe0_1200x500.jpg" alt="【强化学习理论 57】StatisticalRL 1"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习理论 57】StatisticalRL 1</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">18 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>这是UIUC姜楠老师开设的<a href="https://link.zhihu.com/?target=http%3A//nanjiang.cs.illinois.edu/cs598/" class=" wrap external" target="_blank" rel="nofollow noreferrer">CS598统计强化学习（理论）课程</a>的第一讲（第一部分），由于最近在研究state abstraction相关的东西，因此准备借此机会刷一下这个课程，强化一下自己在RL理论方面的水平。</p><h2>原文传送门</h2><a target="_blank" href="https://link.zhihu.com/?target=http%3A//nanjiang.cs.illinois.edu/files/cs598/note1.pdf" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage"><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">CS598 Note1</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>nanjiang.cs.illinois.edu</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><h2>备注</h2><p>此系列Notes写的很好，如果大家有兴趣可以直接去看Note。我这里主要贴截图，在重难点的地方讲解一下（如果我能搞懂的话）。大家不懂的地方可以上来看看我这里有没有讲清楚。</p><hr/><h2>一、马可夫决策过程</h2><p>首先是定义</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d2b4ea34015f9f4e0f3a5faaa2ce0030_b.jpg" data-caption="" data-size="normal" data-rawwidth="1188" data-rawheight="409" class="origin_image zh-lightbox-thumb" width="1188" data-original="https://pic1.zhimg.com/v2-d2b4ea34015f9f4e0f3a5faaa2ce0030_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1188&#39; height=&#39;409&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1188" data-rawheight="409" class="origin_image zh-lightbox-thumb lazy" width="1188" data-original="https://pic1.zhimg.com/v2-d2b4ea34015f9f4e0f3a5faaa2ce0030_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-d2b4ea34015f9f4e0f3a5faaa2ce0030_b.jpg"/></figure><p>比较特别的是</p><ul><li>这里的概率转移函数 <img src="https://www.zhihu.com/equation?tex=P" alt="P" eeimg="1"/> 是state-action到状态空间上概率分布 <img src="https://www.zhihu.com/equation?tex=%5CDelta%28%5Cmathcal%7BS%7D%29" alt="\Delta(\mathcal{S})" eeimg="1"/> 的函数，写的比较形式化；</li><li>假设了奖励都是非负数，并且有界，应该是为了后面的分析方便；这其实不影响，有界不是一个很强的假设，并且非负只需要把reward都平移一下即可。</li></ul><h3>1.1. 与环境的交互</h3><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-649943d246c85634d872317a9b34da8a_b.jpg" data-caption="" data-size="normal" data-rawwidth="1185" data-rawheight="398" class="origin_image zh-lightbox-thumb" width="1185" data-original="https://pic3.zhimg.com/v2-649943d246c85634d872317a9b34da8a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1185&#39; height=&#39;398&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1185" data-rawheight="398" class="origin_image zh-lightbox-thumb lazy" width="1185" data-original="https://pic3.zhimg.com/v2-649943d246c85634d872317a9b34da8a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-649943d246c85634d872317a9b34da8a_b.jpg"/></figure><p>定义轨迹什么都很普通，需要注意的是，有的时候分析起来初始状态分布 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 比较关键，这时候就把它显式写到MDP里面。</p><h3>1.2. 策略和价值函数</h3><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c302e3191e2ec5f0d36bf85b14d3a205_b.png" data-caption="" data-size="normal" data-rawwidth="1178" data-rawheight="85" class="origin_image zh-lightbox-thumb" width="1178" data-original="https://pic2.zhimg.com/v2-c302e3191e2ec5f0d36bf85b14d3a205_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1178&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1178" data-rawheight="85" class="origin_image zh-lightbox-thumb lazy" width="1178" data-original="https://pic2.zhimg.com/v2-c302e3191e2ec5f0d36bf85b14d3a205_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c302e3191e2ec5f0d36bf85b14d3a205_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-644a7d0cb3ffae0185700466f3503c79_b.png" data-caption="" data-size="normal" data-rawwidth="1198" data-rawheight="116" class="origin_image zh-lightbox-thumb" width="1198" data-original="https://pic2.zhimg.com/v2-644a7d0cb3ffae0185700466f3503c79_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1198&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1198" data-rawheight="116" class="origin_image zh-lightbox-thumb lazy" width="1198" data-original="https://pic2.zhimg.com/v2-644a7d0cb3ffae0185700466f3503c79_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-644a7d0cb3ffae0185700466f3503c79_b.png"/></figure><p>策略主要分为确定性策略（deterministic policy）和随机策略（stochastic policy），注意到这里主要讲的是稳定（stationary）的策略，个人理解是每次遇到同样的状态都会按照同样的概率分布去选择行动，这样的策略叫做稳定的。而如果策略取决于现在是走的第几步（timestep）那么就是non-stationay的。本专栏讲的Distributional RL涉及到这个。</p><p>这里对于确定性策略和随机策略都用了同样的notation <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-a3a8d858f0c19cabaa621697ed8f74ba_b.png" data-caption="" data-size="normal" data-rawwidth="1169" data-rawheight="188" class="origin_image zh-lightbox-thumb" width="1169" data-original="https://pic3.zhimg.com/v2-a3a8d858f0c19cabaa621697ed8f74ba_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1169&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1169" data-rawheight="188" class="origin_image zh-lightbox-thumb lazy" width="1169" data-original="https://pic3.zhimg.com/v2-a3a8d858f0c19cabaa621697ed8f74ba_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-a3a8d858f0c19cabaa621697ed8f74ba_b.png"/></figure><p>目标是最大化expected discounted sum of rewards。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-cbc4340b8cf9a7f45169eeac9791ef94_b.jpg" data-caption="" data-size="normal" data-rawwidth="1212" data-rawheight="352" class="origin_image zh-lightbox-thumb" width="1212" data-original="https://pic1.zhimg.com/v2-cbc4340b8cf9a7f45169eeac9791ef94_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1212&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1212" data-rawheight="352" class="origin_image zh-lightbox-thumb lazy" width="1212" data-original="https://pic1.zhimg.com/v2-cbc4340b8cf9a7f45169eeac9791ef94_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-cbc4340b8cf9a7f45169eeac9791ef94_b.jpg"/></figure><p>刚刚对于reward的假设这里就能得到一个小结论了，即优化的目标的上界为 <img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7BR_%5Cmax%7D%7B1-%5Cgamma%7D" alt="\dfrac{R_\max}{1-\gamma}" eeimg="1"/> 。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-330e0bb46035bc9b703650f9ca051ebe_b.jpg" data-caption="" data-size="normal" data-rawwidth="1170" data-rawheight="346" class="origin_image zh-lightbox-thumb" width="1170" data-original="https://pic3.zhimg.com/v2-330e0bb46035bc9b703650f9ca051ebe_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1170&#39; height=&#39;346&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1170" data-rawheight="346" class="origin_image zh-lightbox-thumb lazy" width="1170" data-original="https://pic3.zhimg.com/v2-330e0bb46035bc9b703650f9ca051ebe_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-330e0bb46035bc9b703650f9ca051ebe_b.jpg"/></figure><p>V函数和Q函数的定义。</p><h3>1.3. Policy evaluation 下的 Bellman 方程</h3><p>Policy evaluation讲的是给定一个策略，然后估计其价值函数；与之相对的是control，即求一个最优策略。</p><p>Policy evaluation下V函数和Q函数的关系如下。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-7e26b4378a3605d311d1a55296902450_b.jpg" data-caption="" data-size="normal" data-rawwidth="1185" data-rawheight="203" class="origin_image zh-lightbox-thumb" width="1185" data-original="https://pic1.zhimg.com/v2-7e26b4378a3605d311d1a55296902450_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1185&#39; height=&#39;203&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1185" data-rawheight="203" class="origin_image zh-lightbox-thumb lazy" width="1185" data-original="https://pic1.zhimg.com/v2-7e26b4378a3605d311d1a55296902450_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-7e26b4378a3605d311d1a55296902450_b.jpg"/></figure><p>为了书写方便，这里就直接当策略是确定性的，如果是随机策略，就在外面套一个期望。</p><p><b><i>下面是对于理论推导比较关键的一个准备工作，即把相关的量都表示成矩阵-向量形式。</i></b></p><p>考虑到 <img src="https://www.zhihu.com/equation?tex=V%5E%5Cpi" alt="V^\pi" eeimg="1"/> 是关于状态的函数，因此对于每个状态 <img src="https://www.zhihu.com/equation?tex=s%5Cin+%5Cmathcal%7BS%7D" alt="s\in \mathcal{S}" eeimg="1"/> 都有一个状态函数值，这样 <img src="https://www.zhihu.com/equation?tex=V%5E%5Cpi" alt="V^\pi" eeimg="1"/> 可以被写成一个向量 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7B%7C%5Cmathcal%7BS%7D%7C%7D" alt="\mathbb{R}^{|\mathcal{S}|}" eeimg="1"/> ；同理， <img src="https://www.zhihu.com/equation?tex=R" alt="R" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=Q%5E%5Cpi" alt="Q^\pi" eeimg="1"/> 可以被写成向量 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7B%7C%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BA%7D%7C%7D" alt="\mathbb{R}^{|\mathcal{S} \times \mathcal{A}|}" eeimg="1"/> 。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-26e8d2a9f2aa475c53be248490701e66_b.jpg" data-caption="" data-size="normal" data-rawwidth="1184" data-rawheight="258" class="origin_image zh-lightbox-thumb" width="1184" data-original="https://pic3.zhimg.com/v2-26e8d2a9f2aa475c53be248490701e66_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1184&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1184" data-rawheight="258" class="origin_image zh-lightbox-thumb lazy" width="1184" data-original="https://pic3.zhimg.com/v2-26e8d2a9f2aa475c53be248490701e66_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-26e8d2a9f2aa475c53be248490701e66_b.jpg"/></figure><p>关于策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 的转移概率可以表示为矩阵，矩阵中的每个元素表示了从一个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> ，在策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 下一步转移到另外一个状态 <img src="https://www.zhihu.com/equation?tex=s%27" alt="s&#39;" eeimg="1"/> 的概率。它同时包含了策略的随机性和环境的随机性。如果规定从状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 出发，那么 <img src="https://www.zhihu.com/equation?tex=P%28s%2C%5Cpi%29" alt="P(s,\pi)" eeimg="1"/> 就是一个向量，表示从状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 出发转移到状态空间中其他状态的概率。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-af1fa58d82db90b22224f49b41d64336_b.png" data-caption="" data-size="normal" data-rawwidth="1203" data-rawheight="114" class="origin_image zh-lightbox-thumb" width="1203" data-original="https://pic3.zhimg.com/v2-af1fa58d82db90b22224f49b41d64336_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1203&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1203" data-rawheight="114" class="origin_image zh-lightbox-thumb lazy" width="1203" data-original="https://pic3.zhimg.com/v2-af1fa58d82db90b22224f49b41d64336_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-af1fa58d82db90b22224f49b41d64336_b.png"/></figure><p>关于策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 的奖励函数可以表示为向量形式，它表示从每个状态出发按照策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 行动能够得到的期望单步奖励。</p><p><b><i>下面开始推导Bellman Equation</i></b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-beb915b2d2272a94f0a2c0c30b919123_b.jpg" data-caption="" data-size="normal" data-rawwidth="1199" data-rawheight="269" class="origin_image zh-lightbox-thumb" width="1199" data-original="https://pic4.zhimg.com/v2-beb915b2d2272a94f0a2c0c30b919123_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1199&#39; height=&#39;269&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1199" data-rawheight="269" class="origin_image zh-lightbox-thumb lazy" width="1199" data-original="https://pic4.zhimg.com/v2-beb915b2d2272a94f0a2c0c30b919123_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-beb915b2d2272a94f0a2c0c30b919123_b.jpg"/></figure><p>第一行就是利用的前面的关于V函数和Q函数之间关系的定义，第二行就是把策略的随机性和环境的随机性都吸收到单步转移概率矩阵里面，第三行就利用前面的定义做了改写。</p><p>由于上式对于每个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 都成立，这样就能写成矩阵形式</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8ceb8b97da9f203d6ee358be4637cb70_b.png" data-caption="" data-size="normal" data-rawwidth="1169" data-rawheight="124" class="origin_image zh-lightbox-thumb" width="1169" data-original="https://pic1.zhimg.com/v2-8ceb8b97da9f203d6ee358be4637cb70_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1169&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1169" data-rawheight="124" class="origin_image zh-lightbox-thumb lazy" width="1169" data-original="https://pic1.zhimg.com/v2-8ceb8b97da9f203d6ee358be4637cb70_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8ceb8b97da9f203d6ee358be4637cb70_b.png"/></figure><p>同时注意到以下矩阵是可逆的，证明的方法在下面写得很清楚了，即证明对于任意的向量，矩阵向量乘都不等于零，那么该矩阵就是非奇异矩阵，非奇异矩阵和可逆矩阵是等价的。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-1ebaebb89cd0f0675c44a61f4c6611e6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1188" data-rawheight="414" class="origin_image zh-lightbox-thumb" width="1188" data-original="https://pic3.zhimg.com/v2-1ebaebb89cd0f0675c44a61f4c6611e6_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1188&#39; height=&#39;414&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1188" data-rawheight="414" class="origin_image zh-lightbox-thumb lazy" width="1188" data-original="https://pic3.zhimg.com/v2-1ebaebb89cd0f0675c44a61f4c6611e6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-1ebaebb89cd0f0675c44a61f4c6611e6_b.jpg"/></figure><p>另外注意到如果奖励只依赖于当前的状态，那么 <img src="https://www.zhihu.com/equation?tex=V%5E%5Cpi" alt="V^\pi" eeimg="1"/> 关于策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 的部分就只有前面一项，并且 <img src="https://www.zhihu.com/equation?tex=V%5E%5Cpi" alt="V^\pi" eeimg="1"/> 可以看做是对于各个状态上奖励的线性组合，组合系数就是前面这一项对应的矩阵，这里叫做discounted state occupancy，我看到有些论文里面叫state visitation frequency，是类似的东西。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-fb8efd19c826c4c7e7c488db263648f6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1187" data-rawheight="267" class="origin_image zh-lightbox-thumb" width="1187" data-original="https://pic3.zhimg.com/v2-fb8efd19c826c4c7e7c488db263648f6_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1187&#39; height=&#39;267&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1187" data-rawheight="267" class="origin_image zh-lightbox-thumb lazy" width="1187" data-original="https://pic3.zhimg.com/v2-fb8efd19c826c4c7e7c488db263648f6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-fb8efd19c826c4c7e7c488db263648f6_b.jpg"/></figure><h3>1.4. Optimal control 下的 Bellman 方程</h3><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-6ee7ec008d0badd72422166cad4de99d_b.jpg" data-caption="" data-size="normal" data-rawwidth="1219" data-rawheight="591" class="origin_image zh-lightbox-thumb" width="1219" data-original="https://pic2.zhimg.com/v2-6ee7ec008d0badd72422166cad4de99d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1219&#39; height=&#39;591&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1219" data-rawheight="591" class="origin_image zh-lightbox-thumb lazy" width="1219" data-original="https://pic2.zhimg.com/v2-6ee7ec008d0badd72422166cad4de99d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-6ee7ec008d0badd72422166cad4de99d_b.jpg"/></figure><p>都是比较平常的定义。值得一提的是，<i><b>总存在deterministic and stationary的策略能够同时对于所有状态最大化其V函数，对于所有的状态和行动最大化其Q函数</b></i>，该定理note里面没给证明。下面给出Bellman optimality operator的定义。对于policy evaluation情况其实也有相应的Bellman operator，区别在于optimal的情况下含有一个取max的操作，而policy evaluation的情况下是对于策略求期望。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-9df9dff2bbbec60afd84a0307c8c6ee2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1190" data-rawheight="327" class="origin_image zh-lightbox-thumb" width="1190" data-original="https://pic3.zhimg.com/v2-9df9dff2bbbec60afd84a0307c8c6ee2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1190&#39; height=&#39;327&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1190" data-rawheight="327" class="origin_image zh-lightbox-thumb lazy" width="1190" data-original="https://pic3.zhimg.com/v2-9df9dff2bbbec60afd84a0307c8c6ee2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-9df9dff2bbbec60afd84a0307c8c6ee2_b.jpg"/></figure><h3>1.5. MDP Setup</h3><p>这里提出了其他一些可能的（前面没有涉及到的）MDP设定，并且给出了它们和本课程所讨论MDP设定直接的联系。</p><p><b><i>Finite horizon and episodic setting</i></b></p><p>我们这里讨论的是infinite horizon discounted setting。</p><p>一种比较常见的是infinite-horizon average reward setting，这种设定下由于是对于奖励取平均，累积奖励是不会发散啦，但是所面临的问题是需要额外的条件才能有一个良好的价值函数定义（比如遍历性条件）。如何理解这一点呢？考虑一个MDP有两个状态 <img src="https://www.zhihu.com/equation?tex=s_1" alt="s_1" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=s_2" alt="s_2" eeimg="1"/> ，对于任意行动 <img src="https://www.zhihu.com/equation?tex=s_1+%5Cto+s_2" alt="s_1 \to s_2" eeimg="1"/> 产生奖励 <img src="https://www.zhihu.com/equation?tex=r%3E0" alt="r&gt;0" eeimg="1"/> ， <img src="https://www.zhihu.com/equation?tex=s_2+%5Cto+s_2" alt="s_2 \to s_2" eeimg="1"/> 产生奖励 <img src="https://www.zhihu.com/equation?tex=0" alt="0" eeimg="1"/> 。在这样的定义下，无论 <img src="https://www.zhihu.com/equation?tex=r" alt="r" eeimg="1"/> 取值如何，每个状态上的价值函数都为 <img src="https://www.zhihu.com/equation?tex=0" alt="0" eeimg="1"/> 。这显然很不好。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-21c0449574d1884a355a8bc113bcb204_b.jpg" data-caption="" data-size="normal" data-rawwidth="1171" data-rawheight="226" class="origin_image zh-lightbox-thumb" width="1171" data-original="https://pic1.zhimg.com/v2-21c0449574d1884a355a8bc113bcb204_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1171&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1171" data-rawheight="226" class="origin_image zh-lightbox-thumb lazy" width="1171" data-original="https://pic1.zhimg.com/v2-21c0449574d1884a355a8bc113bcb204_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-21c0449574d1884a355a8bc113bcb204_b.jpg"/></figure><p>另一种比较常见的是finite-horizon undiscounted setting。</p><p>这种情况可以构造出了另外的一个MDP使得它和我们前面讨论infinite horizon discounted setting匹配。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-9648169e698430710fccf3ba7ee0a015_b.png" data-caption="" data-size="normal" data-rawwidth="1168" data-rawheight="155" class="origin_image zh-lightbox-thumb" width="1168" data-original="https://pic2.zhimg.com/v2-9648169e698430710fccf3ba7ee0a015_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1168&#39; height=&#39;155&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1168" data-rawheight="155" class="origin_image zh-lightbox-thumb lazy" width="1168" data-original="https://pic2.zhimg.com/v2-9648169e698430710fccf3ba7ee0a015_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-9648169e698430710fccf3ba7ee0a015_b.png"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-e208cdc1f370e5a193051af035964548_b.jpg" data-caption="" data-size="normal" data-rawwidth="1173" data-rawheight="418" class="origin_image zh-lightbox-thumb" width="1173" data-original="https://pic1.zhimg.com/v2-e208cdc1f370e5a193051af035964548_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1173&#39; height=&#39;418&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1173" data-rawheight="418" class="origin_image zh-lightbox-thumb lazy" width="1173" data-original="https://pic1.zhimg.com/v2-e208cdc1f370e5a193051af035964548_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e208cdc1f370e5a193051af035964548_b.jpg"/></figure><p><b><i>Stochastic or negative rewards</i></b></p><p>对于随机性的奖励或者奖励不仅仅依赖 <img src="https://www.zhihu.com/equation?tex=s_t%2C+a_t" alt="s_t, a_t" eeimg="1"/> （比如可以依赖 <img src="https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D" alt="s_{t+1}" eeimg="1"/> ），我们可以把相应的随机性取期望之后当它是确定性的（marginalize out），这样做不影响价值函数们，只会对于采样的效率有一定的影响。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1180" data-rawheight="385" class="origin_image zh-lightbox-thumb" width="1180" data-original="https://pic3.zhimg.com/v2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1180&#39; height=&#39;385&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1180" data-rawheight="385" class="origin_image zh-lightbox-thumb lazy" width="1180" data-original="https://pic3.zhimg.com/v2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_b.jpg"/></figure><p>如果奖励可能是负数，我们只需要做一个常数的平移即可。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-beae7e65bf98b4103f5b570db74a63ec_b.jpg" data-caption="" data-size="normal" data-rawwidth="1181" data-rawheight="262" class="origin_image zh-lightbox-thumb" width="1181" data-original="https://pic1.zhimg.com/v2-beae7e65bf98b4103f5b570db74a63ec_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1181&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1181" data-rawheight="262" class="origin_image zh-lightbox-thumb lazy" width="1181" data-original="https://pic1.zhimg.com/v2-beae7e65bf98b4103f5b570db74a63ec_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-beae7e65bf98b4103f5b570db74a63ec_b.jpg"/></figure><p></p></div></div><div class="ContentItem-time">编辑于 2019-05-14</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 18 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 18</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>3 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="0f1cd1c9-76e2-4f74-aa51-d2bffeda5d64" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="0f1cd1c9-76e2-4f74-aa51-d2bffeda5d64">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"65599582":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":65599582,"title":"【强化学习理论 57】StatisticalRL 1","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F65599582","imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dd88831d08188edc9831170cff17dbe0_b.jpg","titleImage":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dd88831d08188edc9831170cff17dbe0_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0f63f40579dc17fff5715793152908f4_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1188\" data-rawheight=\"409\" data-watermark=\"watermark\" data-original-src=\"v2-0f63f40579dc17fff5715793152908f4\" data-watermark-src=\"v2-d2b4ea34015f9f4e0f3a5faaa2ce0030\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0f63f40579dc17fff5715793152908f4_r.png\"\u002F\u003E这是UIUC姜楠老师开设的\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fnanjiang.cs.illinois.edu\u002Fcs598\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ECS598统计强化学习（理论）课程\u003C\u002Fa\u003E的第一讲（第一部分），由于最近在研究state abstraction相关的东西，因此准备借此机会刷一下这个课程，强化一下自己在RL理论方面的水平。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fnanjiang.cs.illinois.edu\u002Ffiles\u002Fcs598\u002Fnote1.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ECS598 Note1\u003C\u002Fa\u003E备注此系列Notes写的很好，如果大家有兴…","created":1557747167,"updated":1557796832,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1270,"imageHeight":371,"content":"\u003Cp\u003E这是UIUC姜楠老师开设的\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fnanjiang.cs.illinois.edu\u002Fcs598\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ECS598统计强化学习（理论）课程\u003C\u002Fa\u003E的第一讲（第一部分），由于最近在研究state abstraction相关的东西，因此准备借此机会刷一下这个课程，强化一下自己在RL理论方面的水平。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fnanjiang.cs.illinois.edu\u002Ffiles\u002Fcs598\u002Fnote1.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ECS598 Note1\u003C\u002Fa\u003E\u003Ch2\u003E备注\u003C\u002Fh2\u003E\u003Cp\u003E此系列Notes写的很好，如果大家有兴趣可以直接去看Note。我这里主要贴截图，在重难点的地方讲解一下（如果我能搞懂的话）。大家不懂的地方可以上来看看我这里有没有讲清楚。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E一、马可夫决策过程\u003C\u002Fh2\u003E\u003Cp\u003E首先是定义\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2b4ea34015f9f4e0f3a5faaa2ce0030_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1188\" data-rawheight=\"409\" class=\"origin_image zh-lightbox-thumb\" width=\"1188\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2b4ea34015f9f4e0f3a5faaa2ce0030_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1188&#39; height=&#39;409&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1188\" data-rawheight=\"409\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1188\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2b4ea34015f9f4e0f3a5faaa2ce0030_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2b4ea34015f9f4e0f3a5faaa2ce0030_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E比较特别的是\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E这里的概率转移函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P\" alt=\"P\" eeimg=\"1\"\u002F\u003E 是state-action到状态空间上概率分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta%28%5Cmathcal%7BS%7D%29\" alt=\"\\Delta(\\mathcal{S})\" eeimg=\"1\"\u002F\u003E 的函数，写的比较形式化；\u003C\u002Fli\u003E\u003Cli\u003E假设了奖励都是非负数，并且有界，应该是为了后面的分析方便；这其实不影响，有界不是一个很强的假设，并且非负只需要把reward都平移一下即可。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E1.1. 与环境的交互\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-649943d246c85634d872317a9b34da8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1185\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb\" width=\"1185\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-649943d246c85634d872317a9b34da8a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1185&#39; height=&#39;398&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1185\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1185\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-649943d246c85634d872317a9b34da8a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-649943d246c85634d872317a9b34da8a_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E定义轨迹什么都很普通，需要注意的是，有的时候分析起来初始状态分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 比较关键，这时候就把它显式写到MDP里面。\u003C\u002Fp\u003E\u003Ch3\u003E1.2. 策略和价值函数\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c302e3191e2ec5f0d36bf85b14d3a205_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"1178\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c302e3191e2ec5f0d36bf85b14d3a205_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1178&#39; height=&#39;85&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1178\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c302e3191e2ec5f0d36bf85b14d3a205_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c302e3191e2ec5f0d36bf85b14d3a205_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-644a7d0cb3ffae0185700466f3503c79_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-644a7d0cb3ffae0185700466f3503c79_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1198&#39; height=&#39;116&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1198\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-644a7d0cb3ffae0185700466f3503c79_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-644a7d0cb3ffae0185700466f3503c79_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E策略主要分为确定性策略（deterministic policy）和随机策略（stochastic policy），注意到这里主要讲的是稳定（stationary）的策略，个人理解是每次遇到同样的状态都会按照同样的概率分布去选择行动，这样的策略叫做稳定的。而如果策略取决于现在是走的第几步（timestep）那么就是non-stationay的。本专栏讲的Distributional RL涉及到这个。\u003C\u002Fp\u003E\u003Cp\u003E这里对于确定性策略和随机策略都用了同样的notation \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a3a8d858f0c19cabaa621697ed8f74ba_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1169\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb\" width=\"1169\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a3a8d858f0c19cabaa621697ed8f74ba_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1169&#39; height=&#39;188&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1169\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1169\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a3a8d858f0c19cabaa621697ed8f74ba_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a3a8d858f0c19cabaa621697ed8f74ba_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E目标是最大化expected discounted sum of rewards。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cbc4340b8cf9a7f45169eeac9791ef94_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1212\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1212\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cbc4340b8cf9a7f45169eeac9791ef94_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1212&#39; height=&#39;352&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1212\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1212\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cbc4340b8cf9a7f45169eeac9791ef94_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cbc4340b8cf9a7f45169eeac9791ef94_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E刚刚对于reward的假设这里就能得到一个小结论了，即优化的目标的上界为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdfrac%7BR_%5Cmax%7D%7B1-%5Cgamma%7D\" alt=\"\\dfrac{R_\\max}{1-\\gamma}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-330e0bb46035bc9b703650f9ca051ebe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1170\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb\" width=\"1170\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-330e0bb46035bc9b703650f9ca051ebe_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1170&#39; height=&#39;346&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1170\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1170\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-330e0bb46035bc9b703650f9ca051ebe_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-330e0bb46035bc9b703650f9ca051ebe_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EV函数和Q函数的定义。\u003C\u002Fp\u003E\u003Ch3\u003E1.3. Policy evaluation 下的 Bellman 方程\u003C\u002Fh3\u003E\u003Cp\u003EPolicy evaluation讲的是给定一个策略，然后估计其价值函数；与之相对的是control，即求一个最优策略。\u003C\u002Fp\u003E\u003Cp\u003EPolicy evaluation下V函数和Q函数的关系如下。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7e26b4378a3605d311d1a55296902450_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1185\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb\" width=\"1185\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7e26b4378a3605d311d1a55296902450_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1185&#39; height=&#39;203&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1185\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1185\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7e26b4378a3605d311d1a55296902450_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7e26b4378a3605d311d1a55296902450_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E为了书写方便，这里就直接当策略是确定性的，如果是随机策略，就在外面套一个期望。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E下面是对于理论推导比较关键的一个准备工作，即把相关的量都表示成矩阵-向量形式。\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E考虑到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi\" alt=\"V^\\pi\" eeimg=\"1\"\u002F\u003E 是关于状态的函数，因此对于每个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s%5Cin+%5Cmathcal%7BS%7D\" alt=\"s\\in \\mathcal{S}\" eeimg=\"1\"\u002F\u003E 都有一个状态函数值，这样 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi\" alt=\"V^\\pi\" eeimg=\"1\"\u002F\u003E 可以被写成一个向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BR%7D%5E%7B%7C%5Cmathcal%7BS%7D%7C%7D\" alt=\"\\mathbb{R}^{|\\mathcal{S}|}\" eeimg=\"1\"\u002F\u003E ；同理， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R\" alt=\"R\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%5Cpi\" alt=\"Q^\\pi\" eeimg=\"1\"\u002F\u003E 可以被写成向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BR%7D%5E%7B%7C%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BA%7D%7C%7D\" alt=\"\\mathbb{R}^{|\\mathcal{S} \\times \\mathcal{A}|}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26e8d2a9f2aa475c53be248490701e66_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1184\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"1184\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26e8d2a9f2aa475c53be248490701e66_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1184&#39; height=&#39;258&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1184\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1184\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26e8d2a9f2aa475c53be248490701e66_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-26e8d2a9f2aa475c53be248490701e66_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E关于策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 的转移概率可以表示为矩阵，矩阵中的每个元素表示了从一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E ，在策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 下一步转移到另外一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s%27\" alt=\"s&#39;\" eeimg=\"1\"\u002F\u003E 的概率。它同时包含了策略的随机性和环境的随机性。如果规定从状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 出发，那么 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28s%2C%5Cpi%29\" alt=\"P(s,\\pi)\" eeimg=\"1\"\u002F\u003E 就是一个向量，表示从状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 出发转移到状态空间中其他状态的概率。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af1fa58d82db90b22224f49b41d64336_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1203\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1203\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af1fa58d82db90b22224f49b41d64336_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1203&#39; height=&#39;114&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1203\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1203\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af1fa58d82db90b22224f49b41d64336_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af1fa58d82db90b22224f49b41d64336_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E关于策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 的奖励函数可以表示为向量形式，它表示从每个状态出发按照策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 行动能够得到的期望单步奖励。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E下面开始推导Bellman Equation\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-beb915b2d2272a94f0a2c0c30b919123_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1199\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb\" width=\"1199\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-beb915b2d2272a94f0a2c0c30b919123_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1199&#39; height=&#39;269&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1199\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1199\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-beb915b2d2272a94f0a2c0c30b919123_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-beb915b2d2272a94f0a2c0c30b919123_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E第一行就是利用的前面的关于V函数和Q函数之间关系的定义，第二行就是把策略的随机性和环境的随机性都吸收到单步转移概率矩阵里面，第三行就利用前面的定义做了改写。\u003C\u002Fp\u003E\u003Cp\u003E由于上式对于每个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 都成立，这样就能写成矩阵形式\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8ceb8b97da9f203d6ee358be4637cb70_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1169\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1169\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8ceb8b97da9f203d6ee358be4637cb70_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1169&#39; height=&#39;124&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1169\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1169\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8ceb8b97da9f203d6ee358be4637cb70_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8ceb8b97da9f203d6ee358be4637cb70_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E同时注意到以下矩阵是可逆的，证明的方法在下面写得很清楚了，即证明对于任意的向量，矩阵向量乘都不等于零，那么该矩阵就是非奇异矩阵，非奇异矩阵和可逆矩阵是等价的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1ebaebb89cd0f0675c44a61f4c6611e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1188\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb\" width=\"1188\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1ebaebb89cd0f0675c44a61f4c6611e6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1188&#39; height=&#39;414&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1188\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1188\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1ebaebb89cd0f0675c44a61f4c6611e6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1ebaebb89cd0f0675c44a61f4c6611e6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E另外注意到如果奖励只依赖于当前的状态，那么 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi\" alt=\"V^\\pi\" eeimg=\"1\"\u002F\u003E 关于策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 的部分就只有前面一项，并且 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi\" alt=\"V^\\pi\" eeimg=\"1\"\u002F\u003E 可以看做是对于各个状态上奖励的线性组合，组合系数就是前面这一项对应的矩阵，这里叫做discounted state occupancy，我看到有些论文里面叫state visitation frequency，是类似的东西。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fb8efd19c826c4c7e7c488db263648f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"267\" class=\"origin_image zh-lightbox-thumb\" width=\"1187\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fb8efd19c826c4c7e7c488db263648f6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1187&#39; height=&#39;267&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"267\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1187\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fb8efd19c826c4c7e7c488db263648f6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fb8efd19c826c4c7e7c488db263648f6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E1.4. Optimal control 下的 Bellman 方程\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6ee7ec008d0badd72422166cad4de99d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1219\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb\" width=\"1219\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6ee7ec008d0badd72422166cad4de99d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1219&#39; height=&#39;591&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1219\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1219\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6ee7ec008d0badd72422166cad4de99d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6ee7ec008d0badd72422166cad4de99d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E都是比较平常的定义。值得一提的是，\u003Ci\u003E\u003Cb\u003E总存在deterministic and stationary的策略能够同时对于所有状态最大化其V函数，对于所有的状态和行动最大化其Q函数\u003C\u002Fb\u003E\u003C\u002Fi\u003E，该定理note里面没给证明。下面给出Bellman optimality operator的定义。对于policy evaluation情况其实也有相应的Bellman operator，区别在于optimal的情况下含有一个取max的操作，而policy evaluation的情况下是对于策略求期望。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9df9dff2bbbec60afd84a0307c8c6ee2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb\" width=\"1190\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9df9dff2bbbec60afd84a0307c8c6ee2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1190&#39; height=&#39;327&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1190\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9df9dff2bbbec60afd84a0307c8c6ee2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9df9dff2bbbec60afd84a0307c8c6ee2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E1.5. MDP Setup\u003C\u002Fh3\u003E\u003Cp\u003E这里提出了其他一些可能的（前面没有涉及到的）MDP设定，并且给出了它们和本课程所讨论MDP设定直接的联系。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EFinite horizon and episodic setting\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E我们这里讨论的是infinite horizon discounted setting。\u003C\u002Fp\u003E\u003Cp\u003E一种比较常见的是infinite-horizon average reward setting，这种设定下由于是对于奖励取平均，累积奖励是不会发散啦，但是所面临的问题是需要额外的条件才能有一个良好的价值函数定义（比如遍历性条件）。如何理解这一点呢？考虑一个MDP有两个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"\u002F\u003E ，对于任意行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_1+%5Cto+s_2\" alt=\"s_1 \\to s_2\" eeimg=\"1\"\u002F\u003E 产生奖励 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r%3E0\" alt=\"r&gt;0\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_2+%5Cto+s_2\" alt=\"s_2 \\to s_2\" eeimg=\"1\"\u002F\u003E 产生奖励 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=0\" alt=\"0\" eeimg=\"1\"\u002F\u003E 。在这样的定义下，无论 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r\" alt=\"r\" eeimg=\"1\"\u002F\u003E 取值如何，每个状态上的价值函数都为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=0\" alt=\"0\" eeimg=\"1\"\u002F\u003E 。这显然很不好。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-21c0449574d1884a355a8bc113bcb204_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1171\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb\" width=\"1171\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-21c0449574d1884a355a8bc113bcb204_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1171&#39; height=&#39;226&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1171\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1171\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-21c0449574d1884a355a8bc113bcb204_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-21c0449574d1884a355a8bc113bcb204_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E另一种比较常见的是finite-horizon undiscounted setting。\u003C\u002Fp\u003E\u003Cp\u003E这种情况可以构造出了另外的一个MDP使得它和我们前面讨论infinite horizon discounted setting匹配。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9648169e698430710fccf3ba7ee0a015_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1168\" data-rawheight=\"155\" class=\"origin_image zh-lightbox-thumb\" width=\"1168\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9648169e698430710fccf3ba7ee0a015_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1168&#39; height=&#39;155&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1168\" data-rawheight=\"155\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1168\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9648169e698430710fccf3ba7ee0a015_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9648169e698430710fccf3ba7ee0a015_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e208cdc1f370e5a193051af035964548_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1173\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb\" width=\"1173\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e208cdc1f370e5a193051af035964548_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1173&#39; height=&#39;418&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1173\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1173\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e208cdc1f370e5a193051af035964548_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e208cdc1f370e5a193051af035964548_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EStochastic or negative rewards\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E对于随机性的奖励或者奖励不仅仅依赖 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t%2C+a_t\" alt=\"s_t, a_t\" eeimg=\"1\"\u002F\u003E （比如可以依赖 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_%7Bt%2B1%7D\" alt=\"s_{t+1}\" eeimg=\"1\"\u002F\u003E ），我们可以把相应的随机性取期望之后当它是确定性的（marginalize out），这样做不影响价值函数们，只会对于采样的效率有一定的影响。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"385\" class=\"origin_image zh-lightbox-thumb\" width=\"1180\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1180&#39; height=&#39;385&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"385\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1180\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-9dbe5fc8d6911fd72681d0e6dc3b2ce6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E如果奖励可能是负数，我们只需要做一个常数的平移即可。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-beae7e65bf98b4103f5b570db74a63ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1181\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"1181\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-beae7e65bf98b4103f5b570db74a63ec_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1181&#39; height=&#39;262&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1181\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1181\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-beae7e65bf98b4103f5b570db74a63ec_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-beae7e65bf98b4103f5b570db74a63ec_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":18,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":3,"contributions":[{"id":20834052,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习理论 57】StatisticalRL 1 - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F65599582 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F65599582","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F65599582","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>