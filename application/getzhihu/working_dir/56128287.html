<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习入门 3】强化学习策略梯度类方法 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="目录让策略梯度形式简单 —— Policy Gradient Theorem用最简单的采样方法来实现 —— REINFORCE两个网络一台戏 —— Actor-Critic应用到高维行动空间 —— DPG策略梯度类也可以玩Atari —— 从DQN到DDPG引言强化…"/><meta data-react-helmet="true" property="og:title" content="【强化学习入门 3】强化学习策略梯度类方法"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/56128287"/><meta data-react-helmet="true" property="og:description" content="目录让策略梯度形式简单 —— Policy Gradient Theorem用最简单的采样方法来实现 —— REINFORCE两个网络一台戏 —— Actor-Critic应用到高维行动空间 —— DPG策略梯度类也可以玩Atari —— 从DQN到DDPG引言强化…"/><meta data-react-helmet="true" property="og:image" content=""/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:56128287,&quot;title&quot;:&quot;【强化学习入门 3】强化学习策略梯度类方法&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习入门 3】强化学习策略梯度类方法</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">31 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><h2><b>目录</b></h2><ul><li>让策略梯度形式简单 —— Policy Gradient Theorem</li><li>用最简单的采样方法来实现 —— REINFORCE</li><li>两个网络一台戏 —— Actor-Critic</li><li>应用到高维行动空间 —— DPG</li><li>策略梯度类也可以玩Atari —— 从DQN到DDPG</li></ul><h2><b>引言</b></h2><p>强化学习领域最为核心的问题是<b><i>控制问题</i></b>（control problem），即找到一个好的策略，使得个体按照这个策略进行行动能够在期望上取得最大的收益。而强化学习基础的数学模型就是MDP，当这个MDP足够简单的时候，我们就可以使用<b><i>基于解析的方法</i></b>找到最优策略，比如Linear Programming类的方法和前面介绍到的Dynamic Programming类的算法。不过在现实应用中遇到的强化学习问题常常都比较复杂，因此，我们需要采用<b><i>基于采样的方法</i></b>。这一讲和前一讲都是属于这一类。</p><p>我们上一讲就是介绍的这一类基于状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 或者行动价值函数 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> 的控制问题解法，这里称之为策略迭代类算法，或者叫做<b><i>基于价值函数的方法</i></b>（value-based method）。价值函数衡量了在某个状态或者采取某个行动之后能够取得的收益。在确定性的环境中，当我们知道了每个状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 的数值之后，策略就可以选择使得下一个状态期望状态价值函数最大的行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 。在随机的环境中，状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 就不太好用了，因为它不能直接指导我们进行行动的选择。在控制问题中更常用的是行动价值函数 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> ，它描述了在某个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 下采取不同的行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 产生的期望收益，这样我们的最优策略就可以写成 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28a%5C%7Cs%29+%3D+arg%5Cmax_%7Ba%27%7D+Q%28s%2Ca%27%29" alt="\pi(a\|s) = arg\max_{a&#39;} Q(s,a&#39;)" eeimg="1"/> 。</p><p>在这一讲中用另外一种更为直接方式来解决强化学习中的控制问题——策略梯度类方法，它属于另外一类，即<b><i>基于策略的方法</i></b>（policy-based method）。基于价值函数的方法是先估计得到价值函数，然后直接依赖价值函数找到对应的最优策略。而基于策略的方法更为直接，直接参数化策略，然后对着一个性能评价函数去优化控制策略的参数，如果使用梯度上升方法来进行优化，那么得到的就是我们这一讲里面要了解到的<b><i>策略梯度类方法</i></b>（policy gradient method）；如果使用一些无导数优化方法来优化这些控制策略的参数，那么得到的就是我们下一讲里面会讲到的<b><i>无导数优化方法</i></b>（derivative-free method）。</p><h2><b>让策略梯度形式简单 —— Policy Gradient Theorem</b></h2><p>前面已经说到基于策略的方法就是直接把策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%29" alt="\pi(s,a)" eeimg="1"/> 进行参数化表述，即 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%7C%5Ctheta%29" alt="\pi(s,a|\theta)" eeimg="1"/> ，为了保证参数化的结果能够满足概率的归一化条件，一般令 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%7C%5Ctheta%29+%3D+%5Cdfrac%7B%5Cexp%28h%28s%2Ca%29%29%7D%7B%5Csum_%7Ba%27%7D+%5Cexp%28h%28s%2Ca%27%29%29%7D" alt="\pi(s,a|\theta) = \dfrac{\exp(h(s,a))}{\sum_{a&#39;} \exp(h(s,a&#39;))}" eeimg="1"/> 。如果是线性参数化那么 <img src="https://www.zhihu.com/equation?tex=h%28s%2Ca%7C%5Ctheta%29+%3D+%5Cphi%28s%2Ca%29%5ET+%5Ctheta" alt="h(s,a|\theta) = \phi(s,a)^T \theta" eeimg="1"/> ；如果使用神经网络，为了保证概率的归一化条件，网络 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%7C%5Ctheta%29" alt="\pi(s,a|\theta)" eeimg="1"/> 的最后一层需要设定为softmax层。</p><p>接下来定义策略性能的度量。</p><ol><li>对于回合制任务，可以定义性能的度量为一回合中能够取得的总收益， <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+R_k%5D" alt="J(\theta) = \mathbb{E}[\sum_{k=1}^T R_k]" eeimg="1"/> ；</li><li>对于回合制任务，也可以在回合制任务里面引入衰减率，来更多地强调近期的收益，即 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+%5Cgamma%5E%7Bk-1%7D+R_k%5D" alt="J(\theta) = \mathbb{E}[\sum_{k=1}^T \gamma^{k-1} R_k]" eeimg="1"/> ；</li><li>对于连续任务，由于任务不会终结，那么总收益常常会发散，这时就必须加入衰减率，来保证其收敛，这样可以得到相应的性能度量， <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5E%5Cinfty+%5Cgamma%5E%7Bk-1%7D+R_k%5D" alt="J(\theta) = \mathbb{E}[\sum_{k=1}^\infty \gamma^{k-1} R_k]" eeimg="1"/> ；</li><li>对于连续任务，如果希望长期收益能和近期收益一样重要，即 <img src="https://www.zhihu.com/equation?tex=%5Cgamma%3D1" alt="\gamma=1" eeimg="1"/> ，这样如果直接使用上面的定义会导致 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 发散；这时可以使用平均每步收益来作为性能的度量，即 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Clim_%7Bt%5Cto+%5Cinfty%7D+%5Cdfrac%7B1%7D%7Bt%7D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+R_k%5D+%3D+%5Clim_%7Bt%5Cto+%5Cinfty%7D+%5Cmathbb%7BE%7D%5BR_k%5D" alt="J(\theta) = \lim_{t\to \infty} \dfrac{1}{t} \mathbb{E}[\sum_{k=1}^T R_k] = \lim_{t\to \infty} \mathbb{E}[R_k]" eeimg="1"/> 。</li></ol><p>做好了策略的参数化 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2C+a%7C+%5Ctheta%29" alt="\pi(s, a| \theta)" eeimg="1"/> 并且定义好了策略性能的度量 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 之后，我们就自然想到使用梯度上升方法来进行参数的更新， <img src="https://www.zhihu.com/equation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+%5Calpha+%5Cnabla_%5Ctheta+J%28%5Ctheta%29" alt="\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)" eeimg="1"/> 。</p><p>那么下面的问题就转化为了如何计算 <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29" alt="\nabla_\theta J(\theta)" eeimg="1"/> 。为了简洁起见，从现在开始使用第一种定义 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+R_k%5D" alt="J(\theta) = \mathbb{E}[\sum_{k=1}^T R_k]" eeimg="1"/> ，之后我们再说明其他定义对应的情况。为了去对 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 求 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 的梯度，我们至少要把 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 写成含有 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2C+a%7C+%5Ctheta%29" alt="\pi(s, a| \theta)" eeimg="1"/> 的形式吧。下面我们就开始做这件事情。</p><p><img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Csum_s+%5Crho_%5Cpi%28s%29+%5Csum_a+%5Cpi%28s%2C+a+%7C+%5Ctheta%29+Q_%5Cpi%28s%2C+a%29" alt="J(\theta) = \sum_s \rho_\pi(s) \sum_a \pi(s, a | \theta) Q_\pi(s, a)" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Crho_%5Cpi%28s%29+%3D+%5Csum_%7Bs%27%7D+%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D+p_0%28s%27%29+p%28s%27+%5Cto+s%2C+t%2C+%5Cpi%29" alt="\rho_\pi(s) = \sum_{s&#39;} \sum_{t=0}^{T-1} p_0(s&#39;) p(s&#39; \to s, t, \pi)" eeimg="1"/> 是策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 下的状态密度，它表示在一个回合中个体处于状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 的期望次数。之前的 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 是一个时序上连续的收益和的形式，现在我们把它按照状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 和行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 拆开分别统计，并且对状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 和行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 分别加上了 <img src="https://www.zhihu.com/equation?tex=%5Crho_%5Cpi%28s%29" alt="\rho_\pi(s)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2C+a+%7C+%5Ctheta%29" alt="\pi(s, a | \theta)" eeimg="1"/> 的权重。</p><p>现在就产生了一个担忧。当策略进行更新的时候，可能因为选择了更好的行动而使得 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2C+a+%7C+%5Ctheta%29+q_%5Cpi%28s%2C+a%29" alt="\pi(s, a | \theta) q_\pi(s, a)" eeimg="1"/> 变得更大，即在这个状态上能够获得更大的期望收益；但与此同时，策略的改变也会影响策略下的状态密度 <img src="https://www.zhihu.com/equation?tex=%5Crho_%5Cpi%28s%29" alt="\rho_\pi(s)" eeimg="1"/> ，这时策略的总体性能是否能够保证变得更好就需要存疑了。换句话说，如果解析地对于 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 求梯度，我们希望表达式里面只含有对于策略的梯度 <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+%5Cpi%28s%2C+a%7C%5Ctheta%29" alt="\nabla_\theta \pi(s, a|\theta)" eeimg="1"/> ，因为不管是线性的参数化还是神经网络，梯度计算都非常的方便；如果得到的表达式里面如果含有状态密度 <img src="https://www.zhihu.com/equation?tex=%5Crho_%5Cpi%28s%29" alt="\rho_\pi(s)" eeimg="1"/> 相对于参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 的导数就非常的不好办了，这样基本上没法计算。</p><p>下面要引出的<b><i>策略梯度定理</i></b>（policy gradient theorem）就给咱们带来了一个好消息，即 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 的梯度不含有状态密度相对参数的梯度。</p><p><img src="https://www.zhihu.com/equation?tex=+%5Cnabla+J%28%5Ctheta%29+%3D+%5Csum_s+%5Crho_%5Cpi%28s%29+%5Csum_a+%5Cnabla+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29" alt=" \nabla J(\theta) = \sum_s \rho_\pi(s) \sum_a \nabla \pi(s, a, \theta) Q_\pi (s, a)" eeimg="1"/> </p><p>形式上来看就是直接把求导放到了 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2C+a%2C+%5Ctheta%29" alt="\pi(s, a, \theta)" eeimg="1"/> ，但是推导过程并不是这么得无聊，具体推导见[1]。</p><p>对于 <img src="https://www.zhihu.com/equation?tex=J%28%5Ctheta%29" alt="J(\theta)" eeimg="1"/> 的第二种和第三种定义来说，与此的主要区别就是加入了衰减率，衰减率可以整理写到 <img src="https://www.zhihu.com/equation?tex=%5Crho_%5Cpi%28s%29" alt="\rho_\pi(s)" eeimg="1"/> 中，即 <img src="https://www.zhihu.com/equation?tex=%5Crho_%5Cpi%28s%29+%3D+%5Csum_%7Bs%27%7D+%5Csum_%7Bt%3D0%7D%5E%7B%28%5Ccdot%29%7D+%5Cgamma%5Et+p_0%28s%27%29+p%28s%27+%5Cto+s%2C+t%2C+%5Cpi%29" alt="\rho_\pi(s) = \sum_{s&#39;} \sum_{t=0}^{(\cdot)} \gamma^t p_0(s&#39;) p(s&#39; \to s, t, \pi)" eeimg="1"/> ，其中 <img src="https://www.zhihu.com/equation?tex=%28%5Ccdot%29" alt="(\cdot)" eeimg="1"/> 可以分别写出回合制和连续任务的形式。其对应的策略梯度定理和上面是完全一样的形式。值得注意的是，如果初始的概率分布 <img src="https://www.zhihu.com/equation?tex=p_0%28s%29" alt="p_0(s)" eeimg="1"/> 和策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 下的稳定分布 <img src="https://www.zhihu.com/equation?tex=%5Cmu_%5Cpi%28s%29" alt="\mu_\pi(s)" eeimg="1"/> 一致，那么不管是否有加衰减率，都有 <img src="https://www.zhihu.com/equation?tex=%5Crho_%5Cpi%28s%29+%5Cpropto+%5Cmu_%5Cpi%28s%29" alt="\rho_\pi(s) \propto \mu_\pi(s)" eeimg="1"/> 。</p><p>对于第四种定义来说，也可以写出同样形式的策略梯度定理，只不过此时 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2Ca%29+%3D+%5Cmathbb%7BE%7D%5BG_t%7CS_t+%3D+s%2C+A_t+%3D+a%2C+%5Cpi%5D" alt="Q_\pi(s,a) = \mathbb{E}[G_t|S_t = s, A_t = a, \pi]" eeimg="1"/> 的定义稍有不同，其中的 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> 不再是 <img src="https://www.zhihu.com/equation?tex=t" alt="t" eeimg="1"/> 时刻之后的总收益，而是 <img src="https://www.zhihu.com/equation?tex=t" alt="t" eeimg="1"/> 时刻之后的<b><i>差分收益</i></b>（differential return）， <img src="https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+-+J%28%5Ctheta%29+%2B+R_%7Bt%2B2%7D+-+J%28%5Ctheta%29+%2B+%5Ccdots" alt="G_t = R_{t+1} - J(\theta) + R_{t+2} - J(\theta) + \cdots" eeimg="1"/> 。</p><h2><b>用最简单的采样方法来实现 —— REINFORCE</b></h2><p>通过策略梯度定理得到了策略梯度的表示形式，注意到表达式中涉及到对于所有状态和所有行动的求和，这样的求和的计算量是很大的，如果我们能够把它进一步化成某个策略下的期望的形式，就可以通过采样的方式来对梯度进行估计了。根据前面提到的性质，假设初始状态分布和策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 下稳态分布一致，那么 <img src="https://www.zhihu.com/equation?tex=%5Cmu_%5Cpi%28s%29+%3D+%5Crho_%5Cpi%28s%29" alt="\mu_\pi(s) = \rho_\pi(s)" eeimg="1"/>。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cnabla+J%28%5Ctheta%29+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+%5Csum_a++%5Cpi%28s%2C+a%2C+%5Ctheta%29+%5Cdfrac%7B%5Cnabla+%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D%7B%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D+Q_%5Cpi+%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D_%5Cpi%5B%5Cnabla+%5Clog+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29%5D" alt="\nabla J(\theta) = \sum_s \mu_\pi(s) \sum_a  \pi(s, a, \theta) \dfrac{\nabla \pi(s, a, \theta)}{\pi(s, a, \theta)} Q_\pi (s, a) = \mathbb{E}_\pi[\nabla \log \pi(s, a, \theta) Q_\pi (s, a)]" eeimg="1"/> </p><p>期望内的表达式就是对于策略梯度的一个估计，现在还需要计算 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2Ca%29" alt="Q_\pi(s,a)" eeimg="1"/> ，可以使用蒙特卡洛方法采样得到从 <img src="https://www.zhihu.com/equation?tex=%28s%2Ca%29" alt="(s,a)" eeimg="1"/> 开始的收益 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> ，它是一个 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2Ca%29" alt="Q_\pi(s,a)" eeimg="1"/> 的无偏估计，由此得到的 <img src="https://www.zhihu.com/equation?tex=G_t+%5Cnabla+%5Clog+%5Cpi%28s%2Ca%2C%5Ctheta%29" alt="G_t \nabla \log \pi(s,a,\theta)" eeimg="1"/> 也是策略梯度的一个无偏估计。由此得到REINFORCE算法更新公式</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+%5Calpha+G_t+%5Cnabla+%5Clog+%5Cpi%28S_t%2C+A_t%2C+%5Ctheta%29" alt="\theta \leftarrow \theta + \alpha G_t \nabla \log \pi(S_t, A_t, \theta)" eeimg="1"/> </p><p>注意到只有当一个回合结束之后我们才能得到该轨迹上各个状态对应的 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> ，因此，该方法需要先采样一个完整的轨迹，然后再倒回来对每个状态更新参数。</p><p>该方法是无偏的，但是方差非常大，所以运行速度也很慢，在原文[2]中就提出了使用baseline来减少方差，这主要基于一个观察，即在行动价值函数 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2Ca%29" alt="Q_\pi(s,a)" eeimg="1"/> 上减去任意一个只与状态有关的函数 <img src="https://www.zhihu.com/equation?tex=b%28s%29" alt="b(s)" eeimg="1"/> 不影响策略梯度估计的期望，但是能选择一个这样的函数使得其方差最小。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Csum_s+%5Cmu_%5Cpi%28s%29+%5Csum_a+%5Cnabla+%5Cpi%28s%2C+a%2C+%5Ctheta%29+b+%28s%29+%26+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+b%28s%29+%5Csum_a+%5Cnabla_%5Ctheta+%5Cpi%28s%2C+a%2C+%5Ctheta%29+%5C%5C+%26+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+b%28s%29+%5Cnabla_%5Ctheta+%5Csum_a+%5Cpi%28s%2C+a%2C+%5Ctheta%29+%5C%5C+%26+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+b%28s%29+%5Cnabla_%5Ctheta+1+%3D+0+%5Cend%7Baligned%7D" alt="\begin{aligned} \sum_s \mu_\pi(s) \sum_a \nabla \pi(s, a, \theta) b (s) &amp; = \sum_s \mu_\pi(s) b(s) \sum_a \nabla_\theta \pi(s, a, \theta) \\ &amp; = \sum_s \mu_\pi(s) b(s) \nabla_\theta \sum_a \pi(s, a, \theta) \\ &amp; = \sum_s \mu_\pi(s) b(s) \nabla_\theta 1 = 0 \end{aligned}" eeimg="1"/> </p><p>那么怎样选择baseline呢？在[6]里面证明了（在线性近似下）选择状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 上的状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 作为baseline能够最大程度减小方差。这样，算法还需要同时维护一个参数 <img src="https://www.zhihu.com/equation?tex=w" alt="w" eeimg="1"/> 控制的状态价值函数 <img src="https://www.zhihu.com/equation?tex=v%28s%2C+w%29" alt="v(s, w)" eeimg="1"/> 用于baseline。该价值函数的估计使用蒙特卡洛方法，这样我们就得到REINFORCE with baseline的更新公式</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+w+%26+%5Cleftarrow+w+%2B+%5Calpha%5Ew+%28G_t+-+v%28S_t%2C+w%29%29+%5Cnabla_w+w%28S_t%2C+w%29+%5C%5C+%5Ctheta+%26+%5Cleftarrow+%5Ctheta+%2B+%5Calpha%5E%5Ctheta+%28G_t+-+v%28S_t%2C+w%29%29+%5Cnabla_%5Ctheta+%5Clog+%5Cpi%28S_t%2C+A_t%2C+%5Ctheta%29+%5Cend%7Baligned%7D" alt="\begin{aligned} w &amp; \leftarrow w + \alpha^w (G_t - v(S_t, w)) \nabla_w w(S_t, w) \\ \theta &amp; \leftarrow \theta + \alpha^\theta (G_t - v(S_t, w)) \nabla_\theta \log \pi(S_t, A_t, \theta) \end{aligned}" eeimg="1"/> </p><p>注意到两个公式里面都出现了 <img src="https://www.zhihu.com/equation?tex=%28G_t+-+v%28S_t%2C+w%29%29" alt="(G_t - v(S_t, w))" eeimg="1"/> 一项，但是其含义并不完全相同。状态价值函数参数 <img src="https://www.zhihu.com/equation?tex=w" alt="w" eeimg="1"/> 更新公式中的该项是对误差 <img src="https://www.zhihu.com/equation?tex=%28G_t+-+v%28S_t%2C+w%29%29%5E2" alt="(G_t - v(S_t, w))^2" eeimg="1"/> 求导得到的，函数逼近加上蒙特卡洛方法就会产生这一项；策略参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 更新公式中出现的这一项是原本的更新目标 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> 减去baseline  <img src="https://www.zhihu.com/equation?tex=v%28S_t%2C+w%29" alt="v(S_t, w)" eeimg="1"/> 的结果。</p><p>最后回答两个困扰过我的问题：</p><ol><li><b><i>名字叫做policy-based method为什么还是涉及到价值函数的估计？</i></b>policy-based和value-based的区别主要在于最后形成的策略是直接由参数控制的还是通过价值函数衍生出来的（比如相对于价值函数的greedy策略）。policy-based方法里面仍然可以用到价值函数的估计，只要最后的策略是直接由参数控制的，就不影响它称作policy-based method。</li><li><b><i>为什么Sutton书中（section 13.4）的REINFORCE算法更新公式中梯度上会乘上</i></b> <img src="https://www.zhihu.com/equation?tex=%5Cgamma%5Et" alt="\gamma^t" eeimg="1"/> <b><i>项？</i></b>书里面为了推导方便，假设了每一个回合的初态都是 <img src="https://www.zhihu.com/equation?tex=s_0" alt="s_0" eeimg="1"/> ，这就会导致远离 <img src="https://www.zhihu.com/equation?tex=s_0" alt="s_0" eeimg="1"/> 的态会因为衰减的缘故不那么重要，因此对于远离 <img src="https://www.zhihu.com/equation?tex=s_0" alt="s_0" eeimg="1"/> 的态需要减小其重要性。这里假设了初态分布和策略下的稳态分布相同，使用策略作用一步前后态的分布不变，这样每一个采样到的态不需要额外再加权重，就没有类似 <img src="https://www.zhihu.com/equation?tex=%5Cgamma%5Et" alt="\gamma^t" eeimg="1"/> 项。对于遍历的MDP来说，可以先按照策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 走足够多步，这样得到的分布可以认为差不多是 <img src="https://www.zhihu.com/equation?tex=%5Cmu_%5Cpi%28s%29" alt="\mu_\pi(s)" eeimg="1"/> ，然后再从此开始。</li></ol><h2><b>两个网络一台戏 —— Actor-Critic</b></h2><p><b>Advantage Actor-Critic (A2C)</b></p><p>REINFORCE方法基于采样，它是无偏的，但是它最大的问题是方差大，由此收敛缓慢。要解决收敛慢的问题，就需要用到武器库中的bootstrapping。也就是说，采样得到的 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> 使用相应的bootstrapped收益来使用 <img src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29" alt="R_{t+1} + \gamma v(S_{t+1}, w)" eeimg="1"/> 代替。（注意，这里不加说明地加上了衰减率 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"/> ）这样得到的更新公式和REINFORCE with baseline形式上类似</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+w+%26+%5Cleftarrow+w+%2B+%5Calpha%5Ew+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29+%5Cnabla_w+w%28S_t%2C+w%29+%5C%5C+%5Ctheta+%26+%5Cleftarrow+%5Ctheta+%2B+%5Calpha%5E%5Ctheta+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29+%5Cnabla_%5Ctheta+%5Clog+%5Cpi%28S_t%2C+A_t%2C+%5Ctheta%29+%5Cend%7Baligned%7D" alt="\begin{aligned} w &amp; \leftarrow w + \alpha^w (R_{t+1} + \gamma v(S_{t+1}, w) - v(S_t, w)) \nabla_w w(S_t, w) \\ \theta &amp; \leftarrow \theta + \alpha^\theta (R_{t+1} + \gamma v(S_{t+1}, w) - v(S_t, w)) \nabla_\theta \log \pi(S_t, A_t, \theta) \end{aligned}" eeimg="1"/> </p><p>同样，要注意第一个式子中的 <img src="https://www.zhihu.com/equation?tex=%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29" alt="(R_{t+1} + \gamma v(S_{t+1}, w) - v(S_t, w))" eeimg="1"/> 是semi-gradient TD(0)方法中的TD error，后面一个式子中的 <img src="https://www.zhihu.com/equation?tex=%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29" alt="(R_{t+1} + \gamma v(S_{t+1}, w) - v(S_t, w))" eeimg="1"/> 是bootstrapped target减去了baseline，形式上也是TD error。使用状态价值函数作为baseline其实就是使用了前面提到的Advantage <img src="https://www.zhihu.com/equation?tex=A%28s%2Ca%29+%3D+Q%28s%2Ca%29+-+V%28s%29" alt="A(s,a) = Q(s,a) - V(s)" eeimg="1"/> 作为目标，这样的方法也叫做<b><i>Advantage Actor-Critic (A2C)</i></b>。</p><p>这里的策略网络 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%2C%5Ctheta%29" alt="\pi(s,a,\theta)" eeimg="1"/> 就好像一样演员一样进行“表演”，而价值函数网络 <img src="https://www.zhihu.com/equation?tex=v%28s%2C+w%29" alt="v(s, w)" eeimg="1"/> 就好像批评家一样在对其表演进行评判，因此我们把这样的方法称作actor-critic方法。注意到前面的REINFORCE（with baseline）不是actor-critic方法，因为其中的价值函数网络并没有作为策略网络更新的目标，而仅仅是作为baseline出现的。只要价值函数网络以任何形式出现在目标中（即原来 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> 的位置上），都是actor-critic方法，下图列出了各种不同actor-critic方法用到的策略梯度。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-d7802e79b0c20b14c8cb7256758acbb7_b.jpg" data-size="normal" data-rawwidth="2134" data-rawheight="788" class="origin_image zh-lightbox-thumb" width="2134" data-original="https://pic4.zhimg.com/v2-d7802e79b0c20b14c8cb7256758acbb7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2134&#39; height=&#39;788&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="2134" data-rawheight="788" class="origin_image zh-lightbox-thumb lazy" width="2134" data-original="https://pic4.zhimg.com/v2-d7802e79b0c20b14c8cb7256758acbb7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-d7802e79b0c20b14c8cb7256758acbb7_b.jpg"/><figcaption>图：不同的Actor-critic算法</figcaption></figure><p><b>Asynchronous Advantage Actor-Critic (A3C)</b></p><p>我们下面再介绍一种在多核机器上表现更为优秀的算法，<b><i>Asynchronous Advantage Actor-Critic (A3C)</i></b>[7]。它是异步版本的A2C算法，其异步的方法是使用带不同探索参数的不同的CPU来采样，每个CPU上的采样积累到一定的数量之和再一起拿去更新网络参数。具体的示意图和算法如下图所示。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-4da5df42744f33697b9a72ae6ecae582_b.jpg" data-size="normal" data-rawwidth="1193" data-rawheight="504" class="origin_image zh-lightbox-thumb" width="1193" data-original="https://pic3.zhimg.com/v2-4da5df42744f33697b9a72ae6ecae582_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1193&#39; height=&#39;504&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1193" data-rawheight="504" class="origin_image zh-lightbox-thumb lazy" width="1193" data-original="https://pic3.zhimg.com/v2-4da5df42744f33697b9a72ae6ecae582_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4da5df42744f33697b9a72ae6ecae582_b.jpg"/><figcaption>图：A3C流程图</figcaption></figure><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-4aa880a98f0deb1495003b787191c316_b.jpg" data-size="normal" data-rawwidth="1434" data-rawheight="890" class="origin_image zh-lightbox-thumb" width="1434" data-original="https://pic3.zhimg.com/v2-4aa880a98f0deb1495003b787191c316_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1434&#39; height=&#39;890&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1434" data-rawheight="890" class="origin_image zh-lightbox-thumb lazy" width="1434" data-original="https://pic3.zhimg.com/v2-4aa880a98f0deb1495003b787191c316_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4aa880a98f0deb1495003b787191c316_b.jpg"/><figcaption>图：A3C算法</figcaption></figure><p>这样异步算法有如下一些好处：</p><ol><li>强化学习算法不稳定很大程度上是由于强化学习得到一连串的数据之间具有相关性，只有打破只有的相关性才能得到一个稳定的算法。前面了解到的DQN就是使用了经验池的方法来打破数据时间的相关性，通过从经验池里面采样，使得相关性较低的样本能够在同一个batch里面用来更新价值函数。这里使用了异步的方法来打破数据之间的相关性，多个actor探索到的经历不同，这样actor之间的样本相关性就降低了，从而稳定了算法。而且经验池的使用要求使用off-policy的方法，而采取异步的方法则可以使用更稳定的on-policy方法。</li><li>在不同的actor上能够分别采用不同的探索参数，从而增加全局探索的多样性；同时，实际上不同的actor也能够去探索环境的不同部分。</li></ol><p><b>Natural Actor-Critic</b></p><p>再来介绍一下Natural Actor-Critic[6]。做梯度上升的时候一般希望每次都在当前位置附近朝着梯度下降的方向前进一小步，在策略梯度方法里面即希望每次更新之后策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%2C%5Ctheta%29" alt="\pi(s,a,\theta)" eeimg="1"/> 的变化都不要太大。由于使用了非线性的神经网络来拟合这个策略，因此在参数空间中的一小步 <img src="https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta" alt="\Delta \theta" eeimg="1"/> 并不能保证它在策略空间也是一小步；并且可能在参数空间的不同方向的长度相同的两步，在策略空间可能一个只会引起很小的变化，另一个却会引起很大的变化。因此，我们希望在策略空间的邻域中找到梯度上升最大的一个方向进行更新，而不是在参数空间中做梯度上升。注意到策略空间是一个概率分布，因此使用KL-divergence来衡量策略空间的距离；又由于KL-divergence不满足距离的对称性要求，因此使用KL-divergence的二阶近似来作为距离</p><p><img src="https://www.zhihu.com/equation?tex=KL%28%5Cpi%28%5Ctheta%29+%7C%7C+%5Cpi%28%5Ctheta%2B%5Cdelta+%5Ctheta%29%29+%5Capprox+%5Cdfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+G_%7Bij%7D%28%5Ctheta%29+%5Cdelta+%5Ctheta_i+%5Cdelta+%5Ctheta_j" alt="KL(\pi(\theta) || \pi(\theta+\delta \theta)) \approx \dfrac{1}{2} \sum_{i,j} G_{ij}(\theta) \delta \theta_i \delta \theta_j" eeimg="1"/> </p><p>其中</p><p><img src="https://www.zhihu.com/equation?tex=G_%7Bij%7D%28%5Ctheta%29+%3D+%5Cint_s+%5Cint_a+%5Cdfrac%7B%5Cpartial+%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_i%7D+%5Cdfrac%7B%5Cpartial+%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_j%7D+%5Cpi%28s%2C+a%2C+%5Ctheta%29+ds+da" alt="G_{ij}(\theta) = \int_s \int_a \dfrac{\partial \pi(s, a, \theta)}{\partial \theta_i} \dfrac{\partial \pi(s, a, \theta)}{\partial \theta_j} \pi(s, a, \theta) ds da" eeimg="1"/> </p><p>是参控策略的Fisher Information Matrix。约束策略变化在一个小的邻域内，就可以得到参数应该更新的方向，得到参数的更新公式</p><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+G%28%5Ctheta%29%5E%7B-1%7D+%5Cnabla+J%28%5Ctheta%29" alt="\theta \leftarrow \theta + G(\theta)^{-1} \nabla J(\theta)" eeimg="1"/> </p><p><b>Compatible Actor-Critic</b></p><p>从策略梯度定理可以得到 <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho_%5Cpi%2C+a%5Csim+%5Cpi%7D%5B%5Cnabla+%5Clog+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29%5D" alt="\nabla_\theta J(\theta) = \mathbb{E}_{s\sim \rho_\pi, a\sim \pi}[\nabla \log \pi(s, a, \theta) Q_\pi (s, a)]" eeimg="1"/> 。如果使用蒙特卡洛采样 <img src="https://www.zhihu.com/equation?tex=G" alt="G" eeimg="1"/> 来估计 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi+%28s%2C+a%29" alt="Q_\pi (s, a)" eeimg="1"/> 那么可以保证得到的梯度是无偏估计；但是如果使用函数逼近产生的 <img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D_%5Cpi+%28s%2C+a%2C+w%29" alt="\hat{Q}_\pi (s, a, w)" eeimg="1"/> 来估计，是否还是无偏的呢？目前为止只找到了一种线性近似的方式能够保证这样的估计仍然是无偏的，这样的线性近似的方式我们称之为<b><i>compatible function approximator</i></b>[10]，它需要满足以下条件</p><ol><li><img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D_%5Cpi+%28s%2C+a%2C+w%29+%3D+%5Cnabla_%5Ctheta+%5Clog+%5Cpi+%28s%2C+a%2C+%5Ctheta%29%5ET+w" alt="\hat{Q}_\pi (s, a, w) = \nabla_\theta \log \pi (s, a, \theta)^T w" eeimg="1"/> </li><li><img src="https://www.zhihu.com/equation?tex=w+%3D+arg%5Cmin+%5Cepsilon%5E2+%28w%29+%3D+arg%5Cmin+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho_%5Cpi%2C+a%5Csim+%5Cpi%7D+%5B%28%5Chat%7BQ%7D_%5Cpi+%28s%2C+a%2C+w%29+-+Q_%5Cpi+%28s%2C+a%29%29%5E2%5D" alt="w = arg\min \epsilon^2 (w) = arg\min \mathbb{E}_{s\sim \rho_\pi, a\sim \pi} [(\hat{Q}_\pi (s, a, w) - Q_\pi (s, a))^2]" eeimg="1"/> </li></ol><p>使用这样的Q函数作为目标来更新策略网络参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> ，再使用比如 <img src="https://www.zhihu.com/equation?tex=TD%280%29" alt="TD(0)" eeimg="1"/> 等方法来更新Q网络的参数 <img src="https://www.zhihu.com/equation?tex=w" alt="w" eeimg="1"/> ，这样得到的算法就称作<b><i>Compatible Actor-Critic</i></b>。</p><p><b>应用到高维行动空间 —— DPG</b></p><p>对于行动空间离散的情况，策略网络 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%2C%5Ctheta%29" alt="\pi(s,a,\theta)" eeimg="1"/> 的输入可以是状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> ，对应的输出可以是每个行动对应的概率。但是对于高维度的行动空间或者连续的行动空间，输出端不可能为每一个行动都新增一个维度。这篇文章[8]就提出使用确定性的策略网络 <img src="https://www.zhihu.com/equation?tex=%5Cmu%3A+%5Cmathcal%7BS%7D+%5Cto+%5Cmathcal%7BA%7D" alt="\mu: \mathcal{S} \to \mathcal{A}" eeimg="1"/> ，即网络输入一个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> ，输入相应的一个行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> ，这样就能够处理高维度和连续的行动空间了。在策略网络是随机性的时候，我们推导得到了（随机性）策略梯度定理 <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho_%5Cpi%2C+a%5Csim+%5Cpi%7D%5B%5Cnabla+%5Clog+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29%5D" alt="\nabla_\theta J(\theta) = \mathbb{E}_{s\sim \rho_\pi, a\sim \pi}[\nabla \log \pi(s, a, \theta) Q_\pi (s, a)]" eeimg="1"/> ，当策略变成确定性了之后，是否也有对应的确定性策略梯度定理呢？直观地来想，如果有这样一个定理，那么期望上对于 <img src="https://www.zhihu.com/equation?tex=a%5Csim+%5Cpi" alt="a\sim \pi" eeimg="1"/> 的平均应该就能够被去掉，这样也简化了期望的计算。</p><p>下面就来介绍文章中推导得到的<b><i>确定性策略梯度定理</i></b>（Deterministic Policy Gradient Theorem）。和（随机性）策略梯度定理的推导类似，把第一个状态的价值函数梯度展开，并用后续状态的价值函数梯度表示，然后把关于相同状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 的项收集起来，这样就能够得到最后的策略梯度定理了。确定性策略梯度定理的表述如下</p><p><img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Csum_s+%5Crho%5E%7B%5Cmu%7D%28s%29+%5Cnabla_%5Ctheta+%5Cmu%28s%2C+%5Ctheta%29+%5Cnabla_a+Q_%5Cmu%28s%2C+a%29+%7C_%7Ba%3D%5Cmu%28s%2C%5Ctheta%29%7D+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho%5E%7B%5Cmu%7D%7D%5B%5Cnabla_%5Ctheta+%5Cmu%28s%2C+%5Ctheta%29+%5Cnabla_a+Q_%5Cmu%28s%2C+a%29+%7C_%7Ba%3D%5Cmu%28s%2C%5Ctheta%29%7D%5D" alt="\nabla_\theta J(\theta) = \sum_s \rho^{\mu}(s) \nabla_\theta \mu(s, \theta) \nabla_a Q_\mu(s, a) |_{a=\mu(s,\theta)} = \mathbb{E}_{s\sim \rho^{\mu}}[\nabla_\theta \mu(s, \theta) \nabla_a Q_\mu(s, a) |_{a=\mu(s,\theta)}]" eeimg="1"/> </p><p>如果策略是确定性的，再加上一般环境的噪声也不是十分巨大，那么确定性策略梯度方法必然会遇到一个问题，那就是策略的探索不足。这时我们再取出武器库中的off-policy。在off-policy下可能得到近似的策略梯度</p><p><img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho%5E%7B%5Cbeta%7D%7D%5B%5Cnabla_%5Ctheta+%5Cmu%28s%2C+%5Ctheta%29+%5Cnabla_a+Q_%5Cmu%28s%2C+a%29+%7C_%7Ba%3D%5Cmu%28s%2C%5Ctheta%29%7D%5D" alt="\nabla_\theta J(\theta) = \mathbb{E}_{s\sim \rho^{\beta}}[\nabla_\theta \mu(s, \theta) \nabla_a Q_\mu(s, a) |_{a=\mu(s,\theta)}]" eeimg="1"/> </p><p>这样就得到了off-policy下的确定性actor-critic方法（off-policy deterministic actor-critic, OPDAC）</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+w_%7Bt%2B1%7D+%26+%5Cleftarrow+w_t+%2B+%5Calpha_w+%5Cleft%28+r_t+%2B+%5Cgamma+Q%28s_%7Bt%2B1%7D%2C+%5Cmu%28s_%7Bt%2B1%7D%2C+%5Ctheta_t%29%2C+w_t%29+-+Q%28s_t%2C+a_t%2C+w_t%29+%5Cright%29+%5Cnabla_w+Q%28s_t%2C+a_t%2C+w_t%29+%5C%5C+%5Ctheta_%7Bt%2B1%7D+%26+%5Cleftarrow+%5Ctheta_t+%2B+%5Calpha_%5Ctheta+%5Cnabla_%5Ctheta+%5Cmu%28s_t%2C+%5Ctheta_t%29+%5Cnabla_a+Q%28s_t%2C+%5Cmu%28s_t%29%2C+w_t%29+%5Cend%7Baligned%7D" alt="\begin{aligned} w_{t+1} &amp; \leftarrow w_t + \alpha_w \left( r_t + \gamma Q(s_{t+1}, \mu(s_{t+1}, \theta_t), w_t) - Q(s_t, a_t, w_t) \right) \nabla_w Q(s_t, a_t, w_t) \\ \theta_{t+1} &amp; \leftarrow \theta_t + \alpha_\theta \nabla_\theta \mu(s_t, \theta_t) \nabla_a Q(s_t, \mu(s_t), w_t) \end{aligned}" eeimg="1"/> </p><h2><b>策略梯度类也可以玩Atari —— 从DQN到DDPG</b></h2><p>Atari中有一部分游戏的状态空间是离散的，比如对应控制方向的上下左右或者游戏手柄中经常出现的“OX△□”等；但是也有一些有些的行动空间连续的，比如控制施加到小车上面的力。DQN玩的Atari游戏每次都从从18种可能的操作里面选取一种操作来进行，而这篇工作[9]玩的Atari游戏则是输出多维度的连续实数来操作。前面介绍的deterministic policy gradient正好适合这样连续操作的任务，而DQN相关工作引入的一些包含experience replay、target network在内的各种技术又能在更好的保证算法的稳定性。因此这篇工作希望把两者的优势相结合，在连续控制的Atari游戏上得到好的性能。</p><p>在我们已经了解了DQN和DPG之后，DDPG就很好理解了，其算法如下图所示。其主要在我们前面介绍的OPDAC上用到了experience replay，target network和batch norm等技术，它是off-policy的算法，行动策略是在目标策略技术上加入噪声的策略。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-cffc42e7070e50fb0a2631d34454f094_b.jpg" data-size="normal" data-rawwidth="1526" data-rawheight="869" class="origin_image zh-lightbox-thumb" width="1526" data-original="https://pic1.zhimg.com/v2-cffc42e7070e50fb0a2631d34454f094_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1526&#39; height=&#39;869&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1526" data-rawheight="869" class="origin_image zh-lightbox-thumb lazy" width="1526" data-original="https://pic1.zhimg.com/v2-cffc42e7070e50fb0a2631d34454f094_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-cffc42e7070e50fb0a2631d34454f094_b.jpg"/><figcaption>图：DDPG算法</figcaption></figure><h2><b>小结</b></h2><p>这一讲主要介绍了一些基本的策略梯度方法，它是不同于前面介绍的基于价值函数的方法的另一类方法。</p><p>我们先引出了策略梯度类方法的一个基石——策略梯度定理，它告诉了我们策略梯度的具体形式。接着我们先用采样的方法来估计策略梯度，得到了REINFORCE算法；然后我们使用了bootstrap这个武器，得到了actor-critic类方法，大大提高了计算的效率；接下来为了面对连续的状态空间，我们推导出了确定性的策略梯度方法，并且使用off-policy这个武器解决了确定性策略策略梯度方法探索不足的问题；最后我们介绍了一个在Atari上的一个应用DDPG。</p><p>策略梯度类方法的相比于基于价值函数的方法有以下的优势：</p><ol><li>有一些任务的最优策略是一个随机性的策略，策略梯度方法直接参数化了策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%29" alt="\pi(s,a)" eeimg="1"/> ，因此可以表示出这样的随机性策略；而基于价值函数的方法通过价值函数来贪心地得到策略，无法表示出来这样的随机性策略。当然，有类似 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy的随机性策略，但是这样的随机性策略不能自然地过渡到确定性策略上；</li><li>基于价值函数的方法在每一个状态上如何选择出最好的行动呢？它们会比较该状态上每一个行动产生的下一个状态的期望 <img src="https://www.zhihu.com/equation?tex=V%28s%27%29" alt="V(s&#39;)" eeimg="1"/> 或者比较每个行动的 <img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"/> ，从而挑选出最好的行动。但是如果行动空间是连续的或者组合多的，在每个状态上的行动挑选就会变得几乎不可能。但是基于策略的方法就不会产生这样的问题，因为这类方法可以直接给出一个策略，策略接受状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 之后就会输出相应的行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> ；</li><li>有些时候直接参数化策略比参数化价值函数可能更简单；</li><li>直接参数化策略可以方便地把我们对于策略的偏好或者先验知识加进来；</li></ol><p>入门教程中提到的算法：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-a653a18d1b51cf77251c8dd370e32d36_b.jpg" data-caption="" data-size="normal" data-rawwidth="1954" data-rawheight="472" class="origin_image zh-lightbox-thumb" width="1954" data-original="https://pic3.zhimg.com/v2-a653a18d1b51cf77251c8dd370e32d36_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1954&#39; height=&#39;472&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1954" data-rawheight="472" class="origin_image zh-lightbox-thumb lazy" width="1954" data-original="https://pic3.zhimg.com/v2-a653a18d1b51cf77251c8dd370e32d36_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-a653a18d1b51cf77251c8dd370e32d36_b.jpg"/></figure><h2><b>参考文献</b></h2><p>[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 1998.</p><p>[2] Williams, Ronald J. &#34;Simple statistical gradient-following algorithms for connectionist reinforcement learning.&#34; Machine learning 8.3-4 (1992): 229-256.</p><p>REINFORCE文章</p><p>[3] Evan：理解强化学习知识之AC，A3C算法</p><p>知乎：AC与A3C的讲解</p><p>[4] Degris, Thomas, Patrick M. Pilarski, and Richard S. Sutton. &#34;Model-free reinforcement learning with continuous action in practice.&#34; American Control Conference (ACC), 2012. IEEE, 2012.</p><p>AC用于连续状态空间和Eligibility Trace</p><p>[5] 如何理解 natural gradient descent?</p><p>知乎：如何理解natural gradient descent</p><p>[6] Bhatnagar, Shalabh, et al. &#34;Natural actor–critic algorithms.&#34; Automatica 45.11 (2009): 2471-2482.</p><p>Natural AC，包含许多理论上的结论，比如不同AC之间的等价性、证明最优的AC baseline是state value function、boostrapped AC收敛性的证明等。</p><p>[7] Mnih, Volodymyr, et al. &#34;Asynchronous methods for deep reinforcement learning.&#34; International conference on machine learning. 2016.</p><p>A3C文章</p><p>[8] Silver, David, et al. &#34;Deterministic policy gradient algorithms.&#34; ICML. 2014.</p><p>DPG文章</p><p>[9] Lillicrap, Timothy P., et al. &#34;Continuous control with deep reinforcement learning.&#34; arXiv preprint arXiv:1509.02971 (2015).</p><p>DDPG文章</p><p>[10] Sutton, Richard S., et al. &#34;Policy gradient methods for reinforcement learning with function approximation.&#34; Advances in neural information processing systems. 2000.</p><p>讲到了compatible function approximator</p><p>[11] <a href="https://link.zhihu.com/?target=http%3A//sealzhang.tk/assets/files/2018-05-25-DDPG.pdf" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">sealzhang.tk/assets/fil</span><span class="invisible">es/2018-05-25-DDPG.pdf</span><span class="ellipsis"></span></a></p><p>本人组会上关于DPG和DDPG的slides</p><hr/><p>阅读完这三篇强化学习的基础知识讲解，希望大家对强化学习有一个基本的了解，便于大家进一步跟进本专栏中的所介绍的其他前沿进展。</p><p></p></div></div><div class="ContentItem-time">发布于 2019-02-01</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 31 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 31</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="c37a6cd6-d4fd-4639-9a66-d539f5570861" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="c37a6cd6-d4fd-4639-9a66-d539f5570861">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"56128287":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":56128287,"title":"【强化学习入门 3】强化学习策略梯度类方法","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56128287","imageUrl":"","titleImage":"","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1eaa6eeac71056250f489a0279d07c30_200x112.png\" data-caption=\"图：不同的Actor-critic算法\" data-size=\"normal\" data-rawwidth=\"2134\" data-rawheight=\"788\" data-watermark=\"watermark\" data-original-src=\"v2-1eaa6eeac71056250f489a0279d07c30\" data-watermark-src=\"v2-d7802e79b0c20b14c8cb7256758acbb7\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1eaa6eeac71056250f489a0279d07c30_r.png\"\u002F\u003E\u003Cb\u003E目录\u003C\u002Fb\u003E让策略梯度形式简单 —— Policy Gradient Theorem用最简单的采样方法来实现 —— REINFORCE两个网络一台戏 —— Actor-Critic应用到高维行动空间 —— DPG策略梯度类也可以玩Atari —— 从DQN到DDPG\u003Cb\u003E引言\u003C\u002Fb\u003E强化学习领域最为核心的问题是\u003Cb\u003E\u003Ci\u003E控制问题\u003C\u002Fi\u003E\u003C\u002Fb\u003E（control…","created":1549003224,"updated":1549003224,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":0,"imageHeight":0,"content":"\u003Ch2\u003E\u003Cb\u003E目录\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli\u003E让策略梯度形式简单 —— Policy Gradient Theorem\u003C\u002Fli\u003E\u003Cli\u003E用最简单的采样方法来实现 —— REINFORCE\u003C\u002Fli\u003E\u003Cli\u003E两个网络一台戏 —— Actor-Critic\u003C\u002Fli\u003E\u003Cli\u003E应用到高维行动空间 —— DPG\u003C\u002Fli\u003E\u003Cli\u003E策略梯度类也可以玩Atari —— 从DQN到DDPG\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E引言\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E强化学习领域最为核心的问题是\u003Cb\u003E\u003Ci\u003E控制问题\u003C\u002Fi\u003E\u003C\u002Fb\u003E（control problem），即找到一个好的策略，使得个体按照这个策略进行行动能够在期望上取得最大的收益。而强化学习基础的数学模型就是MDP，当这个MDP足够简单的时候，我们就可以使用\u003Cb\u003E\u003Ci\u003E基于解析的方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E找到最优策略，比如Linear Programming类的方法和前面介绍到的Dynamic Programming类的算法。不过在现实应用中遇到的强化学习问题常常都比较复杂，因此，我们需要采用\u003Cb\u003E\u003Ci\u003E基于采样的方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E。这一讲和前一讲都是属于这一类。\u003C\u002Fp\u003E\u003Cp\u003E我们上一讲就是介绍的这一类基于状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 或者行动价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E 的控制问题解法，这里称之为策略迭代类算法，或者叫做\u003Cb\u003E\u003Ci\u003E基于价值函数的方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（value-based method）。价值函数衡量了在某个状态或者采取某个行动之后能够取得的收益。在确定性的环境中，当我们知道了每个状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 的数值之后，策略就可以选择使得下一个状态期望状态价值函数最大的行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 。在随机的环境中，状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 就不太好用了，因为它不能直接指导我们进行行动的选择。在控制问题中更常用的是行动价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E ，它描述了在某个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下采取不同的行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 产生的期望收益，这样我们的最优策略就可以写成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28a%5C%7Cs%29+%3D+arg%5Cmax_%7Ba%27%7D+Q%28s%2Ca%27%29\" alt=\"\\pi(a\\|s) = arg\\max_{a&#39;} Q(s,a&#39;)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E在这一讲中用另外一种更为直接方式来解决强化学习中的控制问题——策略梯度类方法，它属于另外一类，即\u003Cb\u003E\u003Ci\u003E基于策略的方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy-based method）。基于价值函数的方法是先估计得到价值函数，然后直接依赖价值函数找到对应的最优策略。而基于策略的方法更为直接，直接参数化策略，然后对着一个性能评价函数去优化控制策略的参数，如果使用梯度上升方法来进行优化，那么得到的就是我们这一讲里面要了解到的\u003Cb\u003E\u003Ci\u003E策略梯度类方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy gradient method）；如果使用一些无导数优化方法来优化这些控制策略的参数，那么得到的就是我们下一讲里面会讲到的\u003Cb\u003E\u003Ci\u003E无导数优化方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（derivative-free method）。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E让策略梯度形式简单 —— Policy Gradient Theorem\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E前面已经说到基于策略的方法就是直接把策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%29\" alt=\"\\pi(s,a)\" eeimg=\"1\"\u002F\u003E 进行参数化表述，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%7C%5Ctheta%29\" alt=\"\\pi(s,a|\\theta)\" eeimg=\"1\"\u002F\u003E ，为了保证参数化的结果能够满足概率的归一化条件，一般令 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%7C%5Ctheta%29+%3D+%5Cdfrac%7B%5Cexp%28h%28s%2Ca%29%29%7D%7B%5Csum_%7Ba%27%7D+%5Cexp%28h%28s%2Ca%27%29%29%7D\" alt=\"\\pi(s,a|\\theta) = \\dfrac{\\exp(h(s,a))}{\\sum_{a&#39;} \\exp(h(s,a&#39;))}\" eeimg=\"1\"\u002F\u003E 。如果是线性参数化那么 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%28s%2Ca%7C%5Ctheta%29+%3D+%5Cphi%28s%2Ca%29%5ET+%5Ctheta\" alt=\"h(s,a|\\theta) = \\phi(s,a)^T \\theta\" eeimg=\"1\"\u002F\u003E ；如果使用神经网络，为了保证概率的归一化条件，网络 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%7C%5Ctheta%29\" alt=\"\\pi(s,a|\\theta)\" eeimg=\"1\"\u002F\u003E 的最后一层需要设定为softmax层。\u003C\u002Fp\u003E\u003Cp\u003E接下来定义策略性能的度量。\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E对于回合制任务，可以定义性能的度量为一回合中能够取得的总收益， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+R_k%5D\" alt=\"J(\\theta) = \\mathbb{E}[\\sum_{k=1}^T R_k]\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fli\u003E\u003Cli\u003E对于回合制任务，也可以在回合制任务里面引入衰减率，来更多地强调近期的收益，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+%5Cgamma%5E%7Bk-1%7D+R_k%5D\" alt=\"J(\\theta) = \\mathbb{E}[\\sum_{k=1}^T \\gamma^{k-1} R_k]\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fli\u003E\u003Cli\u003E对于连续任务，由于任务不会终结，那么总收益常常会发散，这时就必须加入衰减率，来保证其收敛，这样可以得到相应的性能度量， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5E%5Cinfty+%5Cgamma%5E%7Bk-1%7D+R_k%5D\" alt=\"J(\\theta) = \\mathbb{E}[\\sum_{k=1}^\\infty \\gamma^{k-1} R_k]\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fli\u003E\u003Cli\u003E对于连续任务，如果希望长期收益能和近期收益一样重要，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma%3D1\" alt=\"\\gamma=1\" eeimg=\"1\"\u002F\u003E ，这样如果直接使用上面的定义会导致 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 发散；这时可以使用平均每步收益来作为性能的度量，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29+%3D+%5Clim_%7Bt%5Cto+%5Cinfty%7D+%5Cdfrac%7B1%7D%7Bt%7D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+R_k%5D+%3D+%5Clim_%7Bt%5Cto+%5Cinfty%7D+%5Cmathbb%7BE%7D%5BR_k%5D\" alt=\"J(\\theta) = \\lim_{t\\to \\infty} \\dfrac{1}{t} \\mathbb{E}[\\sum_{k=1}^T R_k] = \\lim_{t\\to \\infty} \\mathbb{E}[R_k]\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E做好了策略的参数化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2C+a%7C+%5Ctheta%29\" alt=\"\\pi(s, a| \\theta)\" eeimg=\"1\"\u002F\u003E 并且定义好了策略性能的度量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 之后，我们就自然想到使用梯度上升方法来进行参数的更新， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+%5Calpha+%5Cnabla_%5Ctheta+J%28%5Ctheta%29\" alt=\"\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E那么下面的问题就转化为了如何计算 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29\" alt=\"\\nabla_\\theta J(\\theta)\" eeimg=\"1\"\u002F\u003E 。为了简洁起见，从现在开始使用第一种定义 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bk%3D1%7D%5ET+R_k%5D\" alt=\"J(\\theta) = \\mathbb{E}[\\sum_{k=1}^T R_k]\" eeimg=\"1\"\u002F\u003E ，之后我们再说明其他定义对应的情况。为了去对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 求 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u002F\u003E 的梯度，我们至少要把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 写成含有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2C+a%7C+%5Ctheta%29\" alt=\"\\pi(s, a| \\theta)\" eeimg=\"1\"\u002F\u003E 的形式吧。下面我们就开始做这件事情。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29+%3D+%5Csum_s+%5Crho_%5Cpi%28s%29+%5Csum_a+%5Cpi%28s%2C+a+%7C+%5Ctheta%29+Q_%5Cpi%28s%2C+a%29\" alt=\"J(\\theta) = \\sum_s \\rho_\\pi(s) \\sum_a \\pi(s, a | \\theta) Q_\\pi(s, a)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_%5Cpi%28s%29+%3D+%5Csum_%7Bs%27%7D+%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D+p_0%28s%27%29+p%28s%27+%5Cto+s%2C+t%2C+%5Cpi%29\" alt=\"\\rho_\\pi(s) = \\sum_{s&#39;} \\sum_{t=0}^{T-1} p_0(s&#39;) p(s&#39; \\to s, t, \\pi)\" eeimg=\"1\"\u002F\u003E 是策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 下的状态密度，它表示在一个回合中个体处于状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 的期望次数。之前的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 是一个时序上连续的收益和的形式，现在我们把它按照状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 和行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 拆开分别统计，并且对状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 和行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 分别加上了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_%5Cpi%28s%29\" alt=\"\\rho_\\pi(s)\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2C+a+%7C+%5Ctheta%29\" alt=\"\\pi(s, a | \\theta)\" eeimg=\"1\"\u002F\u003E 的权重。\u003C\u002Fp\u003E\u003Cp\u003E现在就产生了一个担忧。当策略进行更新的时候，可能因为选择了更好的行动而使得 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2C+a+%7C+%5Ctheta%29+q_%5Cpi%28s%2C+a%29\" alt=\"\\pi(s, a | \\theta) q_\\pi(s, a)\" eeimg=\"1\"\u002F\u003E 变得更大，即在这个状态上能够获得更大的期望收益；但与此同时，策略的改变也会影响策略下的状态密度 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_%5Cpi%28s%29\" alt=\"\\rho_\\pi(s)\" eeimg=\"1\"\u002F\u003E ，这时策略的总体性能是否能够保证变得更好就需要存疑了。换句话说，如果解析地对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 求梯度，我们希望表达式里面只含有对于策略的梯度 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+%5Cpi%28s%2C+a%7C%5Ctheta%29\" alt=\"\\nabla_\\theta \\pi(s, a|\\theta)\" eeimg=\"1\"\u002F\u003E ，因为不管是线性的参数化还是神经网络，梯度计算都非常的方便；如果得到的表达式里面如果含有状态密度 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_%5Cpi%28s%29\" alt=\"\\rho_\\pi(s)\" eeimg=\"1\"\u002F\u003E 相对于参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u002F\u003E 的导数就非常的不好办了，这样基本上没法计算。\u003C\u002Fp\u003E\u003Cp\u003E下面要引出的\u003Cb\u003E\u003Ci\u003E策略梯度定理\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy gradient theorem）就给咱们带来了一个好消息，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 的梯度不含有状态密度相对参数的梯度。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cnabla+J%28%5Ctheta%29+%3D+%5Csum_s+%5Crho_%5Cpi%28s%29+%5Csum_a+%5Cnabla+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29\" alt=\" \\nabla J(\\theta) = \\sum_s \\rho_\\pi(s) \\sum_a \\nabla \\pi(s, a, \\theta) Q_\\pi (s, a)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E形式上来看就是直接把求导放到了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2C+a%2C+%5Ctheta%29\" alt=\"\\pi(s, a, \\theta)\" eeimg=\"1\"\u002F\u003E ，但是推导过程并不是这么得无聊，具体推导见[1]。\u003C\u002Fp\u003E\u003Cp\u003E对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"\u002F\u003E 的第二种和第三种定义来说，与此的主要区别就是加入了衰减率，衰减率可以整理写到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_%5Cpi%28s%29\" alt=\"\\rho_\\pi(s)\" eeimg=\"1\"\u002F\u003E 中，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_%5Cpi%28s%29+%3D+%5Csum_%7Bs%27%7D+%5Csum_%7Bt%3D0%7D%5E%7B%28%5Ccdot%29%7D+%5Cgamma%5Et+p_0%28s%27%29+p%28s%27+%5Cto+s%2C+t%2C+%5Cpi%29\" alt=\"\\rho_\\pi(s) = \\sum_{s&#39;} \\sum_{t=0}^{(\\cdot)} \\gamma^t p_0(s&#39;) p(s&#39; \\to s, t, \\pi)\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Ccdot%29\" alt=\"(\\cdot)\" eeimg=\"1\"\u002F\u003E 可以分别写出回合制和连续任务的形式。其对应的策略梯度定理和上面是完全一样的形式。值得注意的是，如果初始的概率分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p_0%28s%29\" alt=\"p_0(s)\" eeimg=\"1\"\u002F\u003E 和策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 下的稳定分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_%5Cpi%28s%29\" alt=\"\\mu_\\pi(s)\" eeimg=\"1\"\u002F\u003E 一致，那么不管是否有加衰减率，都有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_%5Cpi%28s%29+%5Cpropto+%5Cmu_%5Cpi%28s%29\" alt=\"\\rho_\\pi(s) \\propto \\mu_\\pi(s)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E对于第四种定义来说，也可以写出同样形式的策略梯度定理，只不过此时 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2Ca%29+%3D+%5Cmathbb%7BE%7D%5BG_t%7CS_t+%3D+s%2C+A_t+%3D+a%2C+%5Cpi%5D\" alt=\"Q_\\pi(s,a) = \\mathbb{E}[G_t|S_t = s, A_t = a, \\pi]\" eeimg=\"1\"\u002F\u003E 的定义稍有不同，其中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 不再是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t\" alt=\"t\" eeimg=\"1\"\u002F\u003E 时刻之后的总收益，而是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t\" alt=\"t\" eeimg=\"1\"\u002F\u003E 时刻之后的\u003Cb\u003E\u003Ci\u003E差分收益\u003C\u002Fi\u003E\u003C\u002Fb\u003E（differential return）， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+R_%7Bt%2B1%7D+-+J%28%5Ctheta%29+%2B+R_%7Bt%2B2%7D+-+J%28%5Ctheta%29+%2B+%5Ccdots\" alt=\"G_t = R_{t+1} - J(\\theta) + R_{t+2} - J(\\theta) + \\cdots\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E用最简单的采样方法来实现 —— REINFORCE\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E通过策略梯度定理得到了策略梯度的表示形式，注意到表达式中涉及到对于所有状态和所有行动的求和，这样的求和的计算量是很大的，如果我们能够把它进一步化成某个策略下的期望的形式，就可以通过采样的方式来对梯度进行估计了。根据前面提到的性质，假设初始状态分布和策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 下稳态分布一致，那么 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_%5Cpi%28s%29+%3D+%5Crho_%5Cpi%28s%29\" alt=\"\\mu_\\pi(s) = \\rho_\\pi(s)\" eeimg=\"1\"\u002F\u003E。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla+J%28%5Ctheta%29+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+%5Csum_a++%5Cpi%28s%2C+a%2C+%5Ctheta%29+%5Cdfrac%7B%5Cnabla+%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D%7B%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D+Q_%5Cpi+%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D_%5Cpi%5B%5Cnabla+%5Clog+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29%5D\" alt=\"\\nabla J(\\theta) = \\sum_s \\mu_\\pi(s) \\sum_a  \\pi(s, a, \\theta) \\dfrac{\\nabla \\pi(s, a, \\theta)}{\\pi(s, a, \\theta)} Q_\\pi (s, a) = \\mathbb{E}_\\pi[\\nabla \\log \\pi(s, a, \\theta) Q_\\pi (s, a)]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E期望内的表达式就是对于策略梯度的一个估计，现在还需要计算 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2Ca%29\" alt=\"Q_\\pi(s,a)\" eeimg=\"1\"\u002F\u003E ，可以使用蒙特卡洛方法采样得到从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28s%2Ca%29\" alt=\"(s,a)\" eeimg=\"1\"\u002F\u003E 开始的收益 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E ，它是一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2Ca%29\" alt=\"Q_\\pi(s,a)\" eeimg=\"1\"\u002F\u003E 的无偏估计，由此得到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%5Cnabla+%5Clog+%5Cpi%28s%2Ca%2C%5Ctheta%29\" alt=\"G_t \\nabla \\log \\pi(s,a,\\theta)\" eeimg=\"1\"\u002F\u003E 也是策略梯度的一个无偏估计。由此得到REINFORCE算法更新公式\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+%5Calpha+G_t+%5Cnabla+%5Clog+%5Cpi%28S_t%2C+A_t%2C+%5Ctheta%29\" alt=\"\\theta \\leftarrow \\theta + \\alpha G_t \\nabla \\log \\pi(S_t, A_t, \\theta)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E注意到只有当一个回合结束之后我们才能得到该轨迹上各个状态对应的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E ，因此，该方法需要先采样一个完整的轨迹，然后再倒回来对每个状态更新参数。\u003C\u002Fp\u003E\u003Cp\u003E该方法是无偏的，但是方差非常大，所以运行速度也很慢，在原文[2]中就提出了使用baseline来减少方差，这主要基于一个观察，即在行动价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2Ca%29\" alt=\"Q_\\pi(s,a)\" eeimg=\"1\"\u002F\u003E 上减去任意一个只与状态有关的函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b%28s%29\" alt=\"b(s)\" eeimg=\"1\"\u002F\u003E 不影响策略梯度估计的期望，但是能选择一个这样的函数使得其方差最小。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%5Csum_s+%5Cmu_%5Cpi%28s%29+%5Csum_a+%5Cnabla+%5Cpi%28s%2C+a%2C+%5Ctheta%29+b+%28s%29+%26+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+b%28s%29+%5Csum_a+%5Cnabla_%5Ctheta+%5Cpi%28s%2C+a%2C+%5Ctheta%29+%5C%5C+%26+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+b%28s%29+%5Cnabla_%5Ctheta+%5Csum_a+%5Cpi%28s%2C+a%2C+%5Ctheta%29+%5C%5C+%26+%3D+%5Csum_s+%5Cmu_%5Cpi%28s%29+b%28s%29+%5Cnabla_%5Ctheta+1+%3D+0+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} \\sum_s \\mu_\\pi(s) \\sum_a \\nabla \\pi(s, a, \\theta) b (s) &amp; = \\sum_s \\mu_\\pi(s) b(s) \\sum_a \\nabla_\\theta \\pi(s, a, \\theta) \\\\ &amp; = \\sum_s \\mu_\\pi(s) b(s) \\nabla_\\theta \\sum_a \\pi(s, a, \\theta) \\\\ &amp; = \\sum_s \\mu_\\pi(s) b(s) \\nabla_\\theta 1 = 0 \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E那么怎样选择baseline呢？在[6]里面证明了（在线性近似下）选择状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 上的状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 作为baseline能够最大程度减小方差。这样，算法还需要同时维护一个参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w\" alt=\"w\" eeimg=\"1\"\u002F\u003E 控制的状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v%28s%2C+w%29\" alt=\"v(s, w)\" eeimg=\"1\"\u002F\u003E 用于baseline。该价值函数的估计使用蒙特卡洛方法，这样我们就得到REINFORCE with baseline的更新公式\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+w+%26+%5Cleftarrow+w+%2B+%5Calpha%5Ew+%28G_t+-+v%28S_t%2C+w%29%29+%5Cnabla_w+w%28S_t%2C+w%29+%5C%5C+%5Ctheta+%26+%5Cleftarrow+%5Ctheta+%2B+%5Calpha%5E%5Ctheta+%28G_t+-+v%28S_t%2C+w%29%29+%5Cnabla_%5Ctheta+%5Clog+%5Cpi%28S_t%2C+A_t%2C+%5Ctheta%29+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} w &amp; \\leftarrow w + \\alpha^w (G_t - v(S_t, w)) \\nabla_w w(S_t, w) \\\\ \\theta &amp; \\leftarrow \\theta + \\alpha^\\theta (G_t - v(S_t, w)) \\nabla_\\theta \\log \\pi(S_t, A_t, \\theta) \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E注意到两个公式里面都出现了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28G_t+-+v%28S_t%2C+w%29%29\" alt=\"(G_t - v(S_t, w))\" eeimg=\"1\"\u002F\u003E 一项，但是其含义并不完全相同。状态价值函数参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w\" alt=\"w\" eeimg=\"1\"\u002F\u003E 更新公式中的该项是对误差 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28G_t+-+v%28S_t%2C+w%29%29%5E2\" alt=\"(G_t - v(S_t, w))^2\" eeimg=\"1\"\u002F\u003E 求导得到的，函数逼近加上蒙特卡洛方法就会产生这一项；策略参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u002F\u003E 更新公式中出现的这一项是原本的更新目标 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 减去baseline  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v%28S_t%2C+w%29\" alt=\"v(S_t, w)\" eeimg=\"1\"\u002F\u003E 的结果。\u003C\u002Fp\u003E\u003Cp\u003E最后回答两个困扰过我的问题：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003E名字叫做policy-based method为什么还是涉及到价值函数的估计？\u003C\u002Fi\u003E\u003C\u002Fb\u003Epolicy-based和value-based的区别主要在于最后形成的策略是直接由参数控制的还是通过价值函数衍生出来的（比如相对于价值函数的greedy策略）。policy-based方法里面仍然可以用到价值函数的估计，只要最后的策略是直接由参数控制的，就不影响它称作policy-based method。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E\u003Ci\u003E为什么Sutton书中（section 13.4）的REINFORCE算法更新公式中梯度上会乘上\u003C\u002Fi\u003E\u003C\u002Fb\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma%5Et\" alt=\"\\gamma^t\" eeimg=\"1\"\u002F\u003E \u003Cb\u003E\u003Ci\u003E项？\u003C\u002Fi\u003E\u003C\u002Fb\u003E书里面为了推导方便，假设了每一个回合的初态都是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_0\" alt=\"s_0\" eeimg=\"1\"\u002F\u003E ，这就会导致远离 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_0\" alt=\"s_0\" eeimg=\"1\"\u002F\u003E 的态会因为衰减的缘故不那么重要，因此对于远离 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_0\" alt=\"s_0\" eeimg=\"1\"\u002F\u003E 的态需要减小其重要性。这里假设了初态分布和策略下的稳态分布相同，使用策略作用一步前后态的分布不变，这样每一个采样到的态不需要额外再加权重，就没有类似 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma%5Et\" alt=\"\\gamma^t\" eeimg=\"1\"\u002F\u003E 项。对于遍历的MDP来说，可以先按照策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 走足够多步，这样得到的分布可以认为差不多是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_%5Cpi%28s%29\" alt=\"\\mu_\\pi(s)\" eeimg=\"1\"\u002F\u003E ，然后再从此开始。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E\u003Cb\u003E两个网络一台戏 —— Actor-Critic\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Cb\u003EAdvantage Actor-Critic (A2C)\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EREINFORCE方法基于采样，它是无偏的，但是它最大的问题是方差大，由此收敛缓慢。要解决收敛慢的问题，就需要用到武器库中的bootstrapping。也就是说，采样得到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 使用相应的bootstrapped收益来使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29\" alt=\"R_{t+1} + \\gamma v(S_{t+1}, w)\" eeimg=\"1\"\u002F\u003E 代替。（注意，这里不加说明地加上了衰减率 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E ）这样得到的更新公式和REINFORCE with baseline形式上类似\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+w+%26+%5Cleftarrow+w+%2B+%5Calpha%5Ew+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29+%5Cnabla_w+w%28S_t%2C+w%29+%5C%5C+%5Ctheta+%26+%5Cleftarrow+%5Ctheta+%2B+%5Calpha%5E%5Ctheta+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29+%5Cnabla_%5Ctheta+%5Clog+%5Cpi%28S_t%2C+A_t%2C+%5Ctheta%29+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} w &amp; \\leftarrow w + \\alpha^w (R_{t+1} + \\gamma v(S_{t+1}, w) - v(S_t, w)) \\nabla_w w(S_t, w) \\\\ \\theta &amp; \\leftarrow \\theta + \\alpha^\\theta (R_{t+1} + \\gamma v(S_{t+1}, w) - v(S_t, w)) \\nabla_\\theta \\log \\pi(S_t, A_t, \\theta) \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E同样，要注意第一个式子中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29\" alt=\"(R_{t+1} + \\gamma v(S_{t+1}, w) - v(S_t, w))\" eeimg=\"1\"\u002F\u003E 是semi-gradient TD(0)方法中的TD error，后面一个式子中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28R_%7Bt%2B1%7D+%2B+%5Cgamma+v%28S_%7Bt%2B1%7D%2C+w%29+-+v%28S_t%2C+w%29%29\" alt=\"(R_{t+1} + \\gamma v(S_{t+1}, w) - v(S_t, w))\" eeimg=\"1\"\u002F\u003E 是bootstrapped target减去了baseline，形式上也是TD error。使用状态价值函数作为baseline其实就是使用了前面提到的Advantage \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%28s%2Ca%29+%3D+Q%28s%2Ca%29+-+V%28s%29\" alt=\"A(s,a) = Q(s,a) - V(s)\" eeimg=\"1\"\u002F\u003E 作为目标，这样的方法也叫做\u003Cb\u003E\u003Ci\u003EAdvantage Actor-Critic (A2C)\u003C\u002Fi\u003E\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E这里的策略网络 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%2C%5Ctheta%29\" alt=\"\\pi(s,a,\\theta)\" eeimg=\"1\"\u002F\u003E 就好像一样演员一样进行“表演”，而价值函数网络 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v%28s%2C+w%29\" alt=\"v(s, w)\" eeimg=\"1\"\u002F\u003E 就好像批评家一样在对其表演进行评判，因此我们把这样的方法称作actor-critic方法。注意到前面的REINFORCE（with baseline）不是actor-critic方法，因为其中的价值函数网络并没有作为策略网络更新的目标，而仅仅是作为baseline出现的。只要价值函数网络以任何形式出现在目标中（即原来 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 的位置上），都是actor-critic方法，下图列出了各种不同actor-critic方法用到的策略梯度。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d7802e79b0c20b14c8cb7256758acbb7_b.jpg\" data-size=\"normal\" data-rawwidth=\"2134\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb\" width=\"2134\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d7802e79b0c20b14c8cb7256758acbb7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2134&#39; height=&#39;788&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"2134\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2134\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d7802e79b0c20b14c8cb7256758acbb7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d7802e79b0c20b14c8cb7256758acbb7_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：不同的Actor-critic算法\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003EAsynchronous Advantage Actor-Critic (A3C)\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E我们下面再介绍一种在多核机器上表现更为优秀的算法，\u003Cb\u003E\u003Ci\u003EAsynchronous Advantage Actor-Critic (A3C)\u003C\u002Fi\u003E\u003C\u002Fb\u003E[7]。它是异步版本的A2C算法，其异步的方法是使用带不同探索参数的不同的CPU来采样，每个CPU上的采样积累到一定的数量之和再一起拿去更新网络参数。具体的示意图和算法如下图所示。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4da5df42744f33697b9a72ae6ecae582_b.jpg\" data-size=\"normal\" data-rawwidth=\"1193\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb\" width=\"1193\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4da5df42744f33697b9a72ae6ecae582_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1193&#39; height=&#39;504&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1193\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1193\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4da5df42744f33697b9a72ae6ecae582_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4da5df42744f33697b9a72ae6ecae582_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：A3C流程图\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4aa880a98f0deb1495003b787191c316_b.jpg\" data-size=\"normal\" data-rawwidth=\"1434\" data-rawheight=\"890\" class=\"origin_image zh-lightbox-thumb\" width=\"1434\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4aa880a98f0deb1495003b787191c316_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1434&#39; height=&#39;890&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1434\" data-rawheight=\"890\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1434\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4aa880a98f0deb1495003b787191c316_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4aa880a98f0deb1495003b787191c316_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：A3C算法\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这样异步算法有如下一些好处：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E强化学习算法不稳定很大程度上是由于强化学习得到一连串的数据之间具有相关性，只有打破只有的相关性才能得到一个稳定的算法。前面了解到的DQN就是使用了经验池的方法来打破数据时间的相关性，通过从经验池里面采样，使得相关性较低的样本能够在同一个batch里面用来更新价值函数。这里使用了异步的方法来打破数据之间的相关性，多个actor探索到的经历不同，这样actor之间的样本相关性就降低了，从而稳定了算法。而且经验池的使用要求使用off-policy的方法，而采取异步的方法则可以使用更稳定的on-policy方法。\u003C\u002Fli\u003E\u003Cli\u003E在不同的actor上能够分别采用不同的探索参数，从而增加全局探索的多样性；同时，实际上不同的actor也能够去探索环境的不同部分。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E\u003Cb\u003ENatural Actor-Critic\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E再来介绍一下Natural Actor-Critic[6]。做梯度上升的时候一般希望每次都在当前位置附近朝着梯度下降的方向前进一小步，在策略梯度方法里面即希望每次更新之后策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%2C%5Ctheta%29\" alt=\"\\pi(s,a,\\theta)\" eeimg=\"1\"\u002F\u003E 的变化都不要太大。由于使用了非线性的神经网络来拟合这个策略，因此在参数空间中的一小步 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+%5Ctheta\" alt=\"\\Delta \\theta\" eeimg=\"1\"\u002F\u003E 并不能保证它在策略空间也是一小步；并且可能在参数空间的不同方向的长度相同的两步，在策略空间可能一个只会引起很小的变化，另一个却会引起很大的变化。因此，我们希望在策略空间的邻域中找到梯度上升最大的一个方向进行更新，而不是在参数空间中做梯度上升。注意到策略空间是一个概率分布，因此使用KL-divergence来衡量策略空间的距离；又由于KL-divergence不满足距离的对称性要求，因此使用KL-divergence的二阶近似来作为距离\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=KL%28%5Cpi%28%5Ctheta%29+%7C%7C+%5Cpi%28%5Ctheta%2B%5Cdelta+%5Ctheta%29%29+%5Capprox+%5Cdfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%2Cj%7D+G_%7Bij%7D%28%5Ctheta%29+%5Cdelta+%5Ctheta_i+%5Cdelta+%5Ctheta_j\" alt=\"KL(\\pi(\\theta) || \\pi(\\theta+\\delta \\theta)) \\approx \\dfrac{1}{2} \\sum_{i,j} G_{ij}(\\theta) \\delta \\theta_i \\delta \\theta_j\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_%7Bij%7D%28%5Ctheta%29+%3D+%5Cint_s+%5Cint_a+%5Cdfrac%7B%5Cpartial+%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_i%7D+%5Cdfrac%7B%5Cpartial+%5Cpi%28s%2C+a%2C+%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_j%7D+%5Cpi%28s%2C+a%2C+%5Ctheta%29+ds+da\" alt=\"G_{ij}(\\theta) = \\int_s \\int_a \\dfrac{\\partial \\pi(s, a, \\theta)}{\\partial \\theta_i} \\dfrac{\\partial \\pi(s, a, \\theta)}{\\partial \\theta_j} \\pi(s, a, \\theta) ds da\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E是参控策略的Fisher Information Matrix。约束策略变化在一个小的邻域内，就可以得到参数应该更新的方向，得到参数的更新公式\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+G%28%5Ctheta%29%5E%7B-1%7D+%5Cnabla+J%28%5Ctheta%29\" alt=\"\\theta \\leftarrow \\theta + G(\\theta)^{-1} \\nabla J(\\theta)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003ECompatible Actor-Critic\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E从策略梯度定理可以得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho_%5Cpi%2C+a%5Csim+%5Cpi%7D%5B%5Cnabla+%5Clog+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29%5D\" alt=\"\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s\\sim \\rho_\\pi, a\\sim \\pi}[\\nabla \\log \\pi(s, a, \\theta) Q_\\pi (s, a)]\" eeimg=\"1\"\u002F\u003E 。如果使用蒙特卡洛采样 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G\" alt=\"G\" eeimg=\"1\"\u002F\u003E 来估计 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi+%28s%2C+a%29\" alt=\"Q_\\pi (s, a)\" eeimg=\"1\"\u002F\u003E 那么可以保证得到的梯度是无偏估计；但是如果使用函数逼近产生的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D_%5Cpi+%28s%2C+a%2C+w%29\" alt=\"\\hat{Q}_\\pi (s, a, w)\" eeimg=\"1\"\u002F\u003E 来估计，是否还是无偏的呢？目前为止只找到了一种线性近似的方式能够保证这样的估计仍然是无偏的，这样的线性近似的方式我们称之为\u003Cb\u003E\u003Ci\u003Ecompatible function approximator\u003C\u002Fi\u003E\u003C\u002Fb\u003E[10]，它需要满足以下条件\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D_%5Cpi+%28s%2C+a%2C+w%29+%3D+%5Cnabla_%5Ctheta+%5Clog+%5Cpi+%28s%2C+a%2C+%5Ctheta%29%5ET+w\" alt=\"\\hat{Q}_\\pi (s, a, w) = \\nabla_\\theta \\log \\pi (s, a, \\theta)^T w\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w+%3D+arg%5Cmin+%5Cepsilon%5E2+%28w%29+%3D+arg%5Cmin+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho_%5Cpi%2C+a%5Csim+%5Cpi%7D+%5B%28%5Chat%7BQ%7D_%5Cpi+%28s%2C+a%2C+w%29+-+Q_%5Cpi+%28s%2C+a%29%29%5E2%5D\" alt=\"w = arg\\min \\epsilon^2 (w) = arg\\min \\mathbb{E}_{s\\sim \\rho_\\pi, a\\sim \\pi} [(\\hat{Q}_\\pi (s, a, w) - Q_\\pi (s, a))^2]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E使用这样的Q函数作为目标来更新策略网络参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u002F\u003E ，再使用比如 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=TD%280%29\" alt=\"TD(0)\" eeimg=\"1\"\u002F\u003E 等方法来更新Q网络的参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w\" alt=\"w\" eeimg=\"1\"\u002F\u003E ，这样得到的算法就称作\u003Cb\u003E\u003Ci\u003ECompatible Actor-Critic\u003C\u002Fi\u003E\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E应用到高维行动空间 —— DPG\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E对于行动空间离散的情况，策略网络 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%2C%5Ctheta%29\" alt=\"\\pi(s,a,\\theta)\" eeimg=\"1\"\u002F\u003E 的输入可以是状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E ，对应的输出可以是每个行动对应的概率。但是对于高维度的行动空间或者连续的行动空间，输出端不可能为每一个行动都新增一个维度。这篇文章[8]就提出使用确定性的策略网络 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu%3A+%5Cmathcal%7BS%7D+%5Cto+%5Cmathcal%7BA%7D\" alt=\"\\mu: \\mathcal{S} \\to \\mathcal{A}\" eeimg=\"1\"\u002F\u003E ，即网络输入一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E ，输入相应的一个行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E ，这样就能够处理高维度和连续的行动空间了。在策略网络是随机性的时候，我们推导得到了（随机性）策略梯度定理 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho_%5Cpi%2C+a%5Csim+%5Cpi%7D%5B%5Cnabla+%5Clog+%5Cpi%28s%2C+a%2C+%5Ctheta%29+Q_%5Cpi+%28s%2C+a%29%5D\" alt=\"\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s\\sim \\rho_\\pi, a\\sim \\pi}[\\nabla \\log \\pi(s, a, \\theta) Q_\\pi (s, a)]\" eeimg=\"1\"\u002F\u003E ，当策略变成确定性了之后，是否也有对应的确定性策略梯度定理呢？直观地来想，如果有这样一个定理，那么期望上对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a%5Csim+%5Cpi\" alt=\"a\\sim \\pi\" eeimg=\"1\"\u002F\u003E 的平均应该就能够被去掉，这样也简化了期望的计算。\u003C\u002Fp\u003E\u003Cp\u003E下面就来介绍文章中推导得到的\u003Cb\u003E\u003Ci\u003E确定性策略梯度定理\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Deterministic Policy Gradient Theorem）。和（随机性）策略梯度定理的推导类似，把第一个状态的价值函数梯度展开，并用后续状态的价值函数梯度表示，然后把关于相同状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 的项收集起来，这样就能够得到最后的策略梯度定理了。确定性策略梯度定理的表述如下\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Csum_s+%5Crho%5E%7B%5Cmu%7D%28s%29+%5Cnabla_%5Ctheta+%5Cmu%28s%2C+%5Ctheta%29+%5Cnabla_a+Q_%5Cmu%28s%2C+a%29+%7C_%7Ba%3D%5Cmu%28s%2C%5Ctheta%29%7D+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho%5E%7B%5Cmu%7D%7D%5B%5Cnabla_%5Ctheta+%5Cmu%28s%2C+%5Ctheta%29+%5Cnabla_a+Q_%5Cmu%28s%2C+a%29+%7C_%7Ba%3D%5Cmu%28s%2C%5Ctheta%29%7D%5D\" alt=\"\\nabla_\\theta J(\\theta) = \\sum_s \\rho^{\\mu}(s) \\nabla_\\theta \\mu(s, \\theta) \\nabla_a Q_\\mu(s, a) |_{a=\\mu(s,\\theta)} = \\mathbb{E}_{s\\sim \\rho^{\\mu}}[\\nabla_\\theta \\mu(s, \\theta) \\nabla_a Q_\\mu(s, a) |_{a=\\mu(s,\\theta)}]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E如果策略是确定性的，再加上一般环境的噪声也不是十分巨大，那么确定性策略梯度方法必然会遇到一个问题，那就是策略的探索不足。这时我们再取出武器库中的off-policy。在off-policy下可能得到近似的策略梯度\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+J%28%5Ctheta%29+%3D+%5Cmathbb%7BE%7D_%7Bs%5Csim+%5Crho%5E%7B%5Cbeta%7D%7D%5B%5Cnabla_%5Ctheta+%5Cmu%28s%2C+%5Ctheta%29+%5Cnabla_a+Q_%5Cmu%28s%2C+a%29+%7C_%7Ba%3D%5Cmu%28s%2C%5Ctheta%29%7D%5D\" alt=\"\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s\\sim \\rho^{\\beta}}[\\nabla_\\theta \\mu(s, \\theta) \\nabla_a Q_\\mu(s, a) |_{a=\\mu(s,\\theta)}]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E这样就得到了off-policy下的确定性actor-critic方法（off-policy deterministic actor-critic, OPDAC）\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+w_%7Bt%2B1%7D+%26+%5Cleftarrow+w_t+%2B+%5Calpha_w+%5Cleft%28+r_t+%2B+%5Cgamma+Q%28s_%7Bt%2B1%7D%2C+%5Cmu%28s_%7Bt%2B1%7D%2C+%5Ctheta_t%29%2C+w_t%29+-+Q%28s_t%2C+a_t%2C+w_t%29+%5Cright%29+%5Cnabla_w+Q%28s_t%2C+a_t%2C+w_t%29+%5C%5C+%5Ctheta_%7Bt%2B1%7D+%26+%5Cleftarrow+%5Ctheta_t+%2B+%5Calpha_%5Ctheta+%5Cnabla_%5Ctheta+%5Cmu%28s_t%2C+%5Ctheta_t%29+%5Cnabla_a+Q%28s_t%2C+%5Cmu%28s_t%29%2C+w_t%29+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} w_{t+1} &amp; \\leftarrow w_t + \\alpha_w \\left( r_t + \\gamma Q(s_{t+1}, \\mu(s_{t+1}, \\theta_t), w_t) - Q(s_t, a_t, w_t) \\right) \\nabla_w Q(s_t, a_t, w_t) \\\\ \\theta_{t+1} &amp; \\leftarrow \\theta_t + \\alpha_\\theta \\nabla_\\theta \\mu(s_t, \\theta_t) \\nabla_a Q(s_t, \\mu(s_t), w_t) \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E策略梯度类也可以玩Atari —— 从DQN到DDPG\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003EAtari中有一部分游戏的状态空间是离散的，比如对应控制方向的上下左右或者游戏手柄中经常出现的“OX△□”等；但是也有一些有些的行动空间连续的，比如控制施加到小车上面的力。DQN玩的Atari游戏每次都从从18种可能的操作里面选取一种操作来进行，而这篇工作[9]玩的Atari游戏则是输出多维度的连续实数来操作。前面介绍的deterministic policy gradient正好适合这样连续操作的任务，而DQN相关工作引入的一些包含experience replay、target network在内的各种技术又能在更好的保证算法的稳定性。因此这篇工作希望把两者的优势相结合，在连续控制的Atari游戏上得到好的性能。\u003C\u002Fp\u003E\u003Cp\u003E在我们已经了解了DQN和DPG之后，DDPG就很好理解了，其算法如下图所示。其主要在我们前面介绍的OPDAC上用到了experience replay，target network和batch norm等技术，它是off-policy的算法，行动策略是在目标策略技术上加入噪声的策略。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cffc42e7070e50fb0a2631d34454f094_b.jpg\" data-size=\"normal\" data-rawwidth=\"1526\" data-rawheight=\"869\" class=\"origin_image zh-lightbox-thumb\" width=\"1526\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cffc42e7070e50fb0a2631d34454f094_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1526&#39; height=&#39;869&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1526\" data-rawheight=\"869\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1526\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cffc42e7070e50fb0a2631d34454f094_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-cffc42e7070e50fb0a2631d34454f094_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：DDPG算法\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E小结\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E这一讲主要介绍了一些基本的策略梯度方法，它是不同于前面介绍的基于价值函数的方法的另一类方法。\u003C\u002Fp\u003E\u003Cp\u003E我们先引出了策略梯度类方法的一个基石——策略梯度定理，它告诉了我们策略梯度的具体形式。接着我们先用采样的方法来估计策略梯度，得到了REINFORCE算法；然后我们使用了bootstrap这个武器，得到了actor-critic类方法，大大提高了计算的效率；接下来为了面对连续的状态空间，我们推导出了确定性的策略梯度方法，并且使用off-policy这个武器解决了确定性策略策略梯度方法探索不足的问题；最后我们介绍了一个在Atari上的一个应用DDPG。\u003C\u002Fp\u003E\u003Cp\u003E策略梯度类方法的相比于基于价值函数的方法有以下的优势：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E有一些任务的最优策略是一个随机性的策略，策略梯度方法直接参数化了策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s%2Ca%29\" alt=\"\\pi(s,a)\" eeimg=\"1\"\u002F\u003E ，因此可以表示出这样的随机性策略；而基于价值函数的方法通过价值函数来贪心地得到策略，无法表示出来这样的随机性策略。当然，有类似 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy的随机性策略，但是这样的随机性策略不能自然地过渡到确定性策略上；\u003C\u002Fli\u003E\u003Cli\u003E基于价值函数的方法在每一个状态上如何选择出最好的行动呢？它们会比较该状态上每一个行动产生的下一个状态的期望 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%27%29\" alt=\"V(s&#39;)\" eeimg=\"1\"\u002F\u003E 或者比较每个行动的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"\u002F\u003E ，从而挑选出最好的行动。但是如果行动空间是连续的或者组合多的，在每个状态上的行动挑选就会变得几乎不可能。但是基于策略的方法就不会产生这样的问题，因为这类方法可以直接给出一个策略，策略接受状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 之后就会输出相应的行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fli\u003E\u003Cli\u003E有些时候直接参数化策略比参数化价值函数可能更简单；\u003C\u002Fli\u003E\u003Cli\u003E直接参数化策略可以方便地把我们对于策略的偏好或者先验知识加进来；\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E入门教程中提到的算法：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a653a18d1b51cf77251c8dd370e32d36_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1954\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb\" width=\"1954\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a653a18d1b51cf77251c8dd370e32d36_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1954&#39; height=&#39;472&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1954\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1954\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a653a18d1b51cf77251c8dd370e32d36_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a653a18d1b51cf77251c8dd370e32d36_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E参考文献\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 1998.\u003C\u002Fp\u003E\u003Cp\u003E[2] Williams, Ronald J. &#34;Simple statistical gradient-following algorithms for connectionist reinforcement learning.&#34; Machine learning 8.3-4 (1992): 229-256.\u003C\u002Fp\u003E\u003Cp\u003EREINFORCE文章\u003C\u002Fp\u003E\u003Cp\u003E[3] Evan：理解强化学习知识之AC，A3C算法\u003C\u002Fp\u003E\u003Cp\u003E知乎：AC与A3C的讲解\u003C\u002Fp\u003E\u003Cp\u003E[4] Degris, Thomas, Patrick M. Pilarski, and Richard S. Sutton. &#34;Model-free reinforcement learning with continuous action in practice.&#34; American Control Conference (ACC), 2012. IEEE, 2012.\u003C\u002Fp\u003E\u003Cp\u003EAC用于连续状态空间和Eligibility Trace\u003C\u002Fp\u003E\u003Cp\u003E[5] 如何理解 natural gradient descent?\u003C\u002Fp\u003E\u003Cp\u003E知乎：如何理解natural gradient descent\u003C\u002Fp\u003E\u003Cp\u003E[6] Bhatnagar, Shalabh, et al. &#34;Natural actor–critic algorithms.&#34; Automatica 45.11 (2009): 2471-2482.\u003C\u002Fp\u003E\u003Cp\u003ENatural AC，包含许多理论上的结论，比如不同AC之间的等价性、证明最优的AC baseline是state value function、boostrapped AC收敛性的证明等。\u003C\u002Fp\u003E\u003Cp\u003E[7] Mnih, Volodymyr, et al. &#34;Asynchronous methods for deep reinforcement learning.&#34; International conference on machine learning. 2016.\u003C\u002Fp\u003E\u003Cp\u003EA3C文章\u003C\u002Fp\u003E\u003Cp\u003E[8] Silver, David, et al. &#34;Deterministic policy gradient algorithms.&#34; ICML. 2014.\u003C\u002Fp\u003E\u003Cp\u003EDPG文章\u003C\u002Fp\u003E\u003Cp\u003E[9] Lillicrap, Timothy P., et al. &#34;Continuous control with deep reinforcement learning.&#34; arXiv preprint arXiv:1509.02971 (2015).\u003C\u002Fp\u003E\u003Cp\u003EDDPG文章\u003C\u002Fp\u003E\u003Cp\u003E[10] Sutton, Richard S., et al. &#34;Policy gradient methods for reinforcement learning with function approximation.&#34; Advances in neural information processing systems. 2000.\u003C\u002Fp\u003E\u003Cp\u003E讲到了compatible function approximator\u003C\u002Fp\u003E\u003Cp\u003E[11] \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fsealzhang.tk\u002Fassets\u002Ffiles\u002F2018-05-25-DDPG.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Esealzhang.tk\u002Fassets\u002Ffil\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ees\u002F2018-05-25-DDPG.pdf\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E本人组会上关于DPG和DDPG的slides\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp\u003E阅读完这三篇强化学习的基础知识讲解，希望大家对强化学习有一个基本的了解，便于大家进一步跟进本专栏中的所介绍的其他前沿进展。\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":31,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":0,"contributions":[{"id":20213286,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习入门 3】强化学习策略梯度类方法 - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56128287 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":1,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-4","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"web_collect","type":"String","value":"0"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F56128287","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56128287","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>