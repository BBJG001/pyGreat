<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 47】Distributional RL - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning),金融"/><meta data-react-helmet="true" name="description" content="一篇很有启发性的工作，目前主流的强化学习方法主要关注价值函数的均值，这里提出把价值函数的分布也考虑进来。原文传送门Bellemare, Marc G., Will Dabney, and Rémi Munos. &amp;#34;A distributional perspective …"/><meta data-react-helmet="true" property="og:title" content="【强化学习 47】Distributional RL"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/60632660"/><meta data-react-helmet="true" property="og:description" content="一篇很有启发性的工作，目前主流的强化学习方法主要关注价值函数的均值，这里提出把价值函数的分布也考虑进来。原文传送门Bellemare, Marc G., Will Dabney, and Rémi Munos. &amp;#34;A distributional perspective …"/><meta data-react-helmet="true" property="og:image" content="https://pic1.zhimg.com/v2-1aa6bd6a904b9c5f502a37a443d7c851_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:60632660,&quot;title&quot;:&quot;【强化学习 47】Distributional RL&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic1.zhimg.com/v2-1aa6bd6a904b9c5f502a37a443d7c851_1200x500.jpg" alt="【强化学习 47】Distributional RL"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 47】Distributional RL</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">57 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>一篇很有启发性的工作，目前主流的强化学习方法主要关注价值函数的均值，这里提出把价值函数的分布也考虑进来。</p><h2><b>原文传送门</b></h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1707.06887" class=" wrap external" target="_blank" rel="nofollow noreferrer">Bellemare, Marc G., Will Dabney, and Rémi Munos. &#34;A distributional perspective on reinforcement learning.&#34; Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.</a></p><h2><b>特色</b></h2><p>这篇工作很有启发性，讲述了当价值函数不在仅仅是一个期望值而是是一个分布会怎样？相应的Bellman Operator是否还具有较好的性质？是否能够形成一个有效的算法？把分布考虑进来是否能够带来算法性能的提升？</p><h2><b>过程</b></h2><h3><b>1. 考虑价值函数的分布有什么好处？</b></h3><ul><li>分布相比于均值能够对决策提供更多的信息，比如对于某些risk aware的场景，我们可能会更倾向于选择方差较小或者最坏情况较好的行动，而不是一味地选择均值较高的行动；</li><li>对于某些具有简并状态（state aliasing）的MDP或者POMDP，表征上看起来一样的状态可能具有完全不一样的两个价值函数，如果仅仅考虑均值，这部分信息就会被完全混淆；</li><li>考虑价值函数的分布能够缓解奖励稀疏的问题，较为稀疏的奖励会在通常的迭代中慢慢“稀释”，如果像文中的做法，稀疏的奖励在传播过程中更容易被留存下来（看到具体的算法之后才容易体会到这一点）</li></ul><h3><b>2. 价值函数的分布表示和距离度量</b></h3><p>用分布来表示价值函数只需要把通常的价值函数 <img src="https://www.zhihu.com/equation?tex=V%28x%29" alt="V(x)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=Q%28x%2Ca%29" alt="Q(x,a)" eeimg="1"/> ，替换成相应的分布 <img src="https://www.zhihu.com/equation?tex=Z%28x%29" alt="Z(x)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=Z%28x%2Ca%29" alt="Z(x,a)" eeimg="1"/> 即可。</p><p>距离度量使用Wasserstein距离</p><p><img src="https://www.zhihu.com/equation?tex=d_p%28U%2C+V%29+%3A%3D+%5Cinf_%7BU%2CV%7D+%7C%7CU-V%7C%7C_p+%3A%3D+%5Cinf_%7BU%2CV%7D+%5B%5Cmathbb%7BE%7D_%7B%5Comega%5Cin+%5COmega%7D+%7C%7CU%28%5Comega%29-V%28%5Comega%29%7C%7C%5Ep%5D%5E%7B1%2Fp%7D" alt="d_p(U, V) := \inf_{U,V} ||U-V||_p := \inf_{U,V} [\mathbb{E}_{\omega\in \Omega} ||U(\omega)-V(\omega)||^p]^{1/p}" eeimg="1"/> </p><p>对于两个随机变量 <img src="https://www.zhihu.com/equation?tex=U%2CV" alt="U,V" eeimg="1"/> （由于本文讨论价值函数的分布，可以认为它们都是一维随机变量），第一个等式中的infimum是对于所有符合 <img src="https://www.zhihu.com/equation?tex=U%2CV" alt="U,V" eeimg="1"/> 各自边缘分布的 <img src="https://www.zhihu.com/equation?tex=%28U%2CV%29" alt="(U,V)" eeimg="1"/> 联合分布（参见本专栏另外的文章Wasserstein距离）。 <img src="https://www.zhihu.com/equation?tex=%5Comega+%5Cin+%5COmega" alt="\omega \in \Omega" eeimg="1"/> 表示对于所有可能的实验结果取期望。</p><p>该距离度量有以下性质</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-93cfff9ac6b81150513f9b64d89e9678_b.png" data-caption="" data-size="normal" data-rawwidth="981" data-rawheight="148" class="origin_image zh-lightbox-thumb" width="981" data-original="https://pic1.zhimg.com/v2-93cfff9ac6b81150513f9b64d89e9678_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;981&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="981" data-rawheight="148" class="origin_image zh-lightbox-thumb lazy" width="981" data-original="https://pic1.zhimg.com/v2-93cfff9ac6b81150513f9b64d89e9678_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-93cfff9ac6b81150513f9b64d89e9678_b.png"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-11faa0ee68bbd5dd29dd3691b0b5b5d8_b.jpg" data-caption="" data-size="normal" data-rawwidth="1036" data-rawheight="242" class="origin_image zh-lightbox-thumb" width="1036" data-original="https://pic1.zhimg.com/v2-11faa0ee68bbd5dd29dd3691b0b5b5d8_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1036&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1036" data-rawheight="242" class="origin_image zh-lightbox-thumb lazy" width="1036" data-original="https://pic1.zhimg.com/v2-11faa0ee68bbd5dd29dd3691b0b5b5d8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-11faa0ee68bbd5dd29dd3691b0b5b5d8_b.jpg"/></figure><p><i>（观察到，该引理左边可以在所有的联合分布中寻找最小值，而该定理的右边要求所寻找的联合分布协方差矩阵还需要按照A的划分是分块矩阵，这显然会使得找到的最小值更大，证明按照此思路易得）</i></p><p>注意到以上的定义都只针对两个表征价值的随机变量，当考虑价值函数的时候，还有 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"/> 或者 <img src="https://www.zhihu.com/equation?tex=%28x%2Ca%29" alt="(x,a)" eeimg="1"/> 的自变量输入，下面定义<i><b>两个分布价值函数之间的距离度量</b></i>。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-2a698d33cce4608b0c903e0b7ceded57_b.png" data-caption="" data-size="normal" data-rawwidth="982" data-rawheight="77" class="origin_image zh-lightbox-thumb" width="982" data-original="https://pic4.zhimg.com/v2-2a698d33cce4608b0c903e0b7ceded57_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;982&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="982" data-rawheight="77" class="origin_image zh-lightbox-thumb lazy" width="982" data-original="https://pic4.zhimg.com/v2-2a698d33cce4608b0c903e0b7ceded57_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-2a698d33cce4608b0c903e0b7ceded57_b.png"/></figure><p>可以证明它是一个距离度量，即满足三角不等式。</p><p><b>3. Policy Evaluation</b></p><p>有了分布价值函数的定义之后，我们考虑的第一个问题是对于一个给定的策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> ，是否存在一个类似Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D%5E%5Cpi" alt="\mathcal{T}^\pi" eeimg="1"/> ，使得对于任意的初始分布价值函数，都能够在上面所定义的分布距离度量下，收敛到其真实分布价值函数？即常说的policy evaluation或者prediction问题（Sutton书）。</p><p>定义Bellman算子</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-80a0fe81e2e81f780b64fb8ebf233c81_b.png" data-caption="" data-size="normal" data-rawwidth="1042" data-rawheight="66" class="origin_image zh-lightbox-thumb" width="1042" data-original="https://pic2.zhimg.com/v2-80a0fe81e2e81f780b64fb8ebf233c81_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1042&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1042" data-rawheight="66" class="origin_image zh-lightbox-thumb lazy" width="1042" data-original="https://pic2.zhimg.com/v2-80a0fe81e2e81f780b64fb8ebf233c81_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-80a0fe81e2e81f780b64fb8ebf233c81_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-a7548511984611b701f3302db3abde15_b.png" data-caption="" data-size="normal" data-rawwidth="997" data-rawheight="108" class="origin_image zh-lightbox-thumb" width="997" data-original="https://pic2.zhimg.com/v2-a7548511984611b701f3302db3abde15_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;997&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="997" data-rawheight="108" class="origin_image zh-lightbox-thumb lazy" width="997" data-original="https://pic2.zhimg.com/v2-a7548511984611b701f3302db3abde15_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-a7548511984611b701f3302db3abde15_b.png"/></figure><p>对于这样的算子有很好的结果，即对于任意的初始分布价值函数，都能够在Wasserstein距离度量下，收敛到其真实分布价值函数。要证明这一点只需要证明contraction。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-f37855aef159f62ad46cac171817dbfd_b.png" data-caption="" data-size="normal" data-rawwidth="1086" data-rawheight="62" class="origin_image zh-lightbox-thumb" width="1086" data-original="https://pic2.zhimg.com/v2-f37855aef159f62ad46cac171817dbfd_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1086&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1086" data-rawheight="62" class="origin_image zh-lightbox-thumb lazy" width="1086" data-original="https://pic2.zhimg.com/v2-f37855aef159f62ad46cac171817dbfd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-f37855aef159f62ad46cac171817dbfd_b.png"/></figure><p><i>（要证明这一点即证明 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7Bd%7D_p%28%5Cmathcal%7BT%7D%5E%5Cpi+Z_1%2C+%5Cmathcal%7BT%7D%5E%5Cpi+Z_2%29+%5Cle+%5Cgamma+%5Cbar%7Bd%7D_p%28+Z_1%2C++Z_2%29" alt="\bar{d}_p(\mathcal{T}^\pi Z_1, \mathcal{T}^\pi Z_2) \le \gamma \bar{d}_p( Z_1,  Z_2)" eeimg="1"/> ，把Bellman算子展开并且利用前面关于Wasserstein距离的性质就可以得到）</i></p><p>另外，Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D%5E%5Cpi" alt="\mathcal{T}^\pi" eeimg="1"/> 对于分布的前两阶中心矩也是contraction。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-e9f7584d33432ca504571373feaacd02_b.jpg" data-caption="" data-size="normal" data-rawwidth="1129" data-rawheight="249" class="origin_image zh-lightbox-thumb" width="1129" data-original="https://pic3.zhimg.com/v2-e9f7584d33432ca504571373feaacd02_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1129&#39; height=&#39;249&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1129" data-rawheight="249" class="origin_image zh-lightbox-thumb lazy" width="1129" data-original="https://pic3.zhimg.com/v2-e9f7584d33432ca504571373feaacd02_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-e9f7584d33432ca504571373feaacd02_b.jpg"/></figure><p>（第一个式子的证明只需要利用期望算子的线性，这样就转化为了普通Bellman算子的contraction了；第二个式子还是把Bellman算子展开之后证明）</p><h3><b>4. Control</b></h3><p>前面讨论了prediction问题，现在讨论control的情形，即是否存在一个类似Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D" alt="\mathcal{T}" eeimg="1"/> ，使得对于任意的初始分布价值函数，都能够在Wasserstein距离度量下，收敛到最优分布价值函数？</p><p>先说结论，对于value iteration类算法，一般关心两件事情，即</p><ul><li>【问题1】是否每迭代一轮，都离最优分布函数更近，即在Wasserstein距离度量下有contraction？答案是<i><b>不能保证每轮迭代，Wasserstein距离都缩小</b></i>。在此问题上有一个更弱一点的结论，<i><b>其期望在 <img src="https://www.zhihu.com/equation?tex=%7C%7C%5Ccdot%7C%7C_%5Cinfty" alt="||\cdot||_\infty" eeimg="1"/> 度量下有contraction</b></i>。</li><li>【问题2】多轮迭代之后，是否能够收敛到最优分布价值函数？该问题通常分为两部分，即<i><b>是否有不动点</b></i>和<i><b>是否能收敛到不动点</b></i>。对于第一个问题，<i><b>在排除掉一些琐碎的简并情况后，它具有不动点，并且不动点属于某个最优分布价值函数。</b></i>对于第二个问题，<i><b>它能够在Wasserstein距离度量下收敛到一族nonstationary的最优价值函数</b></i>。（<i>由于我们考虑的“最优”仍然是相对于期望值的，因此把分布引入进来的时候，期望相同的不同分布都同等地“最优”，这会造成一些混淆，这样的混淆造成前面所说的“琐碎的简并情况”</i>）</li></ul><p>下面来具体说。</p><p>首先，最优策略的定义是按照均值来定义的，即均值最大的策略。相应的最优分布价值函数也是这种最优策略下对应的分布价值函数。</p><p>其次，Bellman算子定义如下</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-2132df99078f5690c5fe762071c5b650_b.png" data-caption="" data-size="normal" data-rawwidth="1010" data-rawheight="55" class="origin_image zh-lightbox-thumb" width="1010" data-original="https://pic1.zhimg.com/v2-2132df99078f5690c5fe762071c5b650_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1010&#39; height=&#39;55&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1010" data-rawheight="55" class="origin_image zh-lightbox-thumb lazy" width="1010" data-original="https://pic1.zhimg.com/v2-2132df99078f5690c5fe762071c5b650_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-2132df99078f5690c5fe762071c5b650_b.png"/></figure><p>其中相对于价值函数的贪心策略是相对于分布的期望来定义的（注意正是这样只考虑均值的定义造成了后面的琐碎）</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-1dd2468191a751162ef14b64ea7bd40d_b.png" data-caption="" data-size="normal" data-rawwidth="1009" data-rawheight="63" class="origin_image zh-lightbox-thumb" width="1009" data-original="https://pic2.zhimg.com/v2-1dd2468191a751162ef14b64ea7bd40d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1009&#39; height=&#39;63&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1009" data-rawheight="63" class="origin_image zh-lightbox-thumb lazy" width="1009" data-original="https://pic2.zhimg.com/v2-1dd2468191a751162ef14b64ea7bd40d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-1dd2468191a751162ef14b64ea7bd40d_b.png"/></figure><p>【问题1】</p><p>为了说明不能保证每轮迭代都是的Wasserstein距离减小，文中举了一个反例。如图所示，做 <img src="https://www.zhihu.com/equation?tex=a_1" alt="a_1" eeimg="1"/> 的时候确定性得到奖励0；做 <img src="https://www.zhihu.com/equation?tex=a_2" alt="a_2" eeimg="1"/> 的时候各一半的概率得到图上所示奖励。最优分布价值函数在表中 <img src="https://www.zhihu.com/equation?tex=Z%5E%2A" alt="Z^*" eeimg="1"/> ，从任意一个分布价值函数 <img src="https://www.zhihu.com/equation?tex=Z" alt="Z" eeimg="1"/> 出发，做一次迭代，得到 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7DZ" alt="\mathcal{T}Z" eeimg="1"/> 。观察到 <img src="https://www.zhihu.com/equation?tex=d_1%28%5Cmathcal%7BT%7DZ%2C+%5Cmathcal%7BT%7DZ%5E%2A%29+%3E+d_1%28Z%2C+Z%5E%2A%29" alt="d_1(\mathcal{T}Z, \mathcal{T}Z^*) &gt; d_1(Z, Z^*)" eeimg="1"/> 是可能发生的，即 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D" alt="\mathcal{T}" eeimg="1"/> 不是contraction。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-e96fa8f91e2fe6980a88f3c9b8c8b529_b.jpg" data-caption="" data-size="normal" data-rawwidth="1143" data-rawheight="406" class="origin_image zh-lightbox-thumb" width="1143" data-original="https://pic2.zhimg.com/v2-e96fa8f91e2fe6980a88f3c9b8c8b529_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1143&#39; height=&#39;406&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1143" data-rawheight="406" class="origin_image zh-lightbox-thumb lazy" width="1143" data-original="https://pic2.zhimg.com/v2-e96fa8f91e2fe6980a88f3c9b8c8b529_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-e96fa8f91e2fe6980a88f3c9b8c8b529_b.jpg"/></figure><p>但是，其一阶矩是contraction。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-f7e2f53c6440aa97abc1ce8c2ca14acb_b.jpg" data-caption="" data-size="normal" data-rawwidth="1068" data-rawheight="179" class="origin_image zh-lightbox-thumb" width="1068" data-original="https://pic4.zhimg.com/v2-f7e2f53c6440aa97abc1ce8c2ca14acb_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1068&#39; height=&#39;179&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1068" data-rawheight="179" class="origin_image zh-lightbox-thumb lazy" width="1068" data-original="https://pic4.zhimg.com/v2-f7e2f53c6440aa97abc1ce8c2ca14acb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-f7e2f53c6440aa97abc1ce8c2ca14acb_b.jpg"/></figure><p><i>（证明只需要利用期望的线性，转化为普通的Bellman算子contraction证明即可）</i></p><p>【问题2】</p><p>其收敛到的是一族不稳定的最优分布价值函数 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BZ%7D%5E%7B%2A%2A%7D" alt="\mathcal{Z}^{**}" eeimg="1"/> ，这是啥意思呢？对于一族最优策略，按照任意序列去执行这个最优策略，所对应的价值函数就是<b><i>不稳定的最优分布价值函数</i></b>。</p><p>只考虑期望的时候，最优价值函数只可能是一个（Banach’s fixed point theorem），但是考虑分布的时候，由于没有contraction，因此就很可能出现一族最优价值函数。可以看到之所以会出现一族最优价值函数，是因为会出现一族最优策略，如果对于所有最优策略排个序，只允许有一个最优策略，那么就会产生一个唯一的不动点（定理第二部分）。</p><p>为什么在考虑分布的情况下会出现不稳定的情况呢？个人认为，对于同样的价值函数 <img src="https://www.zhihu.com/equation?tex=Q%28x%2C%5Ccdot%29" alt="Q(x,\cdot)" eeimg="1"/> ，不论是greedy还是 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy策略都是没有歧义的；而对于分布价值函数 <img src="https://www.zhihu.com/equation?tex=Z%28x%2C%5Ccdot%29" alt="Z(x,\cdot)" eeimg="1"/> ，就算是对均值的greedy策略，在均值相同的情况下，还能够根据不同分布的其他高阶矩做出不同的行动，这就产生了一族在原本语义下同等最优的策略，这些策略造成了不稳定。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-fd68d0fa44280218afce508834e6d141_b.jpg" data-caption="" data-size="normal" data-rawwidth="830" data-rawheight="484" class="origin_image zh-lightbox-thumb" width="830" data-original="https://pic2.zhimg.com/v2-fd68d0fa44280218afce508834e6d141_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;830&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="830" data-rawheight="484" class="origin_image zh-lightbox-thumb lazy" width="830" data-original="https://pic2.zhimg.com/v2-fd68d0fa44280218afce508834e6d141_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-fd68d0fa44280218afce508834e6d141_b.jpg"/></figure><p>在不考虑对最优策略排序情况下，会产生以下不稳定的情形。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-8fcfb2c28e5163ce171698b851dbd391_b.png" data-caption="" data-size="normal" data-rawwidth="868" data-rawheight="77" class="origin_image zh-lightbox-thumb" width="868" data-original="https://pic2.zhimg.com/v2-8fcfb2c28e5163ce171698b851dbd391_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;868&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="868" data-rawheight="77" class="origin_image zh-lightbox-thumb lazy" width="868" data-original="https://pic2.zhimg.com/v2-8fcfb2c28e5163ce171698b851dbd391_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-8fcfb2c28e5163ce171698b851dbd391_b.png"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-c768e3d764ecb652ab06728419b95580_b.png" data-caption="" data-size="normal" data-rawwidth="856" data-rawheight="77" class="origin_image zh-lightbox-thumb" width="856" data-original="https://pic1.zhimg.com/v2-c768e3d764ecb652ab06728419b95580_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;856&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="856" data-rawheight="77" class="origin_image zh-lightbox-thumb lazy" width="856" data-original="https://pic1.zhimg.com/v2-c768e3d764ecb652ab06728419b95580_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-c768e3d764ecb652ab06728419b95580_b.png"/></figure><p>个人认为，定理一已经挺好的了，给出的不稳定情形的例子过于极端，都是分布均值相同，而分布不同的情况，这种情况实际数值计算中出现概率较小，不稳定可以被缓解。</p><p>如果上面的定理看迷糊了，这里提供一个易于理解的版本</p><blockquote>If the optimal policy is unique, then the iterates <img src="https://www.zhihu.com/equation?tex=Z+%5Cleftarrow+%5Cmathcal%7BT%7D+Z" alt="Z \leftarrow \mathcal{T} Z" eeimg="1"/>  converge to <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Cpi%5E%2A%7D" alt="Z^{\pi^*}" eeimg="1"/> .</blockquote><h3><b>5. 算法</b></h3><p>首先第一个问题是如何表示一个分布，之前已经有算法来使用高斯分布来表示价值函数的分布，这种做法的缺点在于不能够表示多模的分布。本文把价值函数值的取值范围 <img src="https://www.zhihu.com/equation?tex=%5BV_%5Cmin%2C+V_%5Cmax%5D" alt="[V_\min, V_\max]" eeimg="1"/> 分为 <img src="https://www.zhihu.com/equation?tex=N" alt="N" eeimg="1"/> 个格子，每个格子代表范围为 <img src="https://www.zhihu.com/equation?tex=%5CDelta+z%3D%5Cfrac%7BV_%5Cmax+-+V_%5Cmin%7D%7BN-1%7D" alt="\Delta z=\frac{V_\max - V_\min}{N-1}" eeimg="1"/> 的价值函数值，然后分别估计价值函数值落在每个格子内的概率。</p><p>使用神经网络 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%3A%5Cmathcal%7BX%7D+%5Ctimes+%5Cmathcal%7BA%7D+%5Cto+%5Cmathbb%7BR%7D%5EN" alt="\theta:\mathcal{X} \times \mathcal{A} \to \mathbb{R}^N" eeimg="1"/> ，使用Boltzmann分布来表示价值函数的分布</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-5955cb23fc2a34f9ab1ef87ed0312c3a_b.png" data-caption="" data-size="normal" data-rawwidth="1017" data-rawheight="94" class="origin_image zh-lightbox-thumb" width="1017" data-original="https://pic3.zhimg.com/v2-5955cb23fc2a34f9ab1ef87ed0312c3a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1017&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1017" data-rawheight="94" class="origin_image zh-lightbox-thumb lazy" width="1017" data-original="https://pic3.zhimg.com/v2-5955cb23fc2a34f9ab1ef87ed0312c3a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-5955cb23fc2a34f9ab1ef87ed0312c3a_b.png"/></figure><p>这样表示之后还会存在一个问题，就是本来在同一个格子的值，在通过Bellman算子的更新之后，不能保证还落在同一个格子里面（因为有 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"/> 产生收缩，同时奖励值也不能保证正好是 <img src="https://www.zhihu.com/equation?tex=%5CDelta+z" alt="\Delta z" eeimg="1"/> 的整数倍）。因此还需要做一个投影的操作，即按照线性比例投影到最近的格子中。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-4cafd20ae4694bc0455f1483e21e3d13_b.png" data-caption="" data-size="normal" data-rawwidth="1013" data-rawheight="120" class="origin_image zh-lightbox-thumb" width="1013" data-original="https://pic4.zhimg.com/v2-4cafd20ae4694bc0455f1483e21e3d13_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1013&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1013" data-rawheight="120" class="origin_image zh-lightbox-thumb lazy" width="1013" data-original="https://pic4.zhimg.com/v2-4cafd20ae4694bc0455f1483e21e3d13_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4cafd20ae4694bc0455f1483e21e3d13_b.png"/></figure><p>整个Bellman算子的更新操作可以由下图表示</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-bf950bf08a316dfd75082adb4d08ea48_b.jpg" data-caption="" data-size="normal" data-rawwidth="883" data-rawheight="545" class="origin_image zh-lightbox-thumb" width="883" data-original="https://pic1.zhimg.com/v2-bf950bf08a316dfd75082adb4d08ea48_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;883&#39; height=&#39;545&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="883" data-rawheight="545" class="origin_image zh-lightbox-thumb lazy" width="883" data-original="https://pic1.zhimg.com/v2-bf950bf08a316dfd75082adb4d08ea48_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-bf950bf08a316dfd75082adb4d08ea48_b.jpg"/></figure><p>最后，相对于输出 <img src="https://www.zhihu.com/equation?tex=N" alt="N" eeimg="1"/> 个数值的神经网络来说，对分布拟合的目标就可以自然转化为cross-entropy的损失函数，接下来使用梯度下降类方法就可以对该目标进行优化。最后得到下面的算法。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-acb945c9601d8a910457b0cac9dba266_b.jpg" data-caption="" data-size="normal" data-rawwidth="851" data-rawheight="618" class="origin_image zh-lightbox-thumb" width="851" data-original="https://pic3.zhimg.com/v2-acb945c9601d8a910457b0cac9dba266_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;851&#39; height=&#39;618&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="851" data-rawheight="618" class="origin_image zh-lightbox-thumb lazy" width="851" data-original="https://pic3.zhimg.com/v2-acb945c9601d8a910457b0cac9dba266_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-acb945c9601d8a910457b0cac9dba266_b.jpg"/></figure><h2><b>实验结果</b></h2><p>文章在ALE上做实验，毕竟Q-learning相关算法要求行动空间是离散的。个人认为实验结果里面有以下看点。</p><p>首先，该算法能够学习到非平庸的情况，而不是全是看起来类似Gaussian的分布。比如对于一些致命的操作，能够在分布中很明确地反映出来。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d885091acffc8fa5a13af4df39d76330_b.jpg" data-size="normal" data-rawwidth="871" data-rawheight="496" class="origin_image zh-lightbox-thumb" width="871" data-original="https://pic1.zhimg.com/v2-d885091acffc8fa5a13af4df39d76330_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;871&#39; height=&#39;496&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="871" data-rawheight="496" class="origin_image zh-lightbox-thumb lazy" width="871" data-original="https://pic1.zhimg.com/v2-d885091acffc8fa5a13af4df39d76330_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-d885091acffc8fa5a13af4df39d76330_b.jpg"/><figcaption>一些致命的操作，能够在分布中很明确地反映出来</figcaption></figure><p>其次，实验显示对于一些简并的状态，能够学习到多模的分布（两种可能的情况的加和），之前的很多算法是不能够表示出来这样的结果的。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-89a0e09d2843f9e1da6e1069055210c2_b.png" data-size="normal" data-rawwidth="1447" data-rawheight="232" class="origin_image zh-lightbox-thumb" width="1447" data-original="https://pic3.zhimg.com/v2-89a0e09d2843f9e1da6e1069055210c2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1447&#39; height=&#39;232&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1447" data-rawheight="232" class="origin_image zh-lightbox-thumb lazy" width="1447" data-original="https://pic3.zhimg.com/v2-89a0e09d2843f9e1da6e1069055210c2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-89a0e09d2843f9e1da6e1069055210c2_b.png"/><figcaption>对于一些不确定的状态，能够学习到多模的分布</figcaption></figure><p>最后，该算法对于奖励十分稀疏的任务提升较大，主要是由于稀疏的奖励在分布的传播中相对不容易lost。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-2d90d07d20d7010cfbc3ef66f2ea5023_b.jpg" data-size="normal" data-rawwidth="1517" data-rawheight="590" class="origin_image zh-lightbox-thumb" width="1517" data-original="https://pic4.zhimg.com/v2-2d90d07d20d7010cfbc3ef66f2ea5023_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1517&#39; height=&#39;590&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1517" data-rawheight="590" class="origin_image zh-lightbox-thumb lazy" width="1517" data-original="https://pic4.zhimg.com/v2-2d90d07d20d7010cfbc3ef66f2ea5023_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-2d90d07d20d7010cfbc3ef66f2ea5023_b.jpg"/><figcaption>注意到最后一个任务奖励稀疏，相比于之前的算法提升较大</figcaption></figure><hr/><p>强化学习里面考虑价值函数的分布/深度学习里面考虑预测数值的分布，在金融中有比较实际的意义，很多情况我们希望能够在提高期望收益的时候同时控制风险，这样给出一个分布有用的信息就比单纯一个期望值的信息大很多。比如可以优化一个lower confidence bound而不仅仅是一个均值。</p><hr/><p></p></div></div><div class="ContentItem-time">编辑于 2019-03-29</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19609455" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">金融</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 57 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 57</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>3 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="66b4240a-bf9a-4e2e-975c-6f41eee6ee75" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="66b4240a-bf9a-4e2e-975c-6f41eee6ee75">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"60632660":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":60632660,"title":"【强化学习 47】Distributional RL","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F60632660","imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1aa6bd6a904b9c5f502a37a443d7c851_b.jpg","titleImage":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1aa6bd6a904b9c5f502a37a443d7c851_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-93cfff9ac6b81150513f9b64d89e9678_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"981\" data-rawheight=\"148\" data-watermark=\"\" data-original-src=\"\" data-watermark-src=\"\" data-private-watermark-src=\"\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-93cfff9ac6b81150513f9b64d89e9678_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"\u002F\u003E一篇很有启发性的工作，目前主流的强化学习方法主要关注价值函数的均值，这里提出把价值函数的分布也考虑进来。\u003Cb\u003E原文传送门\u003C\u002Fb\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1707.06887\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBellemare, Marc G., Will Dabney, and Rémi Munos. &#34;A distributional perspective on reinforcement learning.&#34; Proceedings of t…\u003C\u002Fa\u003E","created":1553764474,"updated":1553821974,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1592,"imageHeight":460,"content":"\u003Cp\u003E一篇很有启发性的工作，目前主流的强化学习方法主要关注价值函数的均值，这里提出把价值函数的分布也考虑进来。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E原文传送门\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1707.06887\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBellemare, Marc G., Will Dabney, and Rémi Munos. &#34;A distributional perspective on reinforcement learning.&#34; Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E特色\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E这篇工作很有启发性，讲述了当价值函数不在仅仅是一个期望值而是是一个分布会怎样？相应的Bellman Operator是否还具有较好的性质？是否能够形成一个有效的算法？把分布考虑进来是否能够带来算法性能的提升？\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E过程\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch3\u003E\u003Cb\u003E1. 考虑价值函数的分布有什么好处？\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cul\u003E\u003Cli\u003E分布相比于均值能够对决策提供更多的信息，比如对于某些risk aware的场景，我们可能会更倾向于选择方差较小或者最坏情况较好的行动，而不是一味地选择均值较高的行动；\u003C\u002Fli\u003E\u003Cli\u003E对于某些具有简并状态（state aliasing）的MDP或者POMDP，表征上看起来一样的状态可能具有完全不一样的两个价值函数，如果仅仅考虑均值，这部分信息就会被完全混淆；\u003C\u002Fli\u003E\u003Cli\u003E考虑价值函数的分布能够缓解奖励稀疏的问题，较为稀疏的奖励会在通常的迭代中慢慢“稀释”，如果像文中的做法，稀疏的奖励在传播过程中更容易被留存下来（看到具体的算法之后才容易体会到这一点）\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E\u003Cb\u003E2. 价值函数的分布表示和距离度量\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp\u003E用分布来表示价值函数只需要把通常的价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28x%29\" alt=\"V(x)\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28x%2Ca%29\" alt=\"Q(x,a)\" eeimg=\"1\"\u002F\u003E ，替换成相应的分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%28x%29\" alt=\"Z(x)\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%28x%2Ca%29\" alt=\"Z(x,a)\" eeimg=\"1\"\u002F\u003E 即可。\u003C\u002Fp\u003E\u003Cp\u003E距离度量使用Wasserstein距离\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d_p%28U%2C+V%29+%3A%3D+%5Cinf_%7BU%2CV%7D+%7C%7CU-V%7C%7C_p+%3A%3D+%5Cinf_%7BU%2CV%7D+%5B%5Cmathbb%7BE%7D_%7B%5Comega%5Cin+%5COmega%7D+%7C%7CU%28%5Comega%29-V%28%5Comega%29%7C%7C%5Ep%5D%5E%7B1%2Fp%7D\" alt=\"d_p(U, V) := \\inf_{U,V} ||U-V||_p := \\inf_{U,V} [\\mathbb{E}_{\\omega\\in \\Omega} ||U(\\omega)-V(\\omega)||^p]^{1\u002Fp}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E对于两个随机变量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=U%2CV\" alt=\"U,V\" eeimg=\"1\"\u002F\u003E （由于本文讨论价值函数的分布，可以认为它们都是一维随机变量），第一个等式中的infimum是对于所有符合 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=U%2CV\" alt=\"U,V\" eeimg=\"1\"\u002F\u003E 各自边缘分布的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28U%2CV%29\" alt=\"(U,V)\" eeimg=\"1\"\u002F\u003E 联合分布（参见本专栏另外的文章Wasserstein距离）。 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Comega+%5Cin+%5COmega\" alt=\"\\omega \\in \\Omega\" eeimg=\"1\"\u002F\u003E 表示对于所有可能的实验结果取期望。\u003C\u002Fp\u003E\u003Cp\u003E该距离度量有以下性质\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-93cfff9ac6b81150513f9b64d89e9678_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"981\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"981\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-93cfff9ac6b81150513f9b64d89e9678_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;981&#39; height=&#39;148&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"981\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"981\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-93cfff9ac6b81150513f9b64d89e9678_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-93cfff9ac6b81150513f9b64d89e9678_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-11faa0ee68bbd5dd29dd3691b0b5b5d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1036\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"1036\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-11faa0ee68bbd5dd29dd3691b0b5b5d8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1036&#39; height=&#39;242&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1036\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1036\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-11faa0ee68bbd5dd29dd3691b0b5b5d8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-11faa0ee68bbd5dd29dd3691b0b5b5d8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Ci\u003E（观察到，该引理左边可以在所有的联合分布中寻找最小值，而该定理的右边要求所寻找的联合分布协方差矩阵还需要按照A的划分是分块矩阵，这显然会使得找到的最小值更大，证明按照此思路易得）\u003C\u002Fi\u003E\u003C\u002Fp\u003E\u003Cp\u003E注意到以上的定义都只针对两个表征价值的随机变量，当考虑价值函数的时候，还有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 或者 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E 的自变量输入，下面定义\u003Ci\u003E\u003Cb\u003E两个分布价值函数之间的距离度量\u003C\u002Fb\u003E\u003C\u002Fi\u003E。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2a698d33cce4608b0c903e0b7ceded57_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb\" width=\"982\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2a698d33cce4608b0c903e0b7ceded57_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;982&#39; height=&#39;77&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"982\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2a698d33cce4608b0c903e0b7ceded57_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2a698d33cce4608b0c903e0b7ceded57_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以证明它是一个距离度量，即满足三角不等式。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E3. Policy Evaluation\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E有了分布价值函数的定义之后，我们考虑的第一个问题是对于一个给定的策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E ，是否存在一个类似Bellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D%5E%5Cpi\" alt=\"\\mathcal{T}^\\pi\" eeimg=\"1\"\u002F\u003E ，使得对于任意的初始分布价值函数，都能够在上面所定义的分布距离度量下，收敛到其真实分布价值函数？即常说的policy evaluation或者prediction问题（Sutton书）。\u003C\u002Fp\u003E\u003Cp\u003E定义Bellman算子\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-80a0fe81e2e81f780b64fb8ebf233c81_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"1042\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-80a0fe81e2e81f780b64fb8ebf233c81_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1042&#39; height=&#39;66&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1042\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-80a0fe81e2e81f780b64fb8ebf233c81_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-80a0fe81e2e81f780b64fb8ebf233c81_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a7548511984611b701f3302db3abde15_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"997\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"997\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a7548511984611b701f3302db3abde15_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;997&#39; height=&#39;108&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"997\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"997\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a7548511984611b701f3302db3abde15_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a7548511984611b701f3302db3abde15_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E对于这样的算子有很好的结果，即对于任意的初始分布价值函数，都能够在Wasserstein距离度量下，收敛到其真实分布价值函数。要证明这一点只需要证明contraction。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f37855aef159f62ad46cac171817dbfd_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1086\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb\" width=\"1086\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f37855aef159f62ad46cac171817dbfd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1086&#39; height=&#39;62&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1086\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1086\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f37855aef159f62ad46cac171817dbfd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f37855aef159f62ad46cac171817dbfd_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Ci\u003E（要证明这一点即证明 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7Bd%7D_p%28%5Cmathcal%7BT%7D%5E%5Cpi+Z_1%2C+%5Cmathcal%7BT%7D%5E%5Cpi+Z_2%29+%5Cle+%5Cgamma+%5Cbar%7Bd%7D_p%28+Z_1%2C++Z_2%29\" alt=\"\\bar{d}_p(\\mathcal{T}^\\pi Z_1, \\mathcal{T}^\\pi Z_2) \\le \\gamma \\bar{d}_p( Z_1,  Z_2)\" eeimg=\"1\"\u002F\u003E ，把Bellman算子展开并且利用前面关于Wasserstein距离的性质就可以得到）\u003C\u002Fi\u003E\u003C\u002Fp\u003E\u003Cp\u003E另外，Bellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D%5E%5Cpi\" alt=\"\\mathcal{T}^\\pi\" eeimg=\"1\"\u002F\u003E 对于分布的前两阶中心矩也是contraction。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9f7584d33432ca504571373feaacd02_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1129\" data-rawheight=\"249\" class=\"origin_image zh-lightbox-thumb\" width=\"1129\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9f7584d33432ca504571373feaacd02_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1129&#39; height=&#39;249&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1129\" data-rawheight=\"249\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1129\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9f7584d33432ca504571373feaacd02_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9f7584d33432ca504571373feaacd02_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E（第一个式子的证明只需要利用期望算子的线性，这样就转化为了普通Bellman算子的contraction了；第二个式子还是把Bellman算子展开之后证明）\u003C\u002Fp\u003E\u003Ch3\u003E\u003Cb\u003E4. Control\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp\u003E前面讨论了prediction问题，现在讨论control的情形，即是否存在一个类似Bellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D\" alt=\"\\mathcal{T}\" eeimg=\"1\"\u002F\u003E ，使得对于任意的初始分布价值函数，都能够在Wasserstein距离度量下，收敛到最优分布价值函数？\u003C\u002Fp\u003E\u003Cp\u003E先说结论，对于value iteration类算法，一般关心两件事情，即\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E【问题1】是否每迭代一轮，都离最优分布函数更近，即在Wasserstein距离度量下有contraction？答案是\u003Ci\u003E\u003Cb\u003E不能保证每轮迭代，Wasserstein距离都缩小\u003C\u002Fb\u003E\u003C\u002Fi\u003E。在此问题上有一个更弱一点的结论，\u003Ci\u003E\u003Cb\u003E其期望在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%7C%5Ccdot%7C%7C_%5Cinfty\" alt=\"||\\cdot||_\\infty\" eeimg=\"1\"\u002F\u003E 度量下有contraction\u003C\u002Fb\u003E\u003C\u002Fi\u003E。\u003C\u002Fli\u003E\u003Cli\u003E【问题2】多轮迭代之后，是否能够收敛到最优分布价值函数？该问题通常分为两部分，即\u003Ci\u003E\u003Cb\u003E是否有不动点\u003C\u002Fb\u003E\u003C\u002Fi\u003E和\u003Ci\u003E\u003Cb\u003E是否能收敛到不动点\u003C\u002Fb\u003E\u003C\u002Fi\u003E。对于第一个问题，\u003Ci\u003E\u003Cb\u003E在排除掉一些琐碎的简并情况后，它具有不动点，并且不动点属于某个最优分布价值函数。\u003C\u002Fb\u003E\u003C\u002Fi\u003E对于第二个问题，\u003Ci\u003E\u003Cb\u003E它能够在Wasserstein距离度量下收敛到一族nonstationary的最优价值函数\u003C\u002Fb\u003E\u003C\u002Fi\u003E。（\u003Ci\u003E由于我们考虑的“最优”仍然是相对于期望值的，因此把分布引入进来的时候，期望相同的不同分布都同等地“最优”，这会造成一些混淆，这样的混淆造成前面所说的“琐碎的简并情况”\u003C\u002Fi\u003E）\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E下面来具体说。\u003C\u002Fp\u003E\u003Cp\u003E首先，最优策略的定义是按照均值来定义的，即均值最大的策略。相应的最优分布价值函数也是这种最优策略下对应的分布价值函数。\u003C\u002Fp\u003E\u003Cp\u003E其次，Bellman算子定义如下\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2132df99078f5690c5fe762071c5b650_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"55\" class=\"origin_image zh-lightbox-thumb\" width=\"1010\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2132df99078f5690c5fe762071c5b650_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1010&#39; height=&#39;55&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"55\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1010\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2132df99078f5690c5fe762071c5b650_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2132df99078f5690c5fe762071c5b650_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中相对于价值函数的贪心策略是相对于分布的期望来定义的（注意正是这样只考虑均值的定义造成了后面的琐碎）\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1dd2468191a751162ef14b64ea7bd40d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1009\" data-rawheight=\"63\" class=\"origin_image zh-lightbox-thumb\" width=\"1009\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1dd2468191a751162ef14b64ea7bd40d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1009&#39; height=&#39;63&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1009\" data-rawheight=\"63\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1009\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1dd2468191a751162ef14b64ea7bd40d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1dd2468191a751162ef14b64ea7bd40d_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E【问题1】\u003C\u002Fp\u003E\u003Cp\u003E为了说明不能保证每轮迭代都是的Wasserstein距离减小，文中举了一个反例。如图所示，做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_1\" alt=\"a_1\" eeimg=\"1\"\u002F\u003E 的时候确定性得到奖励0；做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_2\" alt=\"a_2\" eeimg=\"1\"\u002F\u003E 的时候各一半的概率得到图上所示奖励。最优分布价值函数在表中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%2A\" alt=\"Z^*\" eeimg=\"1\"\u002F\u003E ，从任意一个分布价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z\" alt=\"Z\" eeimg=\"1\"\u002F\u003E 出发，做一次迭代，得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7DZ\" alt=\"\\mathcal{T}Z\" eeimg=\"1\"\u002F\u003E 。观察到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d_1%28%5Cmathcal%7BT%7DZ%2C+%5Cmathcal%7BT%7DZ%5E%2A%29+%3E+d_1%28Z%2C+Z%5E%2A%29\" alt=\"d_1(\\mathcal{T}Z, \\mathcal{T}Z^*) &gt; d_1(Z, Z^*)\" eeimg=\"1\"\u002F\u003E 是可能发生的，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D\" alt=\"\\mathcal{T}\" eeimg=\"1\"\u002F\u003E 不是contraction。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-e96fa8f91e2fe6980a88f3c9b8c8b529_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1143\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb\" width=\"1143\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-e96fa8f91e2fe6980a88f3c9b8c8b529_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1143&#39; height=&#39;406&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1143\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1143\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-e96fa8f91e2fe6980a88f3c9b8c8b529_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-e96fa8f91e2fe6980a88f3c9b8c8b529_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E但是，其一阶矩是contraction。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f7e2f53c6440aa97abc1ce8c2ca14acb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"179\" class=\"origin_image zh-lightbox-thumb\" width=\"1068\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f7e2f53c6440aa97abc1ce8c2ca14acb_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1068&#39; height=&#39;179&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"179\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1068\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f7e2f53c6440aa97abc1ce8c2ca14acb_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f7e2f53c6440aa97abc1ce8c2ca14acb_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Ci\u003E（证明只需要利用期望的线性，转化为普通的Bellman算子contraction证明即可）\u003C\u002Fi\u003E\u003C\u002Fp\u003E\u003Cp\u003E【问题2】\u003C\u002Fp\u003E\u003Cp\u003E其收敛到的是一族不稳定的最优分布价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BZ%7D%5E%7B%2A%2A%7D\" alt=\"\\mathcal{Z}^{**}\" eeimg=\"1\"\u002F\u003E ，这是啥意思呢？对于一族最优策略，按照任意序列去执行这个最优策略，所对应的价值函数就是\u003Cb\u003E\u003Ci\u003E不稳定的最优分布价值函数\u003C\u002Fi\u003E\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E只考虑期望的时候，最优价值函数只可能是一个（Banach’s fixed point theorem），但是考虑分布的时候，由于没有contraction，因此就很可能出现一族最优价值函数。可以看到之所以会出现一族最优价值函数，是因为会出现一族最优策略，如果对于所有最优策略排个序，只允许有一个最优策略，那么就会产生一个唯一的不动点（定理第二部分）。\u003C\u002Fp\u003E\u003Cp\u003E为什么在考虑分布的情况下会出现不稳定的情况呢？个人认为，对于同样的价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28x%2C%5Ccdot%29\" alt=\"Q(x,\\cdot)\" eeimg=\"1\"\u002F\u003E ，不论是greedy还是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy策略都是没有歧义的；而对于分布价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%28x%2C%5Ccdot%29\" alt=\"Z(x,\\cdot)\" eeimg=\"1\"\u002F\u003E ，就算是对均值的greedy策略，在均值相同的情况下，还能够根据不同分布的其他高阶矩做出不同的行动，这就产生了一族在原本语义下同等最优的策略，这些策略造成了不稳定。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-fd68d0fa44280218afce508834e6d141_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"830\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"830\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-fd68d0fa44280218afce508834e6d141_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;830&#39; height=&#39;484&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"830\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"830\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-fd68d0fa44280218afce508834e6d141_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-fd68d0fa44280218afce508834e6d141_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在不考虑对最优策略排序情况下，会产生以下不稳定的情形。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8fcfb2c28e5163ce171698b851dbd391_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb\" width=\"868\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8fcfb2c28e5163ce171698b851dbd391_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;868&#39; height=&#39;77&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"868\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8fcfb2c28e5163ce171698b851dbd391_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8fcfb2c28e5163ce171698b851dbd391_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c768e3d764ecb652ab06728419b95580_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"856\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb\" width=\"856\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c768e3d764ecb652ab06728419b95580_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;856&#39; height=&#39;77&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"856\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"856\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c768e3d764ecb652ab06728419b95580_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c768e3d764ecb652ab06728419b95580_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E个人认为，定理一已经挺好的了，给出的不稳定情形的例子过于极端，都是分布均值相同，而分布不同的情况，这种情况实际数值计算中出现概率较小，不稳定可以被缓解。\u003C\u002Fp\u003E\u003Cp\u003E如果上面的定理看迷糊了，这里提供一个易于理解的版本\u003C\u002Fp\u003E\u003Cblockquote\u003EIf the optimal policy is unique, then the iterates \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z+%5Cleftarrow+%5Cmathcal%7BT%7D+Z\" alt=\"Z \\leftarrow \\mathcal{T} Z\" eeimg=\"1\"\u002F\u003E  converge to \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Z%5E%7B%5Cpi%5E%2A%7D\" alt=\"Z^{\\pi^*}\" eeimg=\"1\"\u002F\u003E .\u003C\u002Fblockquote\u003E\u003Ch3\u003E\u003Cb\u003E5. 算法\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp\u003E首先第一个问题是如何表示一个分布，之前已经有算法来使用高斯分布来表示价值函数的分布，这种做法的缺点在于不能够表示多模的分布。本文把价值函数值的取值范围 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5BV_%5Cmin%2C+V_%5Cmax%5D\" alt=\"[V_\\min, V_\\max]\" eeimg=\"1\"\u002F\u003E 分为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=N\" alt=\"N\" eeimg=\"1\"\u002F\u003E 个格子，每个格子代表范围为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+z%3D%5Cfrac%7BV_%5Cmax+-+V_%5Cmin%7D%7BN-1%7D\" alt=\"\\Delta z=\\frac{V_\\max - V_\\min}{N-1}\" eeimg=\"1\"\u002F\u003E 的价值函数值，然后分别估计价值函数值落在每个格子内的概率。\u003C\u002Fp\u003E\u003Cp\u003E使用神经网络 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta%3A%5Cmathcal%7BX%7D+%5Ctimes+%5Cmathcal%7BA%7D+%5Cto+%5Cmathbb%7BR%7D%5EN\" alt=\"\\theta:\\mathcal{X} \\times \\mathcal{A} \\to \\mathbb{R}^N\" eeimg=\"1\"\u002F\u003E ，使用Boltzmann分布来表示价值函数的分布\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5955cb23fc2a34f9ab1ef87ed0312c3a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1017\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"1017\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5955cb23fc2a34f9ab1ef87ed0312c3a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1017&#39; height=&#39;94&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1017\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1017\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5955cb23fc2a34f9ab1ef87ed0312c3a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5955cb23fc2a34f9ab1ef87ed0312c3a_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这样表示之后还会存在一个问题，就是本来在同一个格子的值，在通过Bellman算子的更新之后，不能保证还落在同一个格子里面（因为有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 产生收缩，同时奖励值也不能保证正好是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+z\" alt=\"\\Delta z\" eeimg=\"1\"\u002F\u003E 的整数倍）。因此还需要做一个投影的操作，即按照线性比例投影到最近的格子中。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4cafd20ae4694bc0455f1483e21e3d13_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1013\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb\" width=\"1013\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4cafd20ae4694bc0455f1483e21e3d13_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1013&#39; height=&#39;120&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1013\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1013\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4cafd20ae4694bc0455f1483e21e3d13_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4cafd20ae4694bc0455f1483e21e3d13_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E整个Bellman算子的更新操作可以由下图表示\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf950bf08a316dfd75082adb4d08ea48_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"883\" data-rawheight=\"545\" class=\"origin_image zh-lightbox-thumb\" width=\"883\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf950bf08a316dfd75082adb4d08ea48_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;883&#39; height=&#39;545&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"883\" data-rawheight=\"545\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"883\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf950bf08a316dfd75082adb4d08ea48_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf950bf08a316dfd75082adb4d08ea48_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E最后，相对于输出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=N\" alt=\"N\" eeimg=\"1\"\u002F\u003E 个数值的神经网络来说，对分布拟合的目标就可以自然转化为cross-entropy的损失函数，接下来使用梯度下降类方法就可以对该目标进行优化。最后得到下面的算法。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-acb945c9601d8a910457b0cac9dba266_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"618\" class=\"origin_image zh-lightbox-thumb\" width=\"851\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-acb945c9601d8a910457b0cac9dba266_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;851&#39; height=&#39;618&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"618\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"851\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-acb945c9601d8a910457b0cac9dba266_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-acb945c9601d8a910457b0cac9dba266_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E实验结果\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E文章在ALE上做实验，毕竟Q-learning相关算法要求行动空间是离散的。个人认为实验结果里面有以下看点。\u003C\u002Fp\u003E\u003Cp\u003E首先，该算法能够学习到非平庸的情况，而不是全是看起来类似Gaussian的分布。比如对于一些致命的操作，能够在分布中很明确地反映出来。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d885091acffc8fa5a13af4df39d76330_b.jpg\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"871\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d885091acffc8fa5a13af4df39d76330_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;871&#39; height=&#39;496&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"871\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d885091acffc8fa5a13af4df39d76330_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d885091acffc8fa5a13af4df39d76330_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E一些致命的操作，能够在分布中很明确地反映出来\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其次，实验显示对于一些简并的状态，能够学习到多模的分布（两种可能的情况的加和），之前的很多算法是不能够表示出来这样的结果的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-89a0e09d2843f9e1da6e1069055210c2_b.png\" data-size=\"normal\" data-rawwidth=\"1447\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb\" width=\"1447\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-89a0e09d2843f9e1da6e1069055210c2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1447&#39; height=&#39;232&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1447\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1447\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-89a0e09d2843f9e1da6e1069055210c2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-89a0e09d2843f9e1da6e1069055210c2_b.png\"\u002F\u003E\u003Cfigcaption\u003E对于一些不确定的状态，能够学习到多模的分布\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E最后，该算法对于奖励十分稀疏的任务提升较大，主要是由于稀疏的奖励在分布的传播中相对不容易lost。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d90d07d20d7010cfbc3ef66f2ea5023_b.jpg\" data-size=\"normal\" data-rawwidth=\"1517\" data-rawheight=\"590\" class=\"origin_image zh-lightbox-thumb\" width=\"1517\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d90d07d20d7010cfbc3ef66f2ea5023_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1517&#39; height=&#39;590&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1517\" data-rawheight=\"590\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1517\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d90d07d20d7010cfbc3ef66f2ea5023_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d90d07d20d7010cfbc3ef66f2ea5023_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E注意到最后一个任务奖励稀疏，相比于之前的算法提升较大\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Chr\u002F\u003E\u003Cp\u003E强化学习里面考虑价值函数的分布\u002F深度学习里面考虑预测数值的分布，在金融中有比较实际的意义，很多情况我们希望能够在提高期望收益的时候同时控制风险，这样给出一个分布有用的信息就比单纯一个期望值的信息大很多。比如可以优化一个lower confidence bound而不仅仅是一个均值。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19609455","type":"topic","id":"19609455","name":"金融"}],"voteupCount":57,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":3,"contributions":[{"id":20512884,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 47】Distributional RL - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F60632660 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F60632660","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F60632660","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>