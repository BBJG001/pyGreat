<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 40】PlaNet - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="Google Brain和DeepMind合作的一个model-based强化学习算法——Deep Planning Network（PlaNet）。原文传送门Hafner, Danijar, et al. &amp;#34;Learning Latent Dynamics for Planning from Pixels.&amp;#34; arXiv prepr…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 40】PlaNet"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/57468070"/><meta data-react-helmet="true" property="og:description" content="Google Brain和DeepMind合作的一个model-based强化学习算法——Deep Planning Network（PlaNet）。原文传送门Hafner, Danijar, et al. &amp;#34;Learning Latent Dynamics for Planning from Pixels.&amp;#34; arXiv prepr…"/><meta data-react-helmet="true" property="og:image" content="https://pic2.zhimg.com/v2-9f2b42aa87a2d3b1d1939aa41e755dba_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:57468070,&quot;title&quot;:&quot;【强化学习 40】PlaNet&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic2.zhimg.com/v2-9f2b42aa87a2d3b1d1939aa41e755dba_1200x500.jpg" alt="【强化学习 40】PlaNet"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 40】PlaNet</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">25 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>Google Brain和DeepMind合作的一个model-based强化学习算法——Deep Planning Network（PlaNet）。</p><h2><b>原文传送门</b></h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1811.04551" class=" wrap external" target="_blank" rel="nofollow noreferrer">Hafner, Danijar, et al. &#34;Learning Latent Dynamics for Planning from Pixels.&#34; arXiv preprint arXiv:1811.04551 (2018).</a></p><h2><b>特色</b></h2><p>一种基于模型（model-based）的强化学习算法，能够直接图像作为输入来控制机器人；规划（planning）的过程在隐空间（latent space）上进行，速度较快；采取了一些措施避免了长距离规划中状态估计不准确的问题。</p><h2><b>过程</b></h2><ol><li><b>总体流程</b></li></ol><p>既然是基于模型的方法，那么其两大问题主要就是<b><u><i>如何学习模型</i></u></b>和<b><u><i>如何利用模型来做规划</i></u></b>。</p><p>首先，模型包含些什么？</p><p>这里考虑一个基于图像的输入，这是一个高维度的输入，因此我们认为这是一个POMDP。图像输入是观测量 <img src="https://www.zhihu.com/equation?tex=o_t" alt="o_t" eeimg="1"/> ，状态是一个隐变量 <img src="https://www.zhihu.com/equation?tex=s_t" alt="s_t" eeimg="1"/> 。基于模型的方法中的“模型”指的是transition model <img src="https://www.zhihu.com/equation?tex=p%28s_t%7Cs_%7Bt-1%7D%2C+a_%7Bt-1%7D%29" alt="p(s_t|s_{t-1}, a_{t-1})" eeimg="1"/> 和 reward model <img src="https://www.zhihu.com/equation?tex=p%28r_t%7Cs_t%29" alt="p(r_t|s_t)" eeimg="1"/> ，POMDP还带来了observation model <img src="https://www.zhihu.com/equation?tex=p%28o_t%7Cs_t%29" alt="p(o_t|s_t)" eeimg="1"/> 。另外我们还需要一个encoder，快速地利用可观测量 <img src="https://www.zhihu.com/equation?tex=o_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D" alt="o_{\le t}, a_{&lt; t}" eeimg="1"/> 来估计隐变量 <img src="https://www.zhihu.com/equation?tex=s_t" alt="s_t" eeimg="1"/> ， <img src="https://www.zhihu.com/equation?tex=p%28s_t%7Co_%7B%5Cle+t%7D%2Ca_%7B%3C+t%7D%29" alt="p(s_t|o_{\le t},a_{&lt; t})" eeimg="1"/> 。</p><p>总结一下，要学习以下四个模型的参数 transition model, reward model, observation model, encoder。</p><p>规划的流程如下，使用学习到的模型来针对不同的行动预测收益，并选取最好的下一步的行动。</p><p><img src="https://www.zhihu.com/equation?tex=o_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D+%5Cto+%5Ctext%7Bencoder%7D+%5Cto+s_t+%5Cto+%5Ctext%7Btransition+model%7D+%5Cto+s_%7Bt%2B1%7D%2C+s_%7Bt%2B2%7D%2C+%5Ccdots+%5Cto+%5Ctext%7Breward+model%7D+%5Cto+R+%5Cto+%5Ctext%7BCEM%7D+%5Cto+a_%7Bt%7D" alt="o_{\le t}, a_{&lt; t} \to \text{encoder} \to s_t \to \text{transition model} \to s_{t+1}, s_{t+2}, \cdots \to \text{reward model} \to R \to \text{CEM} \to a_{t}" eeimg="1"/> </p><p>模型学习的流程如下</p><p><img src="https://www.zhihu.com/equation?tex=a_%7B1%3AT%7D%2C+o_%7B1%3AT%7D%5Cto+%5Ctext%7BSGD+on+variational+bound%7D+%5Cto+%5Ctext%7Bobervation+and+transition+model%2C+encoder+%7D" alt="a_{1:T}, o_{1:T}\to \text{SGD on variational bound} \to \text{obervation and transition model, encoder }" eeimg="1"/> </p><p><img src="https://www.zhihu.com/equation?tex=a_%7B1%3AT%7D%2C+o_%7B1%3AT%7D%5Cto+%5Ctext%7Bencoder%7D+%5Cto+s_%7B1%3AT%7D+%28%2Br_%7B1%3AT%7D%29+%5Cto+%5Ctext%7Breward+model%7D" alt="a_{1:T}, o_{1:T}\to \text{encoder} \to s_{1:T} (+r_{1:T}) \to \text{reward model}" eeimg="1"/> </p><p>整体算法如下</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-9fa50840edc1eb38d064bcce501f8ccf_b.jpg" data-caption="" data-size="normal" data-rawwidth="591" data-rawheight="868" class="origin_image zh-lightbox-thumb" width="591" data-original="https://pic4.zhimg.com/v2-9fa50840edc1eb38d064bcce501f8ccf_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;868&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="591" data-rawheight="868" class="origin_image zh-lightbox-thumb lazy" width="591" data-original="https://pic4.zhimg.com/v2-9fa50840edc1eb38d064bcce501f8ccf_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-9fa50840edc1eb38d064bcce501f8ccf_b.jpg"/></figure><p>其中第4~7行表示的是模型训练的部分，第11行表示的是模型的规划部分，第8~16行表示的模型的规划连同行动和采样的部分。</p><p><b>2. 如何利用模型来做规划？</b></p><p>先假设我们前面提到的各个模型的参数已经学习到了，那么如何来利用这些模型来做规划呢？这里使用了一个最为常用的框架，叫做model-predictive control（MPC）。简单说来就是每一步都往后面模拟很多步，然后选取一个收益最高的方案，然后只采取一步的行动；等到下一步的时候再重新规划并且选取新的一步；即并不是规划出一串行动之后开环地做着一串，而是规划一串只走一步。</p><p>回顾刚刚提到的流程框架</p><p><img src="https://www.zhihu.com/equation?tex=o_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D+%5Cto+%5Ctext%7Bencoder%7D+%5Cto+s_t+%5Cto+%5Ctext%7Btransition+model%7D+%5Cto+s_%7Bt%2B1%7D%2C+s_%7Bt%2B2%7D%2C+%5Ccdots+%5Cto+%5Ctext%7Breward+model%7D+%5Cto+R+%5Cto+%5Ctext%7BCEM%7D+%5Cto+a_%7Bt%7D" alt="o_{\le t}, a_{&lt; t} \to \text{encoder} \to s_t \to \text{transition model} \to s_{t+1}, s_{t+2}, \cdots \to \text{reward model} \to R \to \text{CEM} \to a_{t}" eeimg="1"/></p><p>我们先基于先前的观察使用encoder得到当前状态的估计，然后对于未来的行动序列进行采样，并结合transition model来预测未来的状态序列，同时使用reward model来估计获得奖励的期望值。这样对于一个给定的未来行动序列就能够得到一个期望收益了，把这一套当做一个黑盒子我们就能利用CEM来规划出一个好的未来行动序列了。取这个行动序列的第一个行动作为下一步我们选择的行动即可。具体算法如下。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c4203f20ad6e3c777956051ee06ecc21_b.jpg" data-caption="" data-size="normal" data-rawwidth="1148" data-rawheight="587" class="origin_image zh-lightbox-thumb" width="1148" data-original="https://pic2.zhimg.com/v2-c4203f20ad6e3c777956051ee06ecc21_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1148&#39; height=&#39;587&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1148" data-rawheight="587" class="origin_image zh-lightbox-thumb lazy" width="1148" data-original="https://pic2.zhimg.com/v2-c4203f20ad6e3c777956051ee06ecc21_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c4203f20ad6e3c777956051ee06ecc21_b.jpg"/></figure><p><b>3. 如何学习模型？</b></p><p>通过采样得到的数据是观察到的图像 <img src="https://www.zhihu.com/equation?tex=o_%7B1%3AT%7D" alt="o_{1:T}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=a_%7B1%3AT%7D" alt="a_{1:T}" eeimg="1"/> ，因此我们需要同时训练多个模型使得出现该数据的概率最大（maximum log-likelihood）。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-126f80a870d054819c0b6e7644e0fb4f_b.jpg" data-caption="" data-size="normal" data-rawwidth="1115" data-rawheight="438" class="origin_image zh-lightbox-thumb" width="1115" data-original="https://pic4.zhimg.com/v2-126f80a870d054819c0b6e7644e0fb4f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1115&#39; height=&#39;438&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1115" data-rawheight="438" class="origin_image zh-lightbox-thumb lazy" width="1115" data-original="https://pic4.zhimg.com/v2-126f80a870d054819c0b6e7644e0fb4f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-126f80a870d054819c0b6e7644e0fb4f_b.jpg"/></figure><p>这里完全是按照ELBO（evidence lower bound）的推导。注意，不等式的右边包含encoder <img src="https://www.zhihu.com/equation?tex=q%28s_t%7Co_%7B%5Cle+t%7D%2C+a_%7B%3Ct%7D%29" alt="q(s_t|o_{\le t}, a_{&lt;t})" eeimg="1"/> ，observation model <img src="https://www.zhihu.com/equation?tex=p%28o_t%7Cs_t%29" alt="p(o_t|s_t)" eeimg="1"/> 和transition model  <img src="https://www.zhihu.com/equation?tex=p%28s_t%7Cs_%7Bt-1%7D%2C+a_%7Bt-1%7D%29" alt="p(s_t|s_{t-1}, a_{t-1})" eeimg="1"/> 。通过对于右式做随机梯度上升即可学到这三个模型。</p><p>Reward model的学习就比较简单了，类似于普通的有监督学习。</p><p>至此，大致的框架已经讲完了，但是本文的创新点是提出了以下两种技术，使得planning过程中状态的估计更为准确。</p><p><b>4. Recurrent State Space Model （RSSM）</b></p><p>前面我们提到状态的转移模型 transition model 是表示为 <img src="https://www.zhihu.com/equation?tex=s_t+%5Csim+p%28s_t%7Cs_%7Bt-1%7D%2C+a_%7Bt-1%7D%29" alt="s_t \sim p(s_t|s_{t-1}, a_{t-1})" eeimg="1"/> ，文中都是使用的diagonal Gaussian distribution，即由神经网络生成下一步状态的均值和方差，并从高斯模型中进行采样。如果每一个单步的transition model都是完美的，那么逐步估计也会比较准确，但是单步的模型并不完美，它受限于不充足的训练以及较弱的模型表示能力。每一步都完全是随机采样，在经过很多步之后，前面信息就会丢失掉。因此，这里作者还引入了确定性（非随机）的部分。（当然了，如果神经网络自己学出来某一部分的方差就为0，那么就等效于加上确定性部分了，不过不能期望神经网络能够自己学出来）</p><p>通过在状态隐变量中加上确定性的部分，就得到了RSSM。transition model变为了 <img src="https://www.zhihu.com/equation?tex=h_t+%3D+f%28h_%7Bt-1%7D%2C+s_%7Bt-1%7D%2C+a_%7Bt-1%7D%29%2C+s_t%5Csim+p%28s_t%7Ch_t%29" alt="h_t = f(h_{t-1}, s_{t-1}, a_{t-1}), s_t\sim p(s_t|h_t)" eeimg="1"/> ，即原来的 <img src="https://www.zhihu.com/equation?tex=s_t" alt="s_t" eeimg="1"/> 拆成了 <img src="https://www.zhihu.com/equation?tex=%28h_t%2C+s_t%29" alt="(h_t, s_t)" eeimg="1"/> 两个部分。相应地，encoder也进行了一些改变， <img src="https://www.zhihu.com/equation?tex=q%28s_%7B1%3AT%7D%7Co_%7B1%3AT%7D%2C+a_%7B1%3AT%7D%29%3D%5Cprod_t+q%28s_t%7Co_%7B%5Cle+t%7D%2C+a_%7B%3Ct%7D%29+%5Cto+q%28s_%7B1%3AT%7D%7Co_%7B1%3AT%7D%2C+a_%7B1%3AT%7D%29%3D%5Cprod_t+q%28s_t%7Ch_t%2C+o_t%29+%3D+%5Cprod_t+q%28s_t%7Cf%28h_%7Bt-1%7D%2C+s_%7Bt-1%7D%2C+a_%7Bt-1%7D%29%2C+o_t%29" alt="q(s_{1:T}|o_{1:T}, a_{1:T})=\prod_t q(s_t|o_{\le t}, a_{&lt;t}) \to q(s_{1:T}|o_{1:T}, a_{1:T})=\prod_t q(s_t|h_t, o_t) = \prod_t q(s_t|f(h_{t-1}, s_{t-1}, a_{t-1}), o_t)" eeimg="1"/> 。</p><p>这样的改变只是改变了网络的结构，训练的目标和方式没有大的变化。</p><p><b>5. Latent Overshooting</b></p><p>前面推导到的下界（lower bound）是优化的目标，transition model的学习主要是和encoder <img src="https://www.zhihu.com/equation?tex=q%28s_t%7Co_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D%29" alt="q(s_t|o_{\le t}, a_{&lt; t})" eeimg="1"/> 的比较得来的，它们之间的比较是一步一步地比较。如下图所示，蓝色框中的虚线代表encoder一步步的推断，实线代表transition model的生成过程，波浪线代表需要对两者生成的 <img src="https://www.zhihu.com/equation?tex=s_2" alt="s_2" eeimg="1"/> 分布进行比较，并且最小化它们之间的KL散度。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-2b08252fa13471aff1339de9c42af684_b.jpg" data-size="normal" data-rawwidth="343" data-rawheight="285" class="content_image" width="343"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;343&#39; height=&#39;285&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="343" data-rawheight="285" class="content_image lazy" width="343" data-actualsrc="https://pic1.zhimg.com/v2-2b08252fa13471aff1339de9c42af684_b.jpg"/><figcaption>图：原变分下界对应的模型。虚线代表推断，实线代表生成过程，波浪线代表KL散度约束。</figcaption></figure><p>可以观察到，在这样的做法中，transition model始终是一步步分离开的，并没有像之后使用的时候那样多步串在一起去预测一个未来较长时间内的状态分布。少了多步串联起来的训练，得到的transition model在多步串联的时候会产生较大的误差，因此文章采用了以下latent overshooting的方法来改善。</p><p>Latent overshooting的做法如下图所示。以 <img src="https://www.zhihu.com/equation?tex=s_3" alt="s_3" eeimg="1"/> 为例，不仅仅是一步转移产生的状态 <img src="https://www.zhihu.com/equation?tex=s_%7B3%7C2%7D" alt="s_{3|2}" eeimg="1"/> 要和encoder产生的状态 <img src="https://www.zhihu.com/equation?tex=s_%7B3%7C3%7D" alt="s_{3|3}" eeimg="1"/> 作比较并减少差距，而且多步转移（这里是两步）产生的状态 <img src="https://www.zhihu.com/equation?tex=s_%7B3%7C1%7D" alt="s_{3|1}" eeimg="1"/> 也要和 <img src="https://www.zhihu.com/equation?tex=s_%7B3%7C3%7D" alt="s_{3|3}" eeimg="1"/> 作比较并最小化KL散度。这相当于不仅要求transition model在走一步的情况下要和encoder对的上，而且要求它在串联工作的时候也要和encoder对的上。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-242d0501c812fa0c84b7dbb23884090a_b.jpg" data-size="normal" data-rawwidth="319" data-rawheight="341" class="content_image" width="319"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;319&#39; height=&#39;341&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="319" data-rawheight="341" class="content_image lazy" width="319" data-actualsrc="https://pic3.zhimg.com/v2-242d0501c812fa0c84b7dbb23884090a_b.jpg"/><figcaption>图：Latent overshooting对应的模型。虚线代表推断，实线代表生成过程，波浪线代表KL散度约束。</figcaption></figure><p>考虑了latent overshooting之后的变分下界写作</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-4b38ff8a9bf83428ac15fd773d8ca72d_b.jpg" data-caption="" data-size="normal" data-rawwidth="536" data-rawheight="159" class="origin_image zh-lightbox-thumb" width="536" data-original="https://pic2.zhimg.com/v2-4b38ff8a9bf83428ac15fd773d8ca72d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;536&#39; height=&#39;159&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="536" data-rawheight="159" class="origin_image zh-lightbox-thumb lazy" width="536" data-original="https://pic2.zhimg.com/v2-4b38ff8a9bf83428ac15fd773d8ca72d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-4b38ff8a9bf83428ac15fd773d8ca72d_b.jpg"/></figure><p>其中， <img src="https://www.zhihu.com/equation?tex=D" alt="D" eeimg="1"/> 表示transition model最多串联的步数，可以看到，KL散度项被替换为了多对约束的加权平均，权重通过 <img src="https://www.zhihu.com/equation?tex=%5Cbeta_d" alt="\beta_d" eeimg="1"/> 来调节。</p><h2><b>实验结果</b></h2><p>这个工作跑了如下六组实验，这六组实验都有各自的特点，比如有些具有比较复杂的动力学（与环境有接触）、有些只能观察到部分的信息、有些具有稀疏的奖励结构。同时，该工作着重说明了RSSM和Latent overshooting在其中发挥的作用。</p><p>此外，该工作还训练了一个模型，使得该模型能够在所有的环境中运行。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8e709fe4de56fdc8dce20049a86109b4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1104" data-rawheight="708" class="origin_image zh-lightbox-thumb" width="1104" data-original="https://pic1.zhimg.com/v2-8e709fe4de56fdc8dce20049a86109b4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1104&#39; height=&#39;708&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1104" data-rawheight="708" class="origin_image zh-lightbox-thumb lazy" width="1104" data-original="https://pic1.zhimg.com/v2-8e709fe4de56fdc8dce20049a86109b4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8e709fe4de56fdc8dce20049a86109b4_b.jpg"/></figure><hr/><p>文章来源：导师推荐</p></div></div><div class="ContentItem-time">发布于 2019-02-23</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 25 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 25</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>1 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="58410683-1eb7-4d83-a16f-f552f6fa2dcf" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="58410683-1eb7-4d83-a16f-f552f6fa2dcf">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"57468070":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":57468070,"title":"【强化学习 40】PlaNet","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F57468070","imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9f2b42aa87a2d3b1d1939aa41e755dba_b.jpg","titleImage":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9f2b42aa87a2d3b1d1939aa41e755dba_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d6d8673c9102ea8f5cc158c46cf3276b_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"868\" data-watermark=\"watermark\" data-original-src=\"v2-d6d8673c9102ea8f5cc158c46cf3276b\" data-watermark-src=\"v2-9fa50840edc1eb38d064bcce501f8ccf\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d6d8673c9102ea8f5cc158c46cf3276b_r.png\"\u002F\u003EGoogle Brain和DeepMind合作的一个model-based强化学习算法——Deep Planning Network（PlaNet）。\u003Cb\u003E原文传送门\u003C\u002Fb\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1811.04551\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EHafner, Danijar, et al. &#34;Learning Latent Dynamics for Planning from Pixels.&#34; arXiv preprint arXiv:1811.04551 (2018).\u003C\u002Fa\u003E\u003Cb\u003E特色\u003C\u002Fb\u003E一种基于模型（mo…","created":1550924486,"updated":1550924486,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1646,"imageHeight":514,"content":"\u003Cp\u003EGoogle Brain和DeepMind合作的一个model-based强化学习算法——Deep Planning Network（PlaNet）。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E原文传送门\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1811.04551\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EHafner, Danijar, et al. &#34;Learning Latent Dynamics for Planning from Pixels.&#34; arXiv preprint arXiv:1811.04551 (2018).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E特色\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E一种基于模型（model-based）的强化学习算法，能够直接图像作为输入来控制机器人；规划（planning）的过程在隐空间（latent space）上进行，速度较快；采取了一些措施避免了长距离规划中状态估计不准确的问题。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E过程\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Col\u003E\u003Cli\u003E\u003Cb\u003E总体流程\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E既然是基于模型的方法，那么其两大问题主要就是\u003Cb\u003E\u003Cu\u003E\u003Ci\u003E如何学习模型\u003C\u002Fi\u003E\u003C\u002Fu\u003E\u003C\u002Fb\u003E和\u003Cb\u003E\u003Cu\u003E\u003Ci\u003E如何利用模型来做规划\u003C\u002Fi\u003E\u003C\u002Fu\u003E\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E首先，模型包含些什么？\u003C\u002Fp\u003E\u003Cp\u003E这里考虑一个基于图像的输入，这是一个高维度的输入，因此我们认为这是一个POMDP。图像输入是观测量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=o_t\" alt=\"o_t\" eeimg=\"1\"\u002F\u003E ，状态是一个隐变量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"\u002F\u003E 。基于模型的方法中的“模型”指的是transition model \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s_t%7Cs_%7Bt-1%7D%2C+a_%7Bt-1%7D%29\" alt=\"p(s_t|s_{t-1}, a_{t-1})\" eeimg=\"1\"\u002F\u003E 和 reward model \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28r_t%7Cs_t%29\" alt=\"p(r_t|s_t)\" eeimg=\"1\"\u002F\u003E ，POMDP还带来了observation model \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28o_t%7Cs_t%29\" alt=\"p(o_t|s_t)\" eeimg=\"1\"\u002F\u003E 。另外我们还需要一个encoder，快速地利用可观测量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=o_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D\" alt=\"o_{\\le t}, a_{&lt; t}\" eeimg=\"1\"\u002F\u003E 来估计隐变量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s_t%7Co_%7B%5Cle+t%7D%2Ca_%7B%3C+t%7D%29\" alt=\"p(s_t|o_{\\le t},a_{&lt; t})\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E总结一下，要学习以下四个模型的参数 transition model, reward model, observation model, encoder。\u003C\u002Fp\u003E\u003Cp\u003E规划的流程如下，使用学习到的模型来针对不同的行动预测收益，并选取最好的下一步的行动。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=o_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D+%5Cto+%5Ctext%7Bencoder%7D+%5Cto+s_t+%5Cto+%5Ctext%7Btransition+model%7D+%5Cto+s_%7Bt%2B1%7D%2C+s_%7Bt%2B2%7D%2C+%5Ccdots+%5Cto+%5Ctext%7Breward+model%7D+%5Cto+R+%5Cto+%5Ctext%7BCEM%7D+%5Cto+a_%7Bt%7D\" alt=\"o_{\\le t}, a_{&lt; t} \\to \\text{encoder} \\to s_t \\to \\text{transition model} \\to s_{t+1}, s_{t+2}, \\cdots \\to \\text{reward model} \\to R \\to \\text{CEM} \\to a_{t}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E模型学习的流程如下\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_%7B1%3AT%7D%2C+o_%7B1%3AT%7D%5Cto+%5Ctext%7BSGD+on+variational+bound%7D+%5Cto+%5Ctext%7Bobervation+and+transition+model%2C+encoder+%7D\" alt=\"a_{1:T}, o_{1:T}\\to \\text{SGD on variational bound} \\to \\text{obervation and transition model, encoder }\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_%7B1%3AT%7D%2C+o_%7B1%3AT%7D%5Cto+%5Ctext%7Bencoder%7D+%5Cto+s_%7B1%3AT%7D+%28%2Br_%7B1%3AT%7D%29+%5Cto+%5Ctext%7Breward+model%7D\" alt=\"a_{1:T}, o_{1:T}\\to \\text{encoder} \\to s_{1:T} (+r_{1:T}) \\to \\text{reward model}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E整体算法如下\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fa50840edc1eb38d064bcce501f8ccf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fa50840edc1eb38d064bcce501f8ccf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;591&#39; height=&#39;868&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fa50840edc1eb38d064bcce501f8ccf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fa50840edc1eb38d064bcce501f8ccf_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中第4~7行表示的是模型训练的部分，第11行表示的是模型的规划部分，第8~16行表示的模型的规划连同行动和采样的部分。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E2. 如何利用模型来做规划？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E先假设我们前面提到的各个模型的参数已经学习到了，那么如何来利用这些模型来做规划呢？这里使用了一个最为常用的框架，叫做model-predictive control（MPC）。简单说来就是每一步都往后面模拟很多步，然后选取一个收益最高的方案，然后只采取一步的行动；等到下一步的时候再重新规划并且选取新的一步；即并不是规划出一串行动之后开环地做着一串，而是规划一串只走一步。\u003C\u002Fp\u003E\u003Cp\u003E回顾刚刚提到的流程框架\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=o_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D+%5Cto+%5Ctext%7Bencoder%7D+%5Cto+s_t+%5Cto+%5Ctext%7Btransition+model%7D+%5Cto+s_%7Bt%2B1%7D%2C+s_%7Bt%2B2%7D%2C+%5Ccdots+%5Cto+%5Ctext%7Breward+model%7D+%5Cto+R+%5Cto+%5Ctext%7BCEM%7D+%5Cto+a_%7Bt%7D\" alt=\"o_{\\le t}, a_{&lt; t} \\to \\text{encoder} \\to s_t \\to \\text{transition model} \\to s_{t+1}, s_{t+2}, \\cdots \\to \\text{reward model} \\to R \\to \\text{CEM} \\to a_{t}\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E我们先基于先前的观察使用encoder得到当前状态的估计，然后对于未来的行动序列进行采样，并结合transition model来预测未来的状态序列，同时使用reward model来估计获得奖励的期望值。这样对于一个给定的未来行动序列就能够得到一个期望收益了，把这一套当做一个黑盒子我们就能利用CEM来规划出一个好的未来行动序列了。取这个行动序列的第一个行动作为下一步我们选择的行动即可。具体算法如下。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4203f20ad6e3c777956051ee06ecc21_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1148\" data-rawheight=\"587\" class=\"origin_image zh-lightbox-thumb\" width=\"1148\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4203f20ad6e3c777956051ee06ecc21_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1148&#39; height=&#39;587&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1148\" data-rawheight=\"587\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1148\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4203f20ad6e3c777956051ee06ecc21_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c4203f20ad6e3c777956051ee06ecc21_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E3. 如何学习模型？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E通过采样得到的数据是观察到的图像 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=o_%7B1%3AT%7D\" alt=\"o_{1:T}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_%7B1%3AT%7D\" alt=\"a_{1:T}\" eeimg=\"1\"\u002F\u003E ，因此我们需要同时训练多个模型使得出现该数据的概率最大（maximum log-likelihood）。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-126f80a870d054819c0b6e7644e0fb4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1115\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb\" width=\"1115\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-126f80a870d054819c0b6e7644e0fb4f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1115&#39; height=&#39;438&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1115\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1115\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-126f80a870d054819c0b6e7644e0fb4f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-126f80a870d054819c0b6e7644e0fb4f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这里完全是按照ELBO（evidence lower bound）的推导。注意，不等式的右边包含encoder \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=q%28s_t%7Co_%7B%5Cle+t%7D%2C+a_%7B%3Ct%7D%29\" alt=\"q(s_t|o_{\\le t}, a_{&lt;t})\" eeimg=\"1\"\u002F\u003E ，observation model \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28o_t%7Cs_t%29\" alt=\"p(o_t|s_t)\" eeimg=\"1\"\u002F\u003E 和transition model  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s_t%7Cs_%7Bt-1%7D%2C+a_%7Bt-1%7D%29\" alt=\"p(s_t|s_{t-1}, a_{t-1})\" eeimg=\"1\"\u002F\u003E 。通过对于右式做随机梯度上升即可学到这三个模型。\u003C\u002Fp\u003E\u003Cp\u003EReward model的学习就比较简单了，类似于普通的有监督学习。\u003C\u002Fp\u003E\u003Cp\u003E至此，大致的框架已经讲完了，但是本文的创新点是提出了以下两种技术，使得planning过程中状态的估计更为准确。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E4. Recurrent State Space Model （RSSM）\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E前面我们提到状态的转移模型 transition model 是表示为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t+%5Csim+p%28s_t%7Cs_%7Bt-1%7D%2C+a_%7Bt-1%7D%29\" alt=\"s_t \\sim p(s_t|s_{t-1}, a_{t-1})\" eeimg=\"1\"\u002F\u003E ，文中都是使用的diagonal Gaussian distribution，即由神经网络生成下一步状态的均值和方差，并从高斯模型中进行采样。如果每一个单步的transition model都是完美的，那么逐步估计也会比较准确，但是单步的模型并不完美，它受限于不充足的训练以及较弱的模型表示能力。每一步都完全是随机采样，在经过很多步之后，前面信息就会丢失掉。因此，这里作者还引入了确定性（非随机）的部分。（当然了，如果神经网络自己学出来某一部分的方差就为0，那么就等效于加上确定性部分了，不过不能期望神经网络能够自己学出来）\u003C\u002Fp\u003E\u003Cp\u003E通过在状态隐变量中加上确定性的部分，就得到了RSSM。transition model变为了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h_t+%3D+f%28h_%7Bt-1%7D%2C+s_%7Bt-1%7D%2C+a_%7Bt-1%7D%29%2C+s_t%5Csim+p%28s_t%7Ch_t%29\" alt=\"h_t = f(h_{t-1}, s_{t-1}, a_{t-1}), s_t\\sim p(s_t|h_t)\" eeimg=\"1\"\u002F\u003E ，即原来的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"\u002F\u003E 拆成了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28h_t%2C+s_t%29\" alt=\"(h_t, s_t)\" eeimg=\"1\"\u002F\u003E 两个部分。相应地，encoder也进行了一些改变， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=q%28s_%7B1%3AT%7D%7Co_%7B1%3AT%7D%2C+a_%7B1%3AT%7D%29%3D%5Cprod_t+q%28s_t%7Co_%7B%5Cle+t%7D%2C+a_%7B%3Ct%7D%29+%5Cto+q%28s_%7B1%3AT%7D%7Co_%7B1%3AT%7D%2C+a_%7B1%3AT%7D%29%3D%5Cprod_t+q%28s_t%7Ch_t%2C+o_t%29+%3D+%5Cprod_t+q%28s_t%7Cf%28h_%7Bt-1%7D%2C+s_%7Bt-1%7D%2C+a_%7Bt-1%7D%29%2C+o_t%29\" alt=\"q(s_{1:T}|o_{1:T}, a_{1:T})=\\prod_t q(s_t|o_{\\le t}, a_{&lt;t}) \\to q(s_{1:T}|o_{1:T}, a_{1:T})=\\prod_t q(s_t|h_t, o_t) = \\prod_t q(s_t|f(h_{t-1}, s_{t-1}, a_{t-1}), o_t)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E这样的改变只是改变了网络的结构，训练的目标和方式没有大的变化。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E5. Latent Overshooting\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E前面推导到的下界（lower bound）是优化的目标，transition model的学习主要是和encoder \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=q%28s_t%7Co_%7B%5Cle+t%7D%2C+a_%7B%3C+t%7D%29\" alt=\"q(s_t|o_{\\le t}, a_{&lt; t})\" eeimg=\"1\"\u002F\u003E 的比较得来的，它们之间的比较是一步一步地比较。如下图所示，蓝色框中的虚线代表encoder一步步的推断，实线代表transition model的生成过程，波浪线代表需要对两者生成的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_2\" alt=\"s_2\" eeimg=\"1\"\u002F\u003E 分布进行比较，并且最小化它们之间的KL散度。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2b08252fa13471aff1339de9c42af684_b.jpg\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"285\" class=\"content_image\" width=\"343\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;343&#39; height=&#39;285&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"285\" class=\"content_image lazy\" width=\"343\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2b08252fa13471aff1339de9c42af684_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：原变分下界对应的模型。虚线代表推断，实线代表生成过程，波浪线代表KL散度约束。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以观察到，在这样的做法中，transition model始终是一步步分离开的，并没有像之后使用的时候那样多步串在一起去预测一个未来较长时间内的状态分布。少了多步串联起来的训练，得到的transition model在多步串联的时候会产生较大的误差，因此文章采用了以下latent overshooting的方法来改善。\u003C\u002Fp\u003E\u003Cp\u003ELatent overshooting的做法如下图所示。以 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_3\" alt=\"s_3\" eeimg=\"1\"\u002F\u003E 为例，不仅仅是一步转移产生的状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_%7B3%7C2%7D\" alt=\"s_{3|2}\" eeimg=\"1\"\u002F\u003E 要和encoder产生的状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_%7B3%7C3%7D\" alt=\"s_{3|3}\" eeimg=\"1\"\u002F\u003E 作比较并减少差距，而且多步转移（这里是两步）产生的状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_%7B3%7C1%7D\" alt=\"s_{3|1}\" eeimg=\"1\"\u002F\u003E 也要和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_%7B3%7C3%7D\" alt=\"s_{3|3}\" eeimg=\"1\"\u002F\u003E 作比较并最小化KL散度。这相当于不仅要求transition model在走一步的情况下要和encoder对的上，而且要求它在串联工作的时候也要和encoder对的上。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-242d0501c812fa0c84b7dbb23884090a_b.jpg\" data-size=\"normal\" data-rawwidth=\"319\" data-rawheight=\"341\" class=\"content_image\" width=\"319\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;319&#39; height=&#39;341&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"319\" data-rawheight=\"341\" class=\"content_image lazy\" width=\"319\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-242d0501c812fa0c84b7dbb23884090a_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：Latent overshooting对应的模型。虚线代表推断，实线代表生成过程，波浪线代表KL散度约束。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E考虑了latent overshooting之后的变分下界写作\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4b38ff8a9bf83428ac15fd773d8ca72d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"159\" class=\"origin_image zh-lightbox-thumb\" width=\"536\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4b38ff8a9bf83428ac15fd773d8ca72d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;536&#39; height=&#39;159&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"159\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"536\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4b38ff8a9bf83428ac15fd773d8ca72d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4b38ff8a9bf83428ac15fd773d8ca72d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=D\" alt=\"D\" eeimg=\"1\"\u002F\u003E 表示transition model最多串联的步数，可以看到，KL散度项被替换为了多对约束的加权平均，权重通过 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta_d\" alt=\"\\beta_d\" eeimg=\"1\"\u002F\u003E 来调节。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E实验结果\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E这个工作跑了如下六组实验，这六组实验都有各自的特点，比如有些具有比较复杂的动力学（与环境有接触）、有些只能观察到部分的信息、有些具有稀疏的奖励结构。同时，该工作着重说明了RSSM和Latent overshooting在其中发挥的作用。\u003C\u002Fp\u003E\u003Cp\u003E此外，该工作还训练了一个模型，使得该模型能够在所有的环境中运行。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e709fe4de56fdc8dce20049a86109b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb\" width=\"1104\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e709fe4de56fdc8dce20049a86109b4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1104&#39; height=&#39;708&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1104\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e709fe4de56fdc8dce20049a86109b4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8e709fe4de56fdc8dce20049a86109b4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Chr\u002F\u003E\u003Cp\u003E文章来源：导师推荐\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":25,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":1,"contributions":[{"id":20297230,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 40】PlaNet - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F57468070 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_bullet_gui-1","expPrefix":"vd_bullet_gui","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F57468070","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F57468070","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>