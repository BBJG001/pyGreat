<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 68】DeepMDP - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="DeepMDP可以看做是对于原来MDP的一个抽象。原文传送门Gelada, Carles, et al. &amp;#34;DeepMDP: Learning Continuous Latent Space Models for Representation Learning.&amp;#34; International Conference on Machine L…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 68】DeepMDP"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/68364144"/><meta data-react-helmet="true" property="og:description" content="DeepMDP可以看做是对于原来MDP的一个抽象。原文传送门Gelada, Carles, et al. &amp;#34;DeepMDP: Learning Continuous Latent Space Models for Representation Learning.&amp;#34; International Conference on Machine L…"/><meta data-react-helmet="true" property="og:image" content="https://pic2.zhimg.com/v2-fe0d56511c9c30d0718b512d07123ecc_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:68364144,&quot;title&quot;:&quot;【强化学习 68】DeepMDP&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic2.zhimg.com/v2-fe0d56511c9c30d0718b512d07123ecc_1200x500.jpg" alt="【强化学习 68】DeepMDP"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 68】DeepMDP</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">21 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>DeepMDP可以看做是对于原来MDP的一个抽象。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v97/gelada19a/gelada19a.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Gelada, Carles, et al. &#34;DeepMDP: Learning Continuous Latent Space Models for Representation Learning.&#34; International Conference on Machine Learning. 2019.</a></p><h2>特色</h2><p>前面讲了很多state abstraction的东西，看到了state abstraction有很多很好的性质，但是一直都没有讲如何去得到这样一个state abstraction <img src="https://www.zhihu.com/equation?tex=%5Cphi%3A%5Cmathcal%7BS%7D+%5Cto+%5Cphi%28%5Cmathcal%7BS%7D%29" alt="\phi:\mathcal{S} \to \phi(\mathcal{S})" eeimg="1"/> ，实际上这也是一个比较困难的问题。这篇文章讲了一个practical的学习得到state abstraction的方法，并且理论上说明了它和bisumulation的联系（参见本专栏<a href="https://zhuanlan.zhihu.com/p/67027334" class="internal">【强化学习理论 63】StatisticalRL 7</a>）。</p><p>另外，之前去俞扬老师组转了转，他们做的MindGame[1]相当于是人工做了一个这样的state abstraction，感觉也很有意思，不过还没来得及看。这里的DeepMDP相当于是自动学习state abstraction，不过实验效果上来说目前还是MindGame的做法见效更快。</p><h2>过程</h2><h3>1. 目标</h3><p>本文目标是希望把当前比较复杂的状态空间投影到一个比较低维度的连续状态空间上，即 <img src="https://www.zhihu.com/equation?tex=%5Cphi%3A%5Cmathcal%7BS%7D%5Cto%5Cbar%7BS%7D" alt="\phi:\mathcal{S}\to\bar{S}" eeimg="1"/> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7BS%7D+%5Csubset+%5Cmathbb%7BR%7D%5ED" alt="\bar{S} \subset \mathbb{R}^D" eeimg="1"/> 。对于一个MDP <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BM%7D+%3D+%5C%7B%5Cmathcal%7BS%2CA%2CR%2C+P%7D%2C+%5Cgamma%5C%7D" alt="\mathcal{M} = \{\mathcal{S,A,R, P}, \gamma\}" eeimg="1"/> ，定义 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7B%5Cbar%7BM%7D%7D+%3D+%5C%7B%5Cmathcal%7B%5Cbar%7BS%7D%2CA%2C%5Cbar%7BR%7D%2C+%5Cbar%7BP%7D%7D%2C+%5Cgamma%5C%7D" alt="\mathcal{\bar{M}} = \{\mathcal{\bar{S},A,\bar{R}, \bar{P}}, \gamma\}" eeimg="1"/> 为与 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 相关的一个MDP。可以把 <img src="https://www.zhihu.com/equation?tex=%28%5Cbar%7B%5Cmathcal%7BM%7D%7D%2C+%5Cphi%29" alt="(\bar{\mathcal{M}}, \phi)" eeimg="1"/> 称作 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BM%7D" alt="\mathcal{M}" eeimg="1"/> 的隐空间模型（latent space model）。如果用一个参数化的模型（比如神经网络）来拟合这个隐空间模型，可以使用三个神经网络分别来代表 <img src="https://www.zhihu.com/equation?tex=%5Cphi%2C+%5Cmathcal%7B%5Cbar%7BR%7D%7D%2C+%5Cmathcal%7B%5Cbar%7BP%7D%7D" alt="\phi, \mathcal{\bar{R}}, \mathcal{\bar{P}}" eeimg="1"/> 。文章称这样的 <img src="https://www.zhihu.com/equation?tex=%28%5Cbar%7B%5Cmathcal%7BM%7D%7D%2C+%5Cphi%29" alt="(\bar{\mathcal{M}}, \phi)" eeimg="1"/> 为DeepMDP。</p><p>本文的目标是设计参数化的模型来表示DeepMDP，同时设计相应的损失函数；同时本文分析了在DeepMDP相比于原MDP有多大损失（注意到DeepMDP通常维度更低，学习起来更容易，但是一般带来一些approximation error）。</p><h3>2. 做法</h3><p>DeepMDP的做法很直接，就是在采集到的样本上来最小化以下两项损失函数</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_b.png" data-caption="" data-size="normal" data-rawwidth="874" data-rawheight="119" class="origin_image zh-lightbox-thumb" width="874" data-original="https://pic2.zhimg.com/v2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;874&#39; height=&#39;119&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="874" data-rawheight="119" class="origin_image zh-lightbox-thumb lazy" width="874" data-original="https://pic2.zhimg.com/v2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_b.png"/></figure><p>其中，可以把 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"/> 看做是采集到样本的分布，W表示Wasserstein距离。其训练的参数为 <img src="https://www.zhihu.com/equation?tex=%5Cphi%2C+%5Cmathcal%7B%5Cbar%7BR%7D%7D%2C+%5Cmathcal%7B%5Cbar%7BP%7D%7D" alt="\phi, \mathcal{\bar{R}}, \mathcal{\bar{P}}" eeimg="1"/> 三个神经网络的网络参数。</p><p>当然实际中Wasserstein距离很难计算，因此本文就使用了确定性的 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Cmathcal%7BP%7D%7D" alt="\bar{\mathcal{P}}" eeimg="1"/> 模型，这样Wasserstein距离就退化为了L2 loss。同时，后面的理论部分要求 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7B%5Cbar%7BR%7D%7D%2C+%5Cmathcal%7B%5Cbar%7BP%7D%7D" alt="\mathcal{\bar{R}}, \mathcal{\bar{P}}" eeimg="1"/> 需要满足Lipschitz条件，因此使用了gradient penalty来限制训练得到的神经网络满足该条件。</p><h3>3. 定义</h3><p>下面希望进一步说明最小化以上两项损失函数得到的DeepMDP性质较好，即DeepMDP上的价值函数、最优策略，相比于原MDP损失不是太大。直观来说，上面两项损失函数其实是要求了DeepMDP和原MDP在reward和dynamics上都相差不大，但即使这样，如果原MDP或者最优策略很不规则也可能导致DeepMDP和原MDP差距较大，因此我们需要要求MDP和策略都比较“平滑”。对于一个状态空间上的 metric <img src="https://www.zhihu.com/equation?tex=d_%7B%5Cmathcal%7BS%7D%7D" alt="d_{\mathcal{S}}" eeimg="1"/> ，定义 “平滑”的 MDP 和“平滑”的策略。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-31d006f3c19c42185cd9d99d4218db5a_b.jpg" data-caption="" data-size="normal" data-rawwidth="880" data-rawheight="239" class="origin_image zh-lightbox-thumb" width="880" data-original="https://pic3.zhimg.com/v2-31d006f3c19c42185cd9d99d4218db5a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;880&#39; height=&#39;239&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="880" data-rawheight="239" class="origin_image zh-lightbox-thumb lazy" width="880" data-original="https://pic3.zhimg.com/v2-31d006f3c19c42185cd9d99d4218db5a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-31d006f3c19c42185cd9d99d4218db5a_b.jpg"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-9b88e9b36a8822e1661b492ec7debc34_b.jpg" data-caption="" data-size="normal" data-rawwidth="858" data-rawheight="183" class="origin_image zh-lightbox-thumb" width="858" data-original="https://pic1.zhimg.com/v2-9b88e9b36a8822e1661b492ec7debc34_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;858&#39; height=&#39;183&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="858" data-rawheight="183" class="origin_image zh-lightbox-thumb lazy" width="858" data-original="https://pic1.zhimg.com/v2-9b88e9b36a8822e1661b492ec7debc34_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-9b88e9b36a8822e1661b492ec7debc34_b.jpg"/></figure><p>在 Lipschitz MDP 上的策略也是 Lipschitz 的，即以上两个定义之间存在关系</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-0f391633d14357547e04f68918005a07_b.jpg" data-caption="" data-size="normal" data-rawwidth="1590" data-rawheight="352" class="origin_image zh-lightbox-thumb" width="1590" data-original="https://pic4.zhimg.com/v2-0f391633d14357547e04f68918005a07_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1590" data-rawheight="352" class="origin_image zh-lightbox-thumb lazy" width="1590" data-original="https://pic4.zhimg.com/v2-0f391633d14357547e04f68918005a07_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-0f391633d14357547e04f68918005a07_b.jpg"/></figure><p>（我没太想明白，能找到一个反例使得这个 property 不成立？）</p><p>由此可以推论，MDP 上的最优策略也是 Lipschitz 的。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-57b023b4b780d1d84e69034cb9e3c8bb_b.png" data-caption="" data-size="normal" data-rawwidth="1528" data-rawheight="120" class="origin_image zh-lightbox-thumb" width="1528" data-original="https://pic4.zhimg.com/v2-57b023b4b780d1d84e69034cb9e3c8bb_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1528&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1528" data-rawheight="120" class="origin_image zh-lightbox-thumb lazy" width="1528" data-original="https://pic4.zhimg.com/v2-57b023b4b780d1d84e69034cb9e3c8bb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-57b023b4b780d1d84e69034cb9e3c8bb_b.png"/></figure><h3>4. Global DeepMDP bound</h3><p>这里讲的 Global 的意思是， DeepMDP 和原 MDP 之间在所有的 state-action space 上 reward 和 dynamics 都相差不多时，考虑从原 MDP 到 DeepMDP 的差距。</p><p>即给定</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-d6b8ff60227ab0cd1e6727f9f4a665a1_b.png" data-caption="" data-size="normal" data-rawwidth="1576" data-rawheight="218" class="origin_image zh-lightbox-thumb" width="1576" data-original="https://pic2.zhimg.com/v2-d6b8ff60227ab0cd1e6727f9f4a665a1_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1576&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1576" data-rawheight="218" class="origin_image zh-lightbox-thumb lazy" width="1576" data-original="https://pic2.zhimg.com/v2-d6b8ff60227ab0cd1e6727f9f4a665a1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-d6b8ff60227ab0cd1e6727f9f4a665a1_b.png"/></figure><p><b><i>DeepMDP 和原 MDP 价值函数的差距不太大</i></b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-c19d3de8fa952bfe6fc30e980fc799d7_b.png" data-caption="" data-size="normal" data-rawwidth="1600" data-rawheight="128" class="origin_image zh-lightbox-thumb" width="1600" data-original="https://pic4.zhimg.com/v2-c19d3de8fa952bfe6fc30e980fc799d7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1600" data-rawheight="128" class="origin_image zh-lightbox-thumb lazy" width="1600" data-original="https://pic4.zhimg.com/v2-c19d3de8fa952bfe6fc30e980fc799d7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-c19d3de8fa952bfe6fc30e980fc799d7_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-a83217c36bd009c3a41973468e63102d_b.jpg" data-caption="" data-size="normal" data-rawwidth="1564" data-rawheight="300" class="origin_image zh-lightbox-thumb" width="1564" data-original="https://pic2.zhimg.com/v2-a83217c36bd009c3a41973468e63102d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1564&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1564" data-rawheight="300" class="origin_image zh-lightbox-thumb lazy" width="1564" data-original="https://pic2.zhimg.com/v2-a83217c36bd009c3a41973468e63102d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-a83217c36bd009c3a41973468e63102d_b.jpg"/></figure><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5CPi%7D" alt="\bar{\Pi}" eeimg="1"/> 表示 DeepMDP 中的所有策略，它是原 MDP 中所有策略的子集。</p><p><b><i>满足约束的状态表示（abstraction/representation）下距离相近的状态价值函数也相近</i></b></p><p>这里想研究把满足(5)和(6)约束的 representation <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 做为一个 state abstraction 的质量如何。有一种比较显然的 representation 失败的情况：两个状态价值函数很不一样的状态，在representation <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 的作用下，映射到同一个状态。这里想说明的是，这样的情况不会发生，即representation <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 作用下距离较近的状态，其价值函数相差也不大。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-f14855d72887fd6d911d96e0194f99b9_b.jpg" data-caption="" data-size="normal" data-rawwidth="1616" data-rawheight="584" class="origin_image zh-lightbox-thumb" width="1616" data-original="https://pic2.zhimg.com/v2-f14855d72887fd6d911d96e0194f99b9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;584&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1616" data-rawheight="584" class="origin_image zh-lightbox-thumb lazy" width="1616" data-original="https://pic2.zhimg.com/v2-f14855d72887fd6d911d96e0194f99b9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-f14855d72887fd6d911d96e0194f99b9_b.jpg"/></figure><p><b><i>DeepMDP 下的最优策略在原 MDP 中也不会太差</i></b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-4204a2dedce58d2a55c16044a02e681b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1562" data-rawheight="478" class="origin_image zh-lightbox-thumb" width="1562" data-original="https://pic4.zhimg.com/v2-4204a2dedce58d2a55c16044a02e681b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1562&#39; height=&#39;478&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1562" data-rawheight="478" class="origin_image zh-lightbox-thumb lazy" width="1562" data-original="https://pic4.zhimg.com/v2-4204a2dedce58d2a55c16044a02e681b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4204a2dedce58d2a55c16044a02e681b_b.jpg"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-da0fa0e95887dbf0b2bb1d233073b805_b.png" data-caption="" data-size="normal" data-rawwidth="1616" data-rawheight="250" class="origin_image zh-lightbox-thumb" width="1616" data-original="https://pic2.zhimg.com/v2-da0fa0e95887dbf0b2bb1d233073b805_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1616" data-rawheight="250" class="origin_image zh-lightbox-thumb lazy" width="1616" data-original="https://pic2.zhimg.com/v2-da0fa0e95887dbf0b2bb1d233073b805_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-da0fa0e95887dbf0b2bb1d233073b805_b.png"/></figure><h3>5. Local DeepMDP bounds</h3><p>前面讲的 global bound 的要求是对于所有的 state-action space，DeepMDP 和原 MDP 差距都严格地不超过规定的数值。在实际中都是在样本上最小化误差的，没法保证全局的 bound，因此这里考虑一个更为实际的情况，即在样本上 DeepMDP 和原 MDP 差距不太大时，相应的 bound。</p><p>这里要求在样本上 DeepMDP 和原 MDP 的 reward 和 dynamics 相差不大。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-8c1b5e5e1f1d5bcecdfcee5875969721_b.png" data-caption="" data-size="normal" data-rawwidth="1662" data-rawheight="230" class="origin_image zh-lightbox-thumb" width="1662" data-original="https://pic2.zhimg.com/v2-8c1b5e5e1f1d5bcecdfcee5875969721_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1662&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1662" data-rawheight="230" class="origin_image zh-lightbox-thumb lazy" width="1662" data-original="https://pic2.zhimg.com/v2-8c1b5e5e1f1d5bcecdfcee5875969721_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-8c1b5e5e1f1d5bcecdfcee5875969721_b.png"/></figure><p><b><i>DeepMDP 和原 MDP 价值函数的差距不太大</i></b></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c47ba25116a50f75e14a6fd8eba1fbbd_b.jpg" data-caption="" data-size="normal" data-rawwidth="1616" data-rawheight="598" class="origin_image zh-lightbox-thumb" width="1616" data-original="https://pic2.zhimg.com/v2-c47ba25116a50f75e14a6fd8eba1fbbd_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;598&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1616" data-rawheight="598" class="origin_image zh-lightbox-thumb lazy" width="1616" data-original="https://pic2.zhimg.com/v2-c47ba25116a50f75e14a6fd8eba1fbbd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c47ba25116a50f75e14a6fd8eba1fbbd_b.jpg"/></figure><p><b><i>满足约束的状态表示（abstraction/representation）下距离相近的状态价值函数也相近</i></b></p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-f8d834b66d0273dd7d3b21ac73305876_b.jpg" data-caption="" data-size="normal" data-rawwidth="1550" data-rawheight="684" class="origin_image zh-lightbox-thumb" width="1550" data-original="https://pic3.zhimg.com/v2-f8d834b66d0273dd7d3b21ac73305876_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1550&#39; height=&#39;684&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1550" data-rawheight="684" class="origin_image zh-lightbox-thumb lazy" width="1550" data-original="https://pic3.zhimg.com/v2-f8d834b66d0273dd7d3b21ac73305876_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-f8d834b66d0273dd7d3b21ac73305876_b.jpg"/></figure><p>这说明，对于策略访问较频繁的状态，如果其表示比较相近，那么其价值函数也差不太多；但是对于样本较少的区域，其表示就不是特别准确了。</p><h3>6. 与 bisimulation 的联系</h3><p>Bisimulation 对应的映射 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 可以最大程度地压缩状态空间，可以证明在压缩之后的 MDP 上找到的最优策略性能和原 MDP 上最优策略性能差不多。这样，策略空间实际上可以被大大缩小到 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7B%5CPi%7D" alt="\tilde{\Pi}" eeimg="1"/> （注意这上面是一弯，不是一横），从而提高了找到一个好的策略的效率；具体说来，这里的策略空间就不允许在被 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 映射到同一个状态下的两个状态下采取不同的行动；如果是近似的 bisimulation，即不允许在被 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 映射到相似状态下的两个状态下采取非常不同的行动。这里想要证明的是对于任何 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7B%5CPi%7D" alt="\tilde{\Pi}" eeimg="1"/> 中的策略，都可以找到 DeepMDP 中的一个策略，使得这两个策略相差不多。</p><p>下面先定义 bisimulation</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-457c49036a8e9c26f104e6001e4d28f2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1560" data-rawheight="610" class="origin_image zh-lightbox-thumb" width="1560" data-original="https://pic3.zhimg.com/v2-457c49036a8e9c26f104e6001e4d28f2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1560&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1560" data-rawheight="610" class="origin_image zh-lightbox-thumb lazy" width="1560" data-original="https://pic3.zhimg.com/v2-457c49036a8e9c26f104e6001e4d28f2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-457c49036a8e9c26f104e6001e4d28f2_b.jpg"/></figure><p>bisimulation 只给出了两个状态等价不等价的定义，为了便于分析，我们还需要定义两个状态相似的程度。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-fd1b726b734ebe2352b54e5d56f183cc_b.jpg" data-caption="" data-size="normal" data-rawwidth="1538" data-rawheight="352" class="origin_image zh-lightbox-thumb" width="1538" data-original="https://pic1.zhimg.com/v2-fd1b726b734ebe2352b54e5d56f183cc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1538&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1538" data-rawheight="352" class="origin_image zh-lightbox-thumb lazy" width="1538" data-original="https://pic1.zhimg.com/v2-fd1b726b734ebe2352b54e5d56f183cc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-fd1b726b734ebe2352b54e5d56f183cc_b.jpg"/></figure><p>bisimulation 下比较容易学到的策略集合定义如下</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-87032e07d7f2536bce3b78b934d30a5d_b.png" data-caption="" data-size="normal" data-rawwidth="1612" data-rawheight="242" class="origin_image zh-lightbox-thumb" width="1612" data-original="https://pic2.zhimg.com/v2-87032e07d7f2536bce3b78b934d30a5d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1612" data-rawheight="242" class="origin_image zh-lightbox-thumb lazy" width="1612" data-original="https://pic2.zhimg.com/v2-87032e07d7f2536bce3b78b934d30a5d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-87032e07d7f2536bce3b78b934d30a5d_b.png"/></figure><p>一个理想的 state abstraction 产生的效果应该是对应的策略集合包括下面的策略集合（最优策略在此集合中），但是大小尽可能小。下面这个定理说明了这一点。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-e30e8708db303634f0adc4e268b9d186_b.jpg" data-caption="" data-size="normal" data-rawwidth="1644" data-rawheight="658" class="origin_image zh-lightbox-thumb" width="1644" data-original="https://pic3.zhimg.com/v2-e30e8708db303634f0adc4e268b9d186_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1644&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1644" data-rawheight="658" class="origin_image zh-lightbox-thumb lazy" width="1644" data-original="https://pic3.zhimg.com/v2-e30e8708db303634f0adc4e268b9d186_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-e30e8708db303634f0adc4e268b9d186_b.jpg"/></figure><h3>7. 实验</h3><p>文章实验效果还不错，在一个 demo 的环境中说明了该方法确实能够找到一个有意义的 state abstraction；用这种方法做辅助能够在 Atari 游戏上有性能提升。不过文章还是提到如果直接训练 DeepMDP 的三个神经网络，可能导致 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 网络什么都学不到，因为 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Cmathcal%7BP%7D%7D" alt="\bar{\mathcal{P}}" eeimg="1"/> 网络为了预测的更准，会促使 <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1"/> 网络生成没有信息量的状态抽象，即整体的训练过程有一定的冲突。</p><hr/><p>参考文献</p><p>[1] Liu, Ruo-Ze, et al. &#34;Efficient Reinforcement Learning with a Mind-Game for Full-Length StarCraft II.&#34;<i>arXiv preprint arXiv:1903.00715</i>(2019).</p><p></p></div></div><div class="ContentItem-time">发布于 2019-06-08</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 21 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 21</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>3 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="8da61cb0-2e71-42e1-b971-268490030e80" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="8da61cb0-2e71-42e1-b971-268490030e80">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"68364144":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":68364144,"title":"【强化学习 68】DeepMDP","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F68364144","imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-fe0d56511c9c30d0718b512d07123ecc_b.jpg","titleImage":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-fe0d56511c9c30d0718b512d07123ecc_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"119\" data-watermark=\"\" data-original-src=\"\" data-watermark-src=\"\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_r.png\"\u002F\u003EDeepMDP可以看做是对于原来MDP的一个抽象。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fproceedings.mlr.press\u002Fv97\u002Fgelada19a\u002Fgelada19a.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EGelada, Carles, et al. &#34;DeepMDP: Learning Continuous Latent Space Models for Representation Learning.&#34; International Conference on Machine Learning. 2019.\u003C\u002Fa\u003E特色前面讲了很多state abstraction…","created":1559925490,"updated":1559925490,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1356,"imageHeight":483,"content":"\u003Cp\u003EDeepMDP可以看做是对于原来MDP的一个抽象。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fproceedings.mlr.press\u002Fv97\u002Fgelada19a\u002Fgelada19a.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EGelada, Carles, et al. &#34;DeepMDP: Learning Continuous Latent Space Models for Representation Learning.&#34; International Conference on Machine Learning. 2019.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E前面讲了很多state abstraction的东西，看到了state abstraction有很多很好的性质，但是一直都没有讲如何去得到这样一个state abstraction \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%3A%5Cmathcal%7BS%7D+%5Cto+%5Cphi%28%5Cmathcal%7BS%7D%29\" alt=\"\\phi:\\mathcal{S} \\to \\phi(\\mathcal{S})\" eeimg=\"1\"\u002F\u003E ，实际上这也是一个比较困难的问题。这篇文章讲了一个practical的学习得到state abstraction的方法，并且理论上说明了它和bisumulation的联系（参见本专栏\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F67027334\" class=\"internal\"\u003E【强化学习理论 63】StatisticalRL 7\u003C\u002Fa\u003E）。\u003C\u002Fp\u003E\u003Cp\u003E另外，之前去俞扬老师组转了转，他们做的MindGame[1]相当于是人工做了一个这样的state abstraction，感觉也很有意思，不过还没来得及看。这里的DeepMDP相当于是自动学习state abstraction，不过实验效果上来说目前还是MindGame的做法见效更快。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1. 目标\u003C\u002Fh3\u003E\u003Cp\u003E本文目标是希望把当前比较复杂的状态空间投影到一个比较低维度的连续状态空间上，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%3A%5Cmathcal%7BS%7D%5Cto%5Cbar%7BS%7D\" alt=\"\\phi:\\mathcal{S}\\to\\bar{S}\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7BS%7D+%5Csubset+%5Cmathbb%7BR%7D%5ED\" alt=\"\\bar{S} \\subset \\mathbb{R}^D\" eeimg=\"1\"\u002F\u003E 。对于一个MDP \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BM%7D+%3D+%5C%7B%5Cmathcal%7BS%2CA%2CR%2C+P%7D%2C+%5Cgamma%5C%7D\" alt=\"\\mathcal{M} = \\{\\mathcal{S,A,R, P}, \\gamma\\}\" eeimg=\"1\"\u002F\u003E ，定义 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7B%5Cbar%7BM%7D%7D+%3D+%5C%7B%5Cmathcal%7B%5Cbar%7BS%7D%2CA%2C%5Cbar%7BR%7D%2C+%5Cbar%7BP%7D%7D%2C+%5Cgamma%5C%7D\" alt=\"\\mathcal{\\bar{M}} = \\{\\mathcal{\\bar{S},A,\\bar{R}, \\bar{P}}, \\gamma\\}\" eeimg=\"1\"\u002F\u003E 为与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 相关的一个MDP。可以把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Cbar%7B%5Cmathcal%7BM%7D%7D%2C+%5Cphi%29\" alt=\"(\\bar{\\mathcal{M}}, \\phi)\" eeimg=\"1\"\u002F\u003E 称作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BM%7D\" alt=\"\\mathcal{M}\" eeimg=\"1\"\u002F\u003E 的隐空间模型（latent space model）。如果用一个参数化的模型（比如神经网络）来拟合这个隐空间模型，可以使用三个神经网络分别来代表 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%2C+%5Cmathcal%7B%5Cbar%7BR%7D%7D%2C+%5Cmathcal%7B%5Cbar%7BP%7D%7D\" alt=\"\\phi, \\mathcal{\\bar{R}}, \\mathcal{\\bar{P}}\" eeimg=\"1\"\u002F\u003E 。文章称这样的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Cbar%7B%5Cmathcal%7BM%7D%7D%2C+%5Cphi%29\" alt=\"(\\bar{\\mathcal{M}}, \\phi)\" eeimg=\"1\"\u002F\u003E 为DeepMDP。\u003C\u002Fp\u003E\u003Cp\u003E本文的目标是设计参数化的模型来表示DeepMDP，同时设计相应的损失函数；同时本文分析了在DeepMDP相比于原MDP有多大损失（注意到DeepMDP通常维度更低，学习起来更容易，但是一般带来一些approximation error）。\u003C\u002Fp\u003E\u003Ch3\u003E2. 做法\u003C\u002Fh3\u003E\u003Cp\u003EDeepMDP的做法很直接，就是在采集到的样本上来最小化以下两项损失函数\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb\" width=\"874\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;874&#39; height=&#39;119&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"874\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ebeb5ca81b4e8582e3fbd0f13fdca0d9_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中，可以把 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cxi\" alt=\"\\xi\" eeimg=\"1\"\u002F\u003E 看做是采集到样本的分布，W表示Wasserstein距离。其训练的参数为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%2C+%5Cmathcal%7B%5Cbar%7BR%7D%7D%2C+%5Cmathcal%7B%5Cbar%7BP%7D%7D\" alt=\"\\phi, \\mathcal{\\bar{R}}, \\mathcal{\\bar{P}}\" eeimg=\"1\"\u002F\u003E 三个神经网络的网络参数。\u003C\u002Fp\u003E\u003Cp\u003E当然实际中Wasserstein距离很难计算，因此本文就使用了确定性的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Cmathcal%7BP%7D%7D\" alt=\"\\bar{\\mathcal{P}}\" eeimg=\"1\"\u002F\u003E 模型，这样Wasserstein距离就退化为了L2 loss。同时，后面的理论部分要求 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7B%5Cbar%7BR%7D%7D%2C+%5Cmathcal%7B%5Cbar%7BP%7D%7D\" alt=\"\\mathcal{\\bar{R}}, \\mathcal{\\bar{P}}\" eeimg=\"1\"\u002F\u003E 需要满足Lipschitz条件，因此使用了gradient penalty来限制训练得到的神经网络满足该条件。\u003C\u002Fp\u003E\u003Ch3\u003E3. 定义\u003C\u002Fh3\u003E\u003Cp\u003E下面希望进一步说明最小化以上两项损失函数得到的DeepMDP性质较好，即DeepMDP上的价值函数、最优策略，相比于原MDP损失不是太大。直观来说，上面两项损失函数其实是要求了DeepMDP和原MDP在reward和dynamics上都相差不大，但即使这样，如果原MDP或者最优策略很不规则也可能导致DeepMDP和原MDP差距较大，因此我们需要要求MDP和策略都比较“平滑”。对于一个状态空间上的 metric \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d_%7B%5Cmathcal%7BS%7D%7D\" alt=\"d_{\\mathcal{S}}\" eeimg=\"1\"\u002F\u003E ，定义 “平滑”的 MDP 和“平滑”的策略。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-31d006f3c19c42185cd9d99d4218db5a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb\" width=\"880\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-31d006f3c19c42185cd9d99d4218db5a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;880&#39; height=&#39;239&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"880\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-31d006f3c19c42185cd9d99d4218db5a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-31d006f3c19c42185cd9d99d4218db5a_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b88e9b36a8822e1661b492ec7debc34_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"858\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb\" width=\"858\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b88e9b36a8822e1661b492ec7debc34_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;858&#39; height=&#39;183&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"858\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"858\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b88e9b36a8822e1661b492ec7debc34_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b88e9b36a8822e1661b492ec7debc34_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在 Lipschitz MDP 上的策略也是 Lipschitz 的，即以上两个定义之间存在关系\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f391633d14357547e04f68918005a07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f391633d14357547e04f68918005a07_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1590&#39; height=&#39;352&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f391633d14357547e04f68918005a07_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f391633d14357547e04f68918005a07_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E（我没太想明白，能找到一个反例使得这个 property 不成立？）\u003C\u002Fp\u003E\u003Cp\u003E由此可以推论，MDP 上的最优策略也是 Lipschitz 的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57b023b4b780d1d84e69034cb9e3c8bb_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb\" width=\"1528\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57b023b4b780d1d84e69034cb9e3c8bb_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1528&#39; height=&#39;120&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1528\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57b023b4b780d1d84e69034cb9e3c8bb_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57b023b4b780d1d84e69034cb9e3c8bb_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E4. Global DeepMDP bound\u003C\u002Fh3\u003E\u003Cp\u003E这里讲的 Global 的意思是， DeepMDP 和原 MDP 之间在所有的 state-action space 上 reward 和 dynamics 都相差不多时，考虑从原 MDP 到 DeepMDP 的差距。\u003C\u002Fp\u003E\u003Cp\u003E即给定\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d6b8ff60227ab0cd1e6727f9f4a665a1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb\" width=\"1576\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d6b8ff60227ab0cd1e6727f9f4a665a1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1576&#39; height=&#39;218&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1576\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d6b8ff60227ab0cd1e6727f9f4a665a1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d6b8ff60227ab0cd1e6727f9f4a665a1_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EDeepMDP 和原 MDP 价值函数的差距不太大\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c19d3de8fa952bfe6fc30e980fc799d7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c19d3de8fa952bfe6fc30e980fc799d7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1600&#39; height=&#39;128&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c19d3de8fa952bfe6fc30e980fc799d7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c19d3de8fa952bfe6fc30e980fc799d7_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a83217c36bd009c3a41973468e63102d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"1564\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a83217c36bd009c3a41973468e63102d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1564&#39; height=&#39;300&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1564\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a83217c36bd009c3a41973468e63102d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a83217c36bd009c3a41973468e63102d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5CPi%7D\" alt=\"\\bar{\\Pi}\" eeimg=\"1\"\u002F\u003E 表示 DeepMDP 中的所有策略，它是原 MDP 中所有策略的子集。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E满足约束的状态表示（abstraction\u002Frepresentation）下距离相近的状态价值函数也相近\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这里想研究把满足(5)和(6)约束的 representation \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 做为一个 state abstraction 的质量如何。有一种比较显然的 representation 失败的情况：两个状态价值函数很不一样的状态，在representation \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 的作用下，映射到同一个状态。这里想说明的是，这样的情况不会发生，即representation \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 作用下距离较近的状态，其价值函数相差也不大。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f14855d72887fd6d911d96e0194f99b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f14855d72887fd6d911d96e0194f99b9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1616&#39; height=&#39;584&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f14855d72887fd6d911d96e0194f99b9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f14855d72887fd6d911d96e0194f99b9_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EDeepMDP 下的最优策略在原 MDP 中也不会太差\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4204a2dedce58d2a55c16044a02e681b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1562\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb\" width=\"1562\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4204a2dedce58d2a55c16044a02e681b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1562&#39; height=&#39;478&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1562\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1562\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4204a2dedce58d2a55c16044a02e681b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4204a2dedce58d2a55c16044a02e681b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-da0fa0e95887dbf0b2bb1d233073b805_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-da0fa0e95887dbf0b2bb1d233073b805_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1616&#39; height=&#39;250&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-da0fa0e95887dbf0b2bb1d233073b805_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-da0fa0e95887dbf0b2bb1d233073b805_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E5. Local DeepMDP bounds\u003C\u002Fh3\u003E\u003Cp\u003E前面讲的 global bound 的要求是对于所有的 state-action space，DeepMDP 和原 MDP 差距都严格地不超过规定的数值。在实际中都是在样本上最小化误差的，没法保证全局的 bound，因此这里考虑一个更为实际的情况，即在样本上 DeepMDP 和原 MDP 差距不太大时，相应的 bound。\u003C\u002Fp\u003E\u003Cp\u003E这里要求在样本上 DeepMDP 和原 MDP 的 reward 和 dynamics 相差不大。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c1b5e5e1f1d5bcecdfcee5875969721_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1662\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"1662\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c1b5e5e1f1d5bcecdfcee5875969721_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1662&#39; height=&#39;230&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1662\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1662\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c1b5e5e1f1d5bcecdfcee5875969721_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c1b5e5e1f1d5bcecdfcee5875969721_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EDeepMDP 和原 MDP 价值函数的差距不太大\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c47ba25116a50f75e14a6fd8eba1fbbd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"598\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c47ba25116a50f75e14a6fd8eba1fbbd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1616&#39; height=&#39;598&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"598\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c47ba25116a50f75e14a6fd8eba1fbbd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c47ba25116a50f75e14a6fd8eba1fbbd_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E满足约束的状态表示（abstraction\u002Frepresentation）下距离相近的状态价值函数也相近\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8d834b66d0273dd7d3b21ac73305876_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb\" width=\"1550\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8d834b66d0273dd7d3b21ac73305876_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1550&#39; height=&#39;684&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1550\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8d834b66d0273dd7d3b21ac73305876_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8d834b66d0273dd7d3b21ac73305876_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这说明，对于策略访问较频繁的状态，如果其表示比较相近，那么其价值函数也差不太多；但是对于样本较少的区域，其表示就不是特别准确了。\u003C\u002Fp\u003E\u003Ch3\u003E6. 与 bisimulation 的联系\u003C\u002Fh3\u003E\u003Cp\u003EBisimulation 对应的映射 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 可以最大程度地压缩状态空间，可以证明在压缩之后的 MDP 上找到的最优策略性能和原 MDP 上最优策略性能差不多。这样，策略空间实际上可以被大大缩小到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7B%5CPi%7D\" alt=\"\\tilde{\\Pi}\" eeimg=\"1\"\u002F\u003E （注意这上面是一弯，不是一横），从而提高了找到一个好的策略的效率；具体说来，这里的策略空间就不允许在被 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 映射到同一个状态下的两个状态下采取不同的行动；如果是近似的 bisimulation，即不允许在被 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 映射到相似状态下的两个状态下采取非常不同的行动。这里想要证明的是对于任何 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7B%5CPi%7D\" alt=\"\\tilde{\\Pi}\" eeimg=\"1\"\u002F\u003E 中的策略，都可以找到 DeepMDP 中的一个策略，使得这两个策略相差不多。\u003C\u002Fp\u003E\u003Cp\u003E下面先定义 bisimulation\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-457c49036a8e9c26f104e6001e4d28f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1560\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-457c49036a8e9c26f104e6001e4d28f2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1560&#39; height=&#39;610&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1560\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-457c49036a8e9c26f104e6001e4d28f2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-457c49036a8e9c26f104e6001e4d28f2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003Ebisimulation 只给出了两个状态等价不等价的定义，为了便于分析，我们还需要定义两个状态相似的程度。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fd1b726b734ebe2352b54e5d56f183cc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1538\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fd1b726b734ebe2352b54e5d56f183cc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1538&#39; height=&#39;352&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1538\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fd1b726b734ebe2352b54e5d56f183cc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fd1b726b734ebe2352b54e5d56f183cc_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003Ebisimulation 下比较容易学到的策略集合定义如下\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-87032e07d7f2536bce3b78b934d30a5d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-87032e07d7f2536bce3b78b934d30a5d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1612&#39; height=&#39;242&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-87032e07d7f2536bce3b78b934d30a5d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-87032e07d7f2536bce3b78b934d30a5d_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E一个理想的 state abstraction 产生的效果应该是对应的策略集合包括下面的策略集合（最优策略在此集合中），但是大小尽可能小。下面这个定理说明了这一点。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e30e8708db303634f0adc4e268b9d186_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1644\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb\" width=\"1644\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e30e8708db303634f0adc4e268b9d186_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1644&#39; height=&#39;658&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1644\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1644\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e30e8708db303634f0adc4e268b9d186_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e30e8708db303634f0adc4e268b9d186_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E7. 实验\u003C\u002Fh3\u003E\u003Cp\u003E文章实验效果还不错，在一个 demo 的环境中说明了该方法确实能够找到一个有意义的 state abstraction；用这种方法做辅助能够在 Atari 游戏上有性能提升。不过文章还是提到如果直接训练 DeepMDP 的三个神经网络，可能导致 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 网络什么都学不到，因为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Cmathcal%7BP%7D%7D\" alt=\"\\bar{\\mathcal{P}}\" eeimg=\"1\"\u002F\u003E 网络为了预测的更准，会促使 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E 网络生成没有信息量的状态抽象，即整体的训练过程有一定的冲突。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp\u003E参考文献\u003C\u002Fp\u003E\u003Cp\u003E[1] Liu, Ruo-Ze, et al. &#34;Efficient Reinforcement Learning with a Mind-Game for Full-Length StarCraft II.&#34;\u003Ci\u003EarXiv preprint arXiv:1903.00715\u003C\u002Fi\u003E(2019).\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":21,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":3,"contributions":[{"id":21021892,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 68】DeepMDP - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F68364144 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-3","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"web_upload","type":"String","value":"1"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"1","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F68364144","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F68364144","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>