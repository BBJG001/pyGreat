<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 74】Value Function Polytope - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning),优化"/><meta data-react-helmet="true" name="description" content="原文传送门Dadashi, Robert, et al. &amp;#34;The Value Function Polytope in Reinforcement Learning.&amp;#34; arXiv preprint arXiv:1901.11524 (2019).特色一篇理论的工作，描述了在 tabular case 下策略空间到价值函…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 74】Value Function Polytope"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/70027197"/><meta data-react-helmet="true" property="og:description" content="原文传送门Dadashi, Robert, et al. &amp;#34;The Value Function Polytope in Reinforcement Learning.&amp;#34; arXiv preprint arXiv:1901.11524 (2019).特色一篇理论的工作，描述了在 tabular case 下策略空间到价值函…"/><meta data-react-helmet="true" property="og:image" content="https://pic1.zhimg.com/v2-bfea14f4f0f775c0e8373db410a8c9bf_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:70027197,&quot;title&quot;:&quot;【强化学习 74】Value Function Polytope&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic1.zhimg.com/v2-bfea14f4f0f775c0e8373db410a8c9bf_1200x500.jpg" alt="【强化学习 74】Value Function Polytope"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 74】Value Function Polytope</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">12 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1901.11524" class=" wrap external" target="_blank" rel="nofollow noreferrer">Dadashi, Robert, et al. &#34;The Value Function Polytope in Reinforcement Learning.&#34; arXiv preprint arXiv:1901.11524 (2019).</a></p><h2>特色</h2><p>一篇理论的工作，描述了在 tabular case 下策略空间到价值函数的映射关系。虽然该映射是一个非线性的映射，但是该映射中仍然包含了很多线性关系。比如所有策略能够达到的价值函数形成一个 polytope，即其边界都是线性的；如果只有一个状态上的策略不同，那么其对应的价值函数在同一条直线上。从这些这个几何的视角出发，文章还演示了常见强化学习算法在这个 polytope 上的演化规律，比如 value iteration、policy iteration、policy gradient、cross-entropy method 等。本文对于理解这些方法也会有很好的启发和帮助。</p><h2>过程</h2><h3>1. Polytope</h3><p>首先定义了 convex polytope / polyhedron，</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-e2dd22ec734ff3b02ea39647826a0410_b.jpg" data-caption="" data-size="normal" data-rawwidth="983" data-rawheight="311" class="origin_image zh-lightbox-thumb" width="983" data-original="https://pic1.zhimg.com/v2-e2dd22ec734ff3b02ea39647826a0410_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;983&#39; height=&#39;311&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="983" data-rawheight="311" class="origin_image zh-lightbox-thumb lazy" width="983" data-original="https://pic1.zhimg.com/v2-e2dd22ec734ff3b02ea39647826a0410_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e2dd22ec734ff3b02ea39647826a0410_b.jpg"/></figure><p>然后它们的集合即为 polytope / polyhedron</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-bf99ed1a64b0678e8a42a23bbe0d5e38_b.jpg" data-caption="" data-size="normal" data-rawwidth="622" data-rawheight="108" class="origin_image zh-lightbox-thumb" width="622" data-original="https://pic1.zhimg.com/v2-bf99ed1a64b0678e8a42a23bbe0d5e38_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;622&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="622" data-rawheight="108" class="origin_image zh-lightbox-thumb lazy" width="622" data-original="https://pic1.zhimg.com/v2-bf99ed1a64b0678e8a42a23bbe0d5e38_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-bf99ed1a64b0678e8a42a23bbe0d5e38_b.jpg"/></figure><p>它们之间的等价关系如下</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-0c63a0618f5dc53edcc7379e3933d80c_b.png" data-caption="" data-size="normal" data-rawwidth="1322" data-rawheight="166" class="origin_image zh-lightbox-thumb" width="1322" data-original="https://pic1.zhimg.com/v2-0c63a0618f5dc53edcc7379e3933d80c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1322&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1322" data-rawheight="166" class="origin_image zh-lightbox-thumb lazy" width="1322" data-original="https://pic1.zhimg.com/v2-0c63a0618f5dc53edcc7379e3933d80c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0c63a0618f5dc53edcc7379e3933d80c_b.png"/></figure><p>接下来文中定义了 relative neighborhood、relative interior、relative boundary 和 hyperplane。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d94e9275c2218c0f8527cefd4f471030_b.jpg" data-caption="" data-size="normal" data-rawwidth="1374" data-rawheight="714" class="origin_image zh-lightbox-thumb" width="1374" data-original="https://pic1.zhimg.com/v2-d94e9275c2218c0f8527cefd4f471030_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1374&#39; height=&#39;714&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1374" data-rawheight="714" class="origin_image zh-lightbox-thumb lazy" width="1374" data-original="https://pic1.zhimg.com/v2-d94e9275c2218c0f8527cefd4f471030_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-d94e9275c2218c0f8527cefd4f471030_b.jpg"/></figure><p>之所以要定义 relative，可以做如下理解。考虑一个空间 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E3" alt="\mathbb{R}^3" eeimg="1"/> ，考虑一个点集 <img src="https://www.zhihu.com/equation?tex=%5C%7B%28x%2Cy%2C0%29%7C+x%5E2+%2B+y%5E2+%3C+1%5C%7D" alt="\{(x,y,0)| x^2 + y^2 &lt; 1\}" eeimg="1"/> ，它的 interior 是什么呢？考虑 interior 的定义为在其中的一个点上画一个包含这个点的小球，如果小球中的每一个点都在集合内，那么这个点就是 interior。按照这个定义，上面这个点集的 interior 就是空集，因为小球总会包含不在该平面上的点，这显然很不符合直观感受。因此会定义一个 relative interior，这个点集相对于它所在 x-y 平面的 interior 就是这个圆圈的内部。由此，还可以定义 relative neighborhood 和 relative boundary。</p><p>下面给出了 polyhedron 的一些性质。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-3daab80cf0f95d5ba27b76abf8319f3b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1324" data-rawheight="464" class="origin_image zh-lightbox-thumb" width="1324" data-original="https://pic4.zhimg.com/v2-3daab80cf0f95d5ba27b76abf8319f3b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1324&#39; height=&#39;464&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1324" data-rawheight="464" class="origin_image zh-lightbox-thumb lazy" width="1324" data-original="https://pic4.zhimg.com/v2-3daab80cf0f95d5ba27b76abf8319f3b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-3daab80cf0f95d5ba27b76abf8319f3b_b.jpg"/></figure><p>没太懂这里讲的 closed 和前面讲的 bounded 有啥区别。</p><h3>2. 价值函数空间</h3><p>定义：所有稳定策略形成的价值函数空间（space of value functions）为 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BV%7D+%5Csubset+%5Cmathbb%7BR%7D%5E%7B%5Cmathcal%7BS%7D%7D" alt="\mathcal{V} \subset \mathbb{R}^{\mathcal{S}}" eeimg="1"/> 。所有的稳定策略形成的空间 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%28A%29%5ES%7D" alt="\mathcal{P(A)^S}" eeimg="1"/> 。价值函数的映射 <img src="https://www.zhihu.com/equation?tex=f_v" alt="f_v" eeimg="1"/> 把一个策略映射到其对应的价值函数空间上，同时也用它来表示集合的映射关系，即</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c2f22ed92421c2a26b125819b51aa1d1_b.png" data-caption="" data-size="normal" data-rawwidth="1404" data-rawheight="142" class="origin_image zh-lightbox-thumb" width="1404" data-original="https://pic2.zhimg.com/v2-c2f22ed92421c2a26b125819b51aa1d1_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1404&#39; height=&#39;142&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1404" data-rawheight="142" class="origin_image zh-lightbox-thumb lazy" width="1404" data-original="https://pic2.zhimg.com/v2-c2f22ed92421c2a26b125819b51aa1d1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c2f22ed92421c2a26b125819b51aa1d1_b.png"/></figure><p>其映射关系可以写作</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-f57b29fee2689815d7c96017568b6fcc_b.png" data-caption="" data-size="normal" data-rawwidth="1304" data-rawheight="114" class="origin_image zh-lightbox-thumb" width="1304" data-original="https://pic1.zhimg.com/v2-f57b29fee2689815d7c96017568b6fcc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1304&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1304" data-rawheight="114" class="origin_image zh-lightbox-thumb lazy" width="1304" data-original="https://pic1.zhimg.com/v2-f57b29fee2689815d7c96017568b6fcc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-f57b29fee2689815d7c96017568b6fcc_b.png"/></figure><p>下图画出了一些两个状态下的价值函数空间的形态</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-48b0f0e2ae88849c21fedebc7b26b8d0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1216" data-rawheight="1056" class="origin_image zh-lightbox-thumb" width="1216" data-original="https://pic1.zhimg.com/v2-48b0f0e2ae88849c21fedebc7b26b8d0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1216&#39; height=&#39;1056&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1216" data-rawheight="1056" class="origin_image zh-lightbox-thumb lazy" width="1216" data-original="https://pic1.zhimg.com/v2-48b0f0e2ae88849c21fedebc7b26b8d0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-48b0f0e2ae88849c21fedebc7b26b8d0_b.jpg"/></figure><p>它具有如下基本性质</p><ul><li>Dominance：最优价值函数在每一个维度上都比其他价值函数更大；</li><li>Monotonicity：价值函数空间所有的边都和原点的夹角为正（假设了 reward &gt; 0）；</li><li>Continuity：价值函数空间是连续的；</li><li>Compact and connected：价值函数的空间是 compact 并且相互连通；</li><li>Differentiable： <img src="https://www.zhihu.com/equation?tex=f_v" alt="f_v" eeimg="1"/> 在策略空间上无限阶可导；</li></ul><h3>3. 关于策略的定义</h3><p>下面会用到的以下两个概念：policy agreement 和 policy determinism。前者讲的是策略之间的关系，即两个策略在一部分状态上表现一样；后者讲的是策略的性质，即策略在某个状态、一部分状态或者所以状态上是确定性的。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-34c39c6ba20ad395139aaa15d7dd3133_b.jpg" data-caption="" data-size="normal" data-rawwidth="1406" data-rawheight="496" class="origin_image zh-lightbox-thumb" width="1406" data-original="https://pic4.zhimg.com/v2-34c39c6ba20ad395139aaa15d7dd3133_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1406&#39; height=&#39;496&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1406" data-rawheight="496" class="origin_image zh-lightbox-thumb lazy" width="1406" data-original="https://pic4.zhimg.com/v2-34c39c6ba20ad395139aaa15d7dd3133_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-34c39c6ba20ad395139aaa15d7dd3133_b.jpg"/></figure><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-a99e0bd044c4d6a03c2b09d46cf7f57a_b.png" data-caption="" data-size="normal" data-rawwidth="1396" data-rawheight="222" class="origin_image zh-lightbox-thumb" width="1396" data-original="https://pic3.zhimg.com/v2-a99e0bd044c4d6a03c2b09d46cf7f57a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1396&#39; height=&#39;222&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1396" data-rawheight="222" class="origin_image zh-lightbox-thumb lazy" width="1396" data-original="https://pic3.zhimg.com/v2-a99e0bd044c4d6a03c2b09d46cf7f57a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-a99e0bd044c4d6a03c2b09d46cf7f57a_b.png"/></figure><h3>4. 价值函数映射的线性关系</h3><p><b><i>当策略只能在一部分状态上变化时，价值函数的变化范围也限定在一个子空间上</i></b></p><p>考虑到</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-5757bfbed7222d52057108c47c74bb64_b.png" data-caption="" data-size="normal" data-rawwidth="1308" data-rawheight="90" class="origin_image zh-lightbox-thumb" width="1308" data-original="https://pic1.zhimg.com/v2-5757bfbed7222d52057108c47c74bb64_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1308&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1308" data-rawheight="90" class="origin_image zh-lightbox-thumb lazy" width="1308" data-original="https://pic1.zhimg.com/v2-5757bfbed7222d52057108c47c74bb64_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-5757bfbed7222d52057108c47c74bb64_b.png"/></figure><p>令矩阵 <img src="https://www.zhihu.com/equation?tex=%28I-%5Cgamma+P%5E%5Cpi%29%5E%7B-1%7D+%3D+%5BC_1%5E%5Cpi%7CC_1%5E%5Cpi%7C%5Ccdots%7CC_%7B%7CS%7C%7D%5E%5Cpi%5D" alt="(I-\gamma P^\pi)^{-1} = [C_1^\pi|C_1^\pi|\cdots|C_{|S|}^\pi]" eeimg="1"/> ，定义</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-a040133ff4d1070cf7135a56e67d5129_b.png" data-caption="" data-size="normal" data-rawwidth="1316" data-rawheight="78" class="origin_image zh-lightbox-thumb" width="1316" data-original="https://pic2.zhimg.com/v2-a040133ff4d1070cf7135a56e67d5129_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1316&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1316" data-rawheight="78" class="origin_image zh-lightbox-thumb lazy" width="1316" data-original="https://pic2.zhimg.com/v2-a040133ff4d1070cf7135a56e67d5129_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-a040133ff4d1070cf7135a56e67d5129_b.png"/></figure><p>有如下性质</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-44f145a24a0a50325c079cea41367355_b.png" data-caption="" data-size="normal" data-rawwidth="1302" data-rawheight="130" class="origin_image zh-lightbox-thumb" width="1302" data-original="https://pic2.zhimg.com/v2-44f145a24a0a50325c079cea41367355_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1302&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1302" data-rawheight="130" class="origin_image zh-lightbox-thumb lazy" width="1302" data-original="https://pic2.zhimg.com/v2-44f145a24a0a50325c079cea41367355_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-44f145a24a0a50325c079cea41367355_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-7a4a284960352ac4671fbc4e9acf81a1_b.png" data-caption="" data-size="normal" data-rawwidth="1304" data-rawheight="178" class="origin_image zh-lightbox-thumb" width="1304" data-original="https://pic2.zhimg.com/v2-7a4a284960352ac4671fbc4e9acf81a1_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1304&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1304" data-rawheight="178" class="origin_image zh-lightbox-thumb lazy" width="1304" data-original="https://pic2.zhimg.com/v2-7a4a284960352ac4671fbc4e9acf81a1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-7a4a284960352ac4671fbc4e9acf81a1_b.png"/></figure><p>下面考虑只有一个状态上的策略不同的情况。</p><p>下面的引理给出，<b><i>对于任意两个策略只在某一个状态下不同，它们的价值函数形成一条直线；形成的这条直线和原点的夹角为正；并且它们之间可以单调连续地 interpolate</i></b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-ecb7ea89daf7c0d150150fcdbd582973_b.jpg" data-caption="" data-size="normal" data-rawwidth="1288" data-rawheight="848" class="origin_image zh-lightbox-thumb" width="1288" data-original="https://pic4.zhimg.com/v2-ecb7ea89daf7c0d150150fcdbd582973_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1288&#39; height=&#39;848&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1288" data-rawheight="848" class="origin_image zh-lightbox-thumb lazy" width="1288" data-original="https://pic4.zhimg.com/v2-ecb7ea89daf7c0d150150fcdbd582973_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ecb7ea89daf7c0d150150fcdbd582973_b.jpg"/></figure><p>那么这条直线肯定会“戳穿”价值函数空间，那么其顶点是什么呢？下面定理给出，其顶点一定是在该状态下的确定性策略。证明方法用反证法即可，即先证明存在这样的顶点，假设其对应的策略，然后证明该策略可以被分解为确定性策略的加权和，并且被确定性策略“包围”。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-9a524ef727f95996cd2f48a30edf9209_b.jpg" data-caption="" data-size="normal" data-rawwidth="1236" data-rawheight="730" class="origin_image zh-lightbox-thumb" width="1236" data-original="https://pic2.zhimg.com/v2-9a524ef727f95996cd2f48a30edf9209_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1236&#39; height=&#39;730&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1236" data-rawheight="730" class="origin_image zh-lightbox-thumb lazy" width="1236" data-original="https://pic2.zhimg.com/v2-9a524ef727f95996cd2f48a30edf9209_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-9a524ef727f95996cd2f48a30edf9209_b.jpg"/></figure><p>注意到上面 (ii) 和 (iii) 两条线相同，但是其中的元素的一一对应关系并不是线性的，可以参考如下例子。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-b0c38c01a3f293defb5960d957a31377_b.png" data-caption="" data-size="normal" data-rawwidth="1308" data-rawheight="156" class="origin_image zh-lightbox-thumb" width="1308" data-original="https://pic4.zhimg.com/v2-b0c38c01a3f293defb5960d957a31377_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1308&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1308" data-rawheight="156" class="origin_image zh-lightbox-thumb lazy" width="1308" data-original="https://pic4.zhimg.com/v2-b0c38c01a3f293defb5960d957a31377_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b0c38c01a3f293defb5960d957a31377_b.png"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-0de1495458e4b199d6a08e14bec30e90_b.jpg" data-caption="" data-size="normal" data-rawwidth="1384" data-rawheight="302" class="origin_image zh-lightbox-thumb" width="1384" data-original="https://pic1.zhimg.com/v2-0de1495458e4b199d6a08e14bec30e90_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1384&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1384" data-rawheight="302" class="origin_image zh-lightbox-thumb lazy" width="1384" data-original="https://pic1.zhimg.com/v2-0de1495458e4b199d6a08e14bec30e90_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0de1495458e4b199d6a08e14bec30e90_b.jpg"/></figure><p>同时，以上结论只适用于两个只相差一个状态的策略，对于任意的两个策略，其策略内插形成的价值函数不是一条直线，例如下面的例子。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-412e1003a9a871ff32094cf707af84a8_b.jpg" data-caption="" data-size="normal" data-rawwidth="1320" data-rawheight="700" class="origin_image zh-lightbox-thumb" width="1320" data-original="https://pic1.zhimg.com/v2-412e1003a9a871ff32094cf707af84a8_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1320&#39; height=&#39;700&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1320" data-rawheight="700" class="origin_image zh-lightbox-thumb lazy" width="1320" data-original="https://pic1.zhimg.com/v2-412e1003a9a871ff32094cf707af84a8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-412e1003a9a871ff32094cf707af84a8_b.jpg"/></figure><h3>5. 价值函数映射的其他结论以及价值函数空间的形态</h3><p>其实有了定理 1 之后其实就能够对 <img src="https://www.zhihu.com/equation?tex=f_v" alt="f_v" eeimg="1"/> 的映射关系有了很好的认识了，由此这里直接形象地总结一下相应的结论，具体的定理见原文。</p><ul><li>价值函数空间在确定性策略（可能是对于某一些状态的确定性策略）张成的一个 convex hull 中。如下图所示所示。</li></ul><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-5cd4187424d21019bd328cfc61e0a99d_b.jpg" data-caption="" data-size="normal" data-rawwidth="1288" data-rawheight="660" class="origin_image zh-lightbox-thumb" width="1288" data-original="https://pic2.zhimg.com/v2-5cd4187424d21019bd328cfc61e0a99d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1288&#39; height=&#39;660&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1288" data-rawheight="660" class="origin_image zh-lightbox-thumb lazy" width="1288" data-original="https://pic2.zhimg.com/v2-5cd4187424d21019bd328cfc61e0a99d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-5cd4187424d21019bd328cfc61e0a99d_b.jpg"/></figure><ul><li>由于一共有 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BS%7D%7C" alt="|\mathcal{S}|" eeimg="1"/> 个状态，从任意策略出发，每次变化一个状态上的策略，相应地在价值函数空间上会形成一条直线。因此从任意一个策略对应的价值函数出发，都可以经过 <img src="https://www.zhihu.com/equation?tex=k%3C%7C%5Cmathcal%7BS%7D%7C" alt="k&lt;|\mathcal{S}|" eeimg="1"/> 条直线（相应的策略只在一个状态上变化）到达另一个策略对应的价值函数。</li><li>考虑一族策略，它们在状态 <img src="https://www.zhihu.com/equation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D" alt="\{s_1, \cdots, s_k\}" eeimg="1"/> 上都和 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 一样，这一族策略形成的价值函数族的边界一定是由至少某个状态上是确定性的策略构成的，边界上的策略不仅在状态 <img src="https://www.zhihu.com/equation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D" alt="\{s_1, \cdots, s_k\}" eeimg="1"/> 上都和 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 一样，而且在某个除状态 <img src="https://www.zhihu.com/equation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D" alt="\{s_1, \cdots, s_k\}" eeimg="1"/> 外的状态上是确定性的。如下图所示</li></ul><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-c501da1b09a2880224bb44f08cf9713f_b.jpg" data-caption="" data-size="normal" data-rawwidth="1232" data-rawheight="656" class="origin_image zh-lightbox-thumb" width="1232" data-original="https://pic4.zhimg.com/v2-c501da1b09a2880224bb44f08cf9713f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1232&#39; height=&#39;656&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1232" data-rawheight="656" class="origin_image zh-lightbox-thumb lazy" width="1232" data-original="https://pic4.zhimg.com/v2-c501da1b09a2880224bb44f08cf9713f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-c501da1b09a2880224bb44f08cf9713f_b.jpg"/></figure><ul><li>同样考虑一族策略，它们在状态 <img src="https://www.zhihu.com/equation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D" alt="\{s_1, \cdots, s_k\}" eeimg="1"/> 上都和 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 一样，这一族策略形成的价值函数族是一个 polytope。作为一个特殊情况， <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BV%7D" alt="\mathcal{V}" eeimg="1"/> 也是一个 polytope。</li></ul><h3>6. 不同算法在价值函数空间中的演化</h3><p>注意，一下所有的分析中的更新都去掉了 stochastic 的成分，相当于每一步都更新的是更新公式的均值。同时所有涉及到 function approximation 的部分也都等价地改为 tabular case 的情形，即 state embedding 为 one-hot vector，function approximation 为线性拟合。</p><p><b><i>Value iteration</i></b></p><p>Value iteration 是做 optimality Bellman operator 的迭代，可以看出，它迭代过程中产生的价值函数并不保证对应实际的策略（即会超出 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BV%7D" alt="\mathcal{V}" eeimg="1"/> 的范围），不过最终它会迭代到最优价值函数位置。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-97795dad29fc7ed910d0c3e96c496a1e_b.jpg" data-caption="" data-size="normal" data-rawwidth="1130" data-rawheight="472" class="origin_image zh-lightbox-thumb" width="1130" data-original="https://pic3.zhimg.com/v2-97795dad29fc7ed910d0c3e96c496a1e_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1130&#39; height=&#39;472&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1130" data-rawheight="472" class="origin_image zh-lightbox-thumb lazy" width="1130" data-original="https://pic3.zhimg.com/v2-97795dad29fc7ed910d0c3e96c496a1e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-97795dad29fc7ed910d0c3e96c496a1e_b.jpg"/></figure><p><b><i>Policy iteration</i></b></p><p>Policy iteration 分为 policy evaluation 和 policy improvement 两个步骤。考虑每次 policy improvement 都是一个相对于前一步价值函数的 greedy 策略，那么每次的 policy 都会是一个确定性策略，即会对应 polytope 的一个顶点。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-65368bb12c1a2967cb5c8263b0313c7f_b.jpg" data-caption="" data-size="normal" data-rawwidth="1176" data-rawheight="546" class="origin_image zh-lightbox-thumb" width="1176" data-original="https://pic4.zhimg.com/v2-65368bb12c1a2967cb5c8263b0313c7f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1176&#39; height=&#39;546&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1176" data-rawheight="546" class="origin_image zh-lightbox-thumb lazy" width="1176" data-original="https://pic4.zhimg.com/v2-65368bb12c1a2967cb5c8263b0313c7f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-65368bb12c1a2967cb5c8263b0313c7f_b.jpg"/></figure><p><b><i>Policy gradient</i></b></p><p>策略梯度方法的收敛速率对于初始值十分敏感，比如从图 a 可以看出，它在原始状态附近过了很久才开始往最优策略方向更新。同时它还可能会陷入局部极小值点，在该点上，策略逐渐变为确定性策略，这样会缺乏探索，使得它不知道其实在某状态上选择另外一个行动会更好。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-4705c0d32058cc38b2cf8af83974d457_b.jpg" data-caption="" data-size="normal" data-rawwidth="1258" data-rawheight="482" class="origin_image zh-lightbox-thumb" width="1258" data-original="https://pic4.zhimg.com/v2-4705c0d32058cc38b2cf8af83974d457_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1258&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1258" data-rawheight="482" class="origin_image zh-lightbox-thumb lazy" width="1258" data-original="https://pic4.zhimg.com/v2-4705c0d32058cc38b2cf8af83974d457_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4705c0d32058cc38b2cf8af83974d457_b.jpg"/></figure><p>可以看出上述问题的主要原因都在于其策略太过于贴近 polytope 边界（缺乏探索），因此可以加入 entropy 正则项来缓解这一问题。</p><p><b><i>Entropy regularized policy gradient</i></b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-5f63b63ffffbbb09197faf4ee905eb2f_b.jpg" data-caption="" data-size="normal" data-rawwidth="1108" data-rawheight="534" class="origin_image zh-lightbox-thumb" width="1108" data-original="https://pic4.zhimg.com/v2-5f63b63ffffbbb09197faf4ee905eb2f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1108&#39; height=&#39;534&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1108" data-rawheight="534" class="origin_image zh-lightbox-thumb lazy" width="1108" data-original="https://pic4.zhimg.com/v2-5f63b63ffffbbb09197faf4ee905eb2f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-5f63b63ffffbbb09197faf4ee905eb2f_b.jpg"/></figure><p><b><i>Natural policy gradient</i></b></p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-1ce66683db46303e44c0991798b4b4f4_b.png" data-caption="" data-size="normal" data-rawwidth="1116" data-rawheight="102" class="origin_image zh-lightbox-thumb" width="1116" data-original="https://pic1.zhimg.com/v2-1ce66683db46303e44c0991798b4b4f4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1116&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1116" data-rawheight="102" class="origin_image zh-lightbox-thumb lazy" width="1116" data-original="https://pic1.zhimg.com/v2-1ce66683db46303e44c0991798b4b4f4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-1ce66683db46303e44c0991798b4b4f4_b.png"/></figure><p>从下面的实验结果可以看出，使用这样的自然梯度可以避免像之前 policy gradient 那样多次迭代仍然陷在某个区域附近；同时，它不加 entropy regularization 也没有出现（在这个例子中）陷入局部极小的情况。它的更新轨迹和 policy iteration 比较类似。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-5195eae416c3279af47815d046fcefa4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1198" data-rawheight="494" class="origin_image zh-lightbox-thumb" width="1198" data-original="https://pic1.zhimg.com/v2-5195eae416c3279af47815d046fcefa4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1198&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1198" data-rawheight="494" class="origin_image zh-lightbox-thumb lazy" width="1198" data-original="https://pic1.zhimg.com/v2-5195eae416c3279af47815d046fcefa4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-5195eae416c3279af47815d046fcefa4_b.jpg"/></figure><p><b><i>Cross-entropy method</i></b></p><p>这是一种 gradient-free 的方法，可以见专栏之前的文章（CEM）。这里发现如果不加噪声，它比较容易陷入局部极小值。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-41f8f1942dea4f3b6fd09cbf8405fa3a_b.jpg" data-caption="" data-size="normal" data-rawwidth="1354" data-rawheight="844" class="origin_image zh-lightbox-thumb" width="1354" data-original="https://pic3.zhimg.com/v2-41f8f1942dea4f3b6fd09cbf8405fa3a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1354&#39; height=&#39;844&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1354" data-rawheight="844" class="origin_image zh-lightbox-thumb lazy" width="1354" data-original="https://pic3.zhimg.com/v2-41f8f1942dea4f3b6fd09cbf8405fa3a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-41f8f1942dea4f3b6fd09cbf8405fa3a_b.jpg"/></figure><p></p></div></div><div class="ContentItem-time">编辑于 2019-06-21</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19570512" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">优化</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 12 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 12</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>3 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="4811320a-544e-4792-9c69-7117c99a9a30" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="4811320a-544e-4792-9c69-7117c99a9a30">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"70027197":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":70027197,"title":"【强化学习 74】Value Function Polytope","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F70027197","imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bfea14f4f0f775c0e8373db410a8c9bf_b.jpg","titleImage":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bfea14f4f0f775c0e8373db410a8c9bf_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a623d8a65501486bbaf315ab19400a85_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"983\" data-rawheight=\"311\" data-watermark=\"watermark\" data-original-src=\"v2-a623d8a65501486bbaf315ab19400a85\" data-watermark-src=\"v2-e2dd22ec734ff3b02ea39647826a0410\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a623d8a65501486bbaf315ab19400a85_r.png\"\u002F\u003E原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1901.11524\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EDadashi, Robert, et al. &#34;The Value Function Polytope in Reinforcement Learning.&#34; arXiv preprint arXiv:1901.11524 (2019).\u003C\u002Fa\u003E特色一篇理论的工作，描述了在 tabular case 下策略空间到价值函数的映射关系。虽然该映射是一个非线性的映射，但是…","created":1561086637,"updated":1561086731,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1454,"imageHeight":408,"content":"\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1901.11524\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EDadashi, Robert, et al. &#34;The Value Function Polytope in Reinforcement Learning.&#34; arXiv preprint arXiv:1901.11524 (2019).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E一篇理论的工作，描述了在 tabular case 下策略空间到价值函数的映射关系。虽然该映射是一个非线性的映射，但是该映射中仍然包含了很多线性关系。比如所有策略能够达到的价值函数形成一个 polytope，即其边界都是线性的；如果只有一个状态上的策略不同，那么其对应的价值函数在同一条直线上。从这些这个几何的视角出发，文章还演示了常见强化学习算法在这个 polytope 上的演化规律，比如 value iteration、policy iteration、policy gradient、cross-entropy method 等。本文对于理解这些方法也会有很好的启发和帮助。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1. Polytope\u003C\u002Fh3\u003E\u003Cp\u003E首先定义了 convex polytope \u002F polyhedron，\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2dd22ec734ff3b02ea39647826a0410_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"983\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb\" width=\"983\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2dd22ec734ff3b02ea39647826a0410_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;983&#39; height=&#39;311&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"983\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"983\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2dd22ec734ff3b02ea39647826a0410_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2dd22ec734ff3b02ea39647826a0410_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E然后它们的集合即为 polytope \u002F polyhedron\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf99ed1a64b0678e8a42a23bbe0d5e38_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"622\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf99ed1a64b0678e8a42a23bbe0d5e38_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;622&#39; height=&#39;108&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"622\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf99ed1a64b0678e8a42a23bbe0d5e38_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bf99ed1a64b0678e8a42a23bbe0d5e38_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E它们之间的等价关系如下\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0c63a0618f5dc53edcc7379e3933d80c_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1322\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb\" width=\"1322\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0c63a0618f5dc53edcc7379e3933d80c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1322&#39; height=&#39;166&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1322\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1322\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0c63a0618f5dc53edcc7379e3933d80c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0c63a0618f5dc53edcc7379e3933d80c_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E接下来文中定义了 relative neighborhood、relative interior、relative boundary 和 hyperplane。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d94e9275c2218c0f8527cefd4f471030_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"714\" class=\"origin_image zh-lightbox-thumb\" width=\"1374\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d94e9275c2218c0f8527cefd4f471030_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1374&#39; height=&#39;714&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"714\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1374\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d94e9275c2218c0f8527cefd4f471030_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d94e9275c2218c0f8527cefd4f471030_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E之所以要定义 relative，可以做如下理解。考虑一个空间 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BR%7D%5E3\" alt=\"\\mathbb{R}^3\" eeimg=\"1\"\u002F\u003E ，考虑一个点集 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7B%28x%2Cy%2C0%29%7C+x%5E2+%2B+y%5E2+%3C+1%5C%7D\" alt=\"\\{(x,y,0)| x^2 + y^2 &lt; 1\\}\" eeimg=\"1\"\u002F\u003E ，它的 interior 是什么呢？考虑 interior 的定义为在其中的一个点上画一个包含这个点的小球，如果小球中的每一个点都在集合内，那么这个点就是 interior。按照这个定义，上面这个点集的 interior 就是空集，因为小球总会包含不在该平面上的点，这显然很不符合直观感受。因此会定义一个 relative interior，这个点集相对于它所在 x-y 平面的 interior 就是这个圆圈的内部。由此，还可以定义 relative neighborhood 和 relative boundary。\u003C\u002Fp\u003E\u003Cp\u003E下面给出了 polyhedron 的一些性质。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3daab80cf0f95d5ba27b76abf8319f3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb\" width=\"1324\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3daab80cf0f95d5ba27b76abf8319f3b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1324&#39; height=&#39;464&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1324\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3daab80cf0f95d5ba27b76abf8319f3b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3daab80cf0f95d5ba27b76abf8319f3b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E没太懂这里讲的 closed 和前面讲的 bounded 有啥区别。\u003C\u002Fp\u003E\u003Ch3\u003E2. 价值函数空间\u003C\u002Fh3\u003E\u003Cp\u003E定义：所有稳定策略形成的价值函数空间（space of value functions）为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BV%7D+%5Csubset+%5Cmathbb%7BR%7D%5E%7B%5Cmathcal%7BS%7D%7D\" alt=\"\\mathcal{V} \\subset \\mathbb{R}^{\\mathcal{S}}\" eeimg=\"1\"\u002F\u003E 。所有的稳定策略形成的空间 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BP%28A%29%5ES%7D\" alt=\"\\mathcal{P(A)^S}\" eeimg=\"1\"\u002F\u003E 。价值函数的映射 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f_v\" alt=\"f_v\" eeimg=\"1\"\u002F\u003E 把一个策略映射到其对应的价值函数空间上，同时也用它来表示集合的映射关系，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c2f22ed92421c2a26b125819b51aa1d1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1404\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb\" width=\"1404\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c2f22ed92421c2a26b125819b51aa1d1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1404&#39; height=&#39;142&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1404\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1404\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c2f22ed92421c2a26b125819b51aa1d1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c2f22ed92421c2a26b125819b51aa1d1_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其映射关系可以写作\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f57b29fee2689815d7c96017568b6fcc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1304\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f57b29fee2689815d7c96017568b6fcc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1304&#39; height=&#39;114&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1304\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f57b29fee2689815d7c96017568b6fcc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f57b29fee2689815d7c96017568b6fcc_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E下图画出了一些两个状态下的价值函数空间的形态\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-48b0f0e2ae88849c21fedebc7b26b8d0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb\" width=\"1216\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-48b0f0e2ae88849c21fedebc7b26b8d0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1216&#39; height=&#39;1056&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1216\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-48b0f0e2ae88849c21fedebc7b26b8d0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-48b0f0e2ae88849c21fedebc7b26b8d0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E它具有如下基本性质\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EDominance：最优价值函数在每一个维度上都比其他价值函数更大；\u003C\u002Fli\u003E\u003Cli\u003EMonotonicity：价值函数空间所有的边都和原点的夹角为正（假设了 reward &gt; 0）；\u003C\u002Fli\u003E\u003Cli\u003EContinuity：价值函数空间是连续的；\u003C\u002Fli\u003E\u003Cli\u003ECompact and connected：价值函数的空间是 compact 并且相互连通；\u003C\u002Fli\u003E\u003Cli\u003EDifferentiable： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f_v\" alt=\"f_v\" eeimg=\"1\"\u002F\u003E 在策略空间上无限阶可导；\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E3. 关于策略的定义\u003C\u002Fh3\u003E\u003Cp\u003E下面会用到的以下两个概念：policy agreement 和 policy determinism。前者讲的是策略之间的关系，即两个策略在一部分状态上表现一样；后者讲的是策略的性质，即策略在某个状态、一部分状态或者所以状态上是确定性的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34c39c6ba20ad395139aaa15d7dd3133_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1406\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"1406\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34c39c6ba20ad395139aaa15d7dd3133_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1406&#39; height=&#39;496&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1406\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1406\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34c39c6ba20ad395139aaa15d7dd3133_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34c39c6ba20ad395139aaa15d7dd3133_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a99e0bd044c4d6a03c2b09d46cf7f57a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1396\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb\" width=\"1396\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a99e0bd044c4d6a03c2b09d46cf7f57a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1396&#39; height=&#39;222&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1396\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1396\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a99e0bd044c4d6a03c2b09d46cf7f57a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a99e0bd044c4d6a03c2b09d46cf7f57a_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E4. 价值函数映射的线性关系\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E当策略只能在一部分状态上变化时，价值函数的变化范围也限定在一个子空间上\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E考虑到\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5757bfbed7222d52057108c47c74bb64_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"1308\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5757bfbed7222d52057108c47c74bb64_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1308&#39; height=&#39;90&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1308\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5757bfbed7222d52057108c47c74bb64_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5757bfbed7222d52057108c47c74bb64_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E令矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28I-%5Cgamma+P%5E%5Cpi%29%5E%7B-1%7D+%3D+%5BC_1%5E%5Cpi%7CC_1%5E%5Cpi%7C%5Ccdots%7CC_%7B%7CS%7C%7D%5E%5Cpi%5D\" alt=\"(I-\\gamma P^\\pi)^{-1} = [C_1^\\pi|C_1^\\pi|\\cdots|C_{|S|}^\\pi]\" eeimg=\"1\"\u002F\u003E ，定义\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a040133ff4d1070cf7135a56e67d5129_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1316\" data-rawheight=\"78\" class=\"origin_image zh-lightbox-thumb\" width=\"1316\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a040133ff4d1070cf7135a56e67d5129_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1316&#39; height=&#39;78&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1316\" data-rawheight=\"78\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1316\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a040133ff4d1070cf7135a56e67d5129_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-a040133ff4d1070cf7135a56e67d5129_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E有如下性质\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44f145a24a0a50325c079cea41367355_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1302\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb\" width=\"1302\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44f145a24a0a50325c079cea41367355_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1302&#39; height=&#39;130&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1302\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1302\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44f145a24a0a50325c079cea41367355_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44f145a24a0a50325c079cea41367355_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7a4a284960352ac4671fbc4e9acf81a1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1304\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7a4a284960352ac4671fbc4e9acf81a1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1304&#39; height=&#39;178&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1304\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7a4a284960352ac4671fbc4e9acf81a1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7a4a284960352ac4671fbc4e9acf81a1_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E下面考虑只有一个状态上的策略不同的情况。\u003C\u002Fp\u003E\u003Cp\u003E下面的引理给出，\u003Cb\u003E\u003Ci\u003E对于任意两个策略只在某一个状态下不同，它们的价值函数形成一条直线；形成的这条直线和原点的夹角为正；并且它们之间可以单调连续地 interpolate\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ecb7ea89daf7c0d150150fcdbd582973_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"848\" class=\"origin_image zh-lightbox-thumb\" width=\"1288\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ecb7ea89daf7c0d150150fcdbd582973_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1288&#39; height=&#39;848&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"848\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1288\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ecb7ea89daf7c0d150150fcdbd582973_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ecb7ea89daf7c0d150150fcdbd582973_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E那么这条直线肯定会“戳穿”价值函数空间，那么其顶点是什么呢？下面定理给出，其顶点一定是在该状态下的确定性策略。证明方法用反证法即可，即先证明存在这样的顶点，假设其对应的策略，然后证明该策略可以被分解为确定性策略的加权和，并且被确定性策略“包围”。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9a524ef727f95996cd2f48a30edf9209_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb\" width=\"1236\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9a524ef727f95996cd2f48a30edf9209_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1236&#39; height=&#39;730&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1236\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9a524ef727f95996cd2f48a30edf9209_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9a524ef727f95996cd2f48a30edf9209_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到上面 (ii) 和 (iii) 两条线相同，但是其中的元素的一一对应关系并不是线性的，可以参考如下例子。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b0c38c01a3f293defb5960d957a31377_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1308\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b0c38c01a3f293defb5960d957a31377_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1308&#39; height=&#39;156&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1308\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b0c38c01a3f293defb5960d957a31377_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b0c38c01a3f293defb5960d957a31377_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0de1495458e4b199d6a08e14bec30e90_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1384\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"1384\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0de1495458e4b199d6a08e14bec30e90_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1384&#39; height=&#39;302&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1384\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1384\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0de1495458e4b199d6a08e14bec30e90_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0de1495458e4b199d6a08e14bec30e90_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E同时，以上结论只适用于两个只相差一个状态的策略，对于任意的两个策略，其策略内插形成的价值函数不是一条直线，例如下面的例子。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-412e1003a9a871ff32094cf707af84a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1320\" data-rawheight=\"700\" class=\"origin_image zh-lightbox-thumb\" width=\"1320\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-412e1003a9a871ff32094cf707af84a8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1320&#39; height=&#39;700&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1320\" data-rawheight=\"700\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1320\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-412e1003a9a871ff32094cf707af84a8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-412e1003a9a871ff32094cf707af84a8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E5. 价值函数映射的其他结论以及价值函数空间的形态\u003C\u002Fh3\u003E\u003Cp\u003E其实有了定理 1 之后其实就能够对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f_v\" alt=\"f_v\" eeimg=\"1\"\u002F\u003E 的映射关系有了很好的认识了，由此这里直接形象地总结一下相应的结论，具体的定理见原文。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E价值函数空间在确定性策略（可能是对于某一些状态的确定性策略）张成的一个 convex hull 中。如下图所示所示。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5cd4187424d21019bd328cfc61e0a99d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb\" width=\"1288\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5cd4187424d21019bd328cfc61e0a99d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1288&#39; height=&#39;660&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1288\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5cd4187424d21019bd328cfc61e0a99d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5cd4187424d21019bd328cfc61e0a99d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003E由于一共有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BS%7D%7C\" alt=\"|\\mathcal{S}|\" eeimg=\"1\"\u002F\u003E 个状态，从任意策略出发，每次变化一个状态上的策略，相应地在价值函数空间上会形成一条直线。因此从任意一个策略对应的价值函数出发，都可以经过 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k%3C%7C%5Cmathcal%7BS%7D%7C\" alt=\"k&lt;|\\mathcal{S}|\" eeimg=\"1\"\u002F\u003E 条直线（相应的策略只在一个状态上变化）到达另一个策略对应的价值函数。\u003C\u002Fli\u003E\u003Cli\u003E考虑一族策略，它们在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D\" alt=\"\\{s_1, \\cdots, s_k\\}\" eeimg=\"1\"\u002F\u003E 上都和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 一样，这一族策略形成的价值函数族的边界一定是由至少某个状态上是确定性的策略构成的，边界上的策略不仅在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D\" alt=\"\\{s_1, \\cdots, s_k\\}\" eeimg=\"1\"\u002F\u003E 上都和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 一样，而且在某个除状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D\" alt=\"\\{s_1, \\cdots, s_k\\}\" eeimg=\"1\"\u002F\u003E 外的状态上是确定性的。如下图所示\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c501da1b09a2880224bb44f08cf9713f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1232\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb\" width=\"1232\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c501da1b09a2880224bb44f08cf9713f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1232&#39; height=&#39;656&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1232\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1232\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c501da1b09a2880224bb44f08cf9713f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c501da1b09a2880224bb44f08cf9713f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003E同样考虑一族策略，它们在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bs_1%2C+%5Ccdots%2C+s_k%5C%7D\" alt=\"\\{s_1, \\cdots, s_k\\}\" eeimg=\"1\"\u002F\u003E 上都和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 一样，这一族策略形成的价值函数族是一个 polytope。作为一个特殊情况， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BV%7D\" alt=\"\\mathcal{V}\" eeimg=\"1\"\u002F\u003E 也是一个 polytope。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E6. 不同算法在价值函数空间中的演化\u003C\u002Fh3\u003E\u003Cp\u003E注意，一下所有的分析中的更新都去掉了 stochastic 的成分，相当于每一步都更新的是更新公式的均值。同时所有涉及到 function approximation 的部分也都等价地改为 tabular case 的情形，即 state embedding 为 one-hot vector，function approximation 为线性拟合。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EValue iteration\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EValue iteration 是做 optimality Bellman operator 的迭代，可以看出，它迭代过程中产生的价值函数并不保证对应实际的策略（即会超出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BV%7D\" alt=\"\\mathcal{V}\" eeimg=\"1\"\u002F\u003E 的范围），不过最终它会迭代到最优价值函数位置。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-97795dad29fc7ed910d0c3e96c496a1e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb\" width=\"1130\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-97795dad29fc7ed910d0c3e96c496a1e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1130&#39; height=&#39;472&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1130\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1130\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-97795dad29fc7ed910d0c3e96c496a1e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-97795dad29fc7ed910d0c3e96c496a1e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EPolicy iteration\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EPolicy iteration 分为 policy evaluation 和 policy improvement 两个步骤。考虑每次 policy improvement 都是一个相对于前一步价值函数的 greedy 策略，那么每次的 policy 都会是一个确定性策略，即会对应 polytope 的一个顶点。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-65368bb12c1a2967cb5c8263b0313c7f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1176\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb\" width=\"1176\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-65368bb12c1a2967cb5c8263b0313c7f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1176&#39; height=&#39;546&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1176\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1176\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-65368bb12c1a2967cb5c8263b0313c7f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-65368bb12c1a2967cb5c8263b0313c7f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EPolicy gradient\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E策略梯度方法的收敛速率对于初始值十分敏感，比如从图 a 可以看出，它在原始状态附近过了很久才开始往最优策略方向更新。同时它还可能会陷入局部极小值点，在该点上，策略逐渐变为确定性策略，这样会缺乏探索，使得它不知道其实在某状态上选择另外一个行动会更好。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4705c0d32058cc38b2cf8af83974d457_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb\" width=\"1258\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4705c0d32058cc38b2cf8af83974d457_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1258&#39; height=&#39;482&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1258\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4705c0d32058cc38b2cf8af83974d457_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4705c0d32058cc38b2cf8af83974d457_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以看出上述问题的主要原因都在于其策略太过于贴近 polytope 边界（缺乏探索），因此可以加入 entropy 正则项来缓解这一问题。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EEntropy regularized policy gradient\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5f63b63ffffbbb09197faf4ee905eb2f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1108\" data-rawheight=\"534\" class=\"origin_image zh-lightbox-thumb\" width=\"1108\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5f63b63ffffbbb09197faf4ee905eb2f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1108&#39; height=&#39;534&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1108\" data-rawheight=\"534\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1108\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5f63b63ffffbbb09197faf4ee905eb2f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5f63b63ffffbbb09197faf4ee905eb2f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003ENatural policy gradient\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1ce66683db46303e44c0991798b4b4f4_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"1116\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1ce66683db46303e44c0991798b4b4f4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1116&#39; height=&#39;102&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1116\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1ce66683db46303e44c0991798b4b4f4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1ce66683db46303e44c0991798b4b4f4_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从下面的实验结果可以看出，使用这样的自然梯度可以避免像之前 policy gradient 那样多次迭代仍然陷在某个区域附近；同时，它不加 entropy regularization 也没有出现（在这个例子中）陷入局部极小的情况。它的更新轨迹和 policy iteration 比较类似。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5195eae416c3279af47815d046fcefa4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5195eae416c3279af47815d046fcefa4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1198&#39; height=&#39;494&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1198\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5195eae416c3279af47815d046fcefa4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5195eae416c3279af47815d046fcefa4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003ECross-entropy method\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这是一种 gradient-free 的方法，可以见专栏之前的文章（CEM）。这里发现如果不加噪声，它比较容易陷入局部极小值。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-41f8f1942dea4f3b6fd09cbf8405fa3a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"844\" class=\"origin_image zh-lightbox-thumb\" width=\"1354\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-41f8f1942dea4f3b6fd09cbf8405fa3a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1354&#39; height=&#39;844&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"844\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1354\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-41f8f1942dea4f3b6fd09cbf8405fa3a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-41f8f1942dea4f3b6fd09cbf8405fa3a_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19570512","type":"topic","id":"19570512","name":"优化"}],"voteupCount":12,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":3,"contributions":[{"id":21117897,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 74】Value Function Polytope - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F70027197 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F70027197","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F70027197","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>