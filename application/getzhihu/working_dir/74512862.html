<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 82】POLITEX - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="POLITEX 全称是 POLicy ITeration with EXpert advice。原文传送门Abbasi-Yadkori, Yasin, et al. &amp;#34;Politex: Regret Bounds for Policy Iteration using Expert Prediction.&amp;#34; (2019).特色使用 expert prob…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 82】POLITEX"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/74512862"/><meta data-react-helmet="true" property="og:description" content="POLITEX 全称是 POLicy ITeration with EXpert advice。原文传送门Abbasi-Yadkori, Yasin, et al. &amp;#34;Politex: Regret Bounds for Policy Iteration using Expert Prediction.&amp;#34; (2019).特色使用 expert prob…"/><meta data-react-helmet="true" property="og:image" content="https://pic1.zhimg.com/v2-8ebbeb1c6b7a7ba82b472a58a7d51e9f_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:74512862,&quot;title&quot;:&quot;【强化学习 82】POLITEX&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic1.zhimg.com/v2-8ebbeb1c6b7a7ba82b472a58a7d51e9f_1200x500.jpg" alt="【强化学习 82】POLITEX"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 82】POLITEX</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">12 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>POLITEX 全称是 POLicy ITeration with EXpert advice。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v97/lazic19a/lazic19a-supp.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Abbasi-Yadkori, Yasin, et al. &#34;Politex: Regret Bounds for Policy Iteration using Expert Prediction.&#34; (2019).</a></p><h2>特色</h2><p>使用 expert problem 研究中的 exponential weighted average（EWA）算法，得到了一个强化学习策略，并且得到了该算法的 regret bound。注意到对 regret bound 的分析等价于 finite sample analysis。该算法可以使用任意的 Q 函数估计。文章还证明了对于 Q 函数的线性函数拟合，使用 least-squares policy evaluation（LSPE）方法来估计参数，有相应的估计误差的上界。</p><h2>过程</h2><h3>1. Expert problem and EWA</h3><p>这里的 expert problem 设定如下。这是一个多轮的游戏，每一轮分为如下几步： <img src="https://www.zhihu.com/equation?tex=A" alt="A" eeimg="1"/> 个专家分别给出对于未来的预测建议（advice） <img src="https://www.zhihu.com/equation?tex=f_%7Bi%2Ct%7D+%5Cin+%5Cmathcal%7BD%7D" alt="f_{i,t} \in \mathcal{D}" eeimg="1"/> ，属于 decision space；玩家依照一定的策略，挑选出一个专家 <img src="https://www.zhihu.com/equation?tex=I_t%5Cin%5BA%5D" alt="I_t\in[A]" eeimg="1"/> 并且采取它给出的建议；环境揭示该时刻的情形 <img src="https://www.zhihu.com/equation?tex=y_t+%5Cin+%5Cmathcal%7BO%7D" alt="y_t \in \mathcal{O}" eeimg="1"/> ，属于 outcome space；接下来，对于专家给出的每一个建议，都能够通过损失函数计算出相应的损失，该损失可以使用一个向量来表示 <img src="https://www.zhihu.com/equation?tex=l_t%5Cin%5B0%2C+1%5D%5EA" alt="l_t\in[0, 1]^A" eeimg="1"/> 。</p><p>Expert problem 一般考虑环境可能是 adversarial，甚至 expert 也是不靠谱的，如果目标是减小玩家的损失，那么只要 expert 总是给出不靠谱的“建议”，那么玩家不可能有很小的损失。然而，退一步地，expert problem 的目标是最小化玩家损失和任意一个专家（或者可以理解为最好的那个专家）损失差距。注意到，我们一开始并不知道那个专家是最好的，甚至专家也可能是 adversarial ，根据你做的决策而改变。具体来说，优化目标为最小化 regret：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathfrak%7BR%7D_%7BT%2Cj%7D%3D%5Csum_%7Bt%3D1%7D%5ET+%28l_%7Bt%2C+I_t%7D+-+l_%7Bt%2Cj%7D%29" alt="\mathfrak{R}_{T,j}=\sum_{t=1}^T (l_{t, I_t} - l_{t,j})" eeimg="1"/> </p><p>表示截止第 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 轮时，玩家累积受到的损失相比于第 <img src="https://www.zhihu.com/equation?tex=j" alt="j" eeimg="1"/> 个专家受到的累积损失的差距。</p><p>EWA 每次按照一定概率选择相应的专家，即使用一个随机策略，选择第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"/> 个专家的概率为 <img src="https://www.zhihu.com/equation?tex=%5Cpi_t%28i%29+%5Cpropto+%5Cexp%28-%5Ceta+%5Csum_%7Bs%3D1%7D%5E%7Bt-1%7D+l_%7Bs%2Ci%7D%29" alt="\pi_t(i) \propto \exp(-\eta \sum_{s=1}^{t-1} l_{s,i})" eeimg="1"/> 。即，如果该专家历史上表现都比较好，就会以比较高的概率选择该专家。这个策略也叫做 Boltzmann policy。如果使用该策略，可以证明，当时间较长的时候，玩家遭受的损失基本上和最好专家遭受的损失差不多。具体地，见如下定理：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d7b86740e296113b999b50cc1a04cbe4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1458" data-rawheight="270" class="origin_image zh-lightbox-thumb" width="1458" data-original="https://pic1.zhimg.com/v2-d7b86740e296113b999b50cc1a04cbe4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1458&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1458" data-rawheight="270" class="origin_image zh-lightbox-thumb lazy" width="1458" data-original="https://pic1.zhimg.com/v2-d7b86740e296113b999b50cc1a04cbe4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-d7b86740e296113b999b50cc1a04cbe4_b.jpg"/></figure><p>注意到，参数 <img src="https://www.zhihu.com/equation?tex=%5Ceta" alt="\eta" eeimg="1"/> 的选择要求我们实现知道游戏需要玩多少轮，这个条件可以被放松，代价是大概会给 regret bound 带来常数倍的影响。只要 regret 增长速度小于 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> （比如这里是 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BT%7D" alt="\sqrt{T}" eeimg="1"/> ），就说明当轮数变多的时候，每轮和最好专家之间产生的 regret 差距逐渐趋向于零。</p><h3>2. POLITEX 算法框架</h3><p>受到 EWA 算法的启发，文章也希望采取类似的 Boltzmann policy，并且试图分析一下能不能得到相应的 regret bound。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-a29aecc2f4c941346391a8ef6343e7a0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1382" data-rawheight="676" class="origin_image zh-lightbox-thumb" width="1382" data-original="https://pic1.zhimg.com/v2-a29aecc2f4c941346391a8ef6343e7a0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1382&#39; height=&#39;676&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1382" data-rawheight="676" class="origin_image zh-lightbox-thumb lazy" width="1382" data-original="https://pic1.zhimg.com/v2-a29aecc2f4c941346391a8ef6343e7a0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a29aecc2f4c941346391a8ef6343e7a0_b.jpg"/></figure><p>注意到，这里研究的是离散动作空间的问题，可以认为每个 action 都是一个专家。在某状态下，选择某个 action（专家）的概率取决于该 action 的历史表现。</p><p>该算法分为 <img src="https://www.zhihu.com/equation?tex=i%5Cin%5BE%5D" alt="i\in[E]" eeimg="1"/> 个 iteration，每个 iteration 都需要采样 <img src="https://www.zhihu.com/equation?tex=%5Ctau" alt="\tau" eeimg="1"/> 个样本，假设总共采样 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 个样本。采样到第 <img src="https://www.zhihu.com/equation?tex=t%5Cin%5BT%5D" alt="t\in[T]" eeimg="1"/> 个样本的时候，对应的策略记为 <img src="https://www.zhihu.com/equation?tex=%5Cpi_%7B%28t%29%7D" alt="\pi_{(t)}" eeimg="1"/> ；第 <img src="https://www.zhihu.com/equation?tex=i%5Cin%5BE%5D" alt="i\in[E]" eeimg="1"/> 个 iteration 的策略记为 <img src="https://www.zhihu.com/equation?tex=%5Cpi_i" alt="\pi_i" eeimg="1"/> 。按照上述算法，采样 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 步，每一步的 cost 记做 <img src="https://www.zhihu.com/equation?tex=c%28x_t%2Ca_t%29" alt="c(x_t,a_t)" eeimg="1"/> 。假设有一个最优策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%5E%2A" alt="\pi^*" eeimg="1"/> ，采样 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 步，每一步的 cost 记做 <img src="https://www.zhihu.com/equation?tex=c%28x%5E%2A_t%2Ca%5E%2A_t%29" alt="c(x^*_t,a^*_t)" eeimg="1"/> 。</p><p>和 expert problem 不同的是：在 expert problem 里面，即使没有选择其他的专家，也告知假如选择其他专家而产生的 loss；而在强化学习里面，并不会告知其他 action 产生的 cumulative cost。因此这里会使用估计的价值函数来代替实际的 cumulative cost，即算法中的 <img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D" alt="\hat{Q}" eeimg="1"/> 。每一轮， <img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D_i" alt="\hat{Q}_i" eeimg="1"/> 的目标是估计相对于策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi_i" alt="\pi_i" eeimg="1"/> 的价值函数。</p><p>该算法的目标就是最小化如下 regret：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathfrak%7BR%7D_T+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5B+c%28x_t%2C+a_t%29+-+c%28x_t%5E%2A%2C+a_t%5E%2A%29%5D" alt="\mathfrak{R}_T = \sum_{t=1}^T [ c(x_t, a_t) - c(x_t^*, a_t^*)]" eeimg="1"/> </p><h3>3. MDP 设定</h3><p>该工作使用的是 infinite horizon average cost 的设定，而不是我个人比较习惯的 infinite horizon discounted reward 的设定，不过两者基本上是等价的。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-08e4bdf28342f642d475c20bd9e85787_b.png" data-caption="" data-size="normal" data-rawwidth="1116" data-rawheight="146" class="origin_image zh-lightbox-thumb" width="1116" data-original="https://pic4.zhimg.com/v2-08e4bdf28342f642d475c20bd9e85787_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1116&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1116" data-rawheight="146" class="origin_image zh-lightbox-thumb lazy" width="1116" data-original="https://pic4.zhimg.com/v2-08e4bdf28342f642d475c20bd9e85787_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-08e4bdf28342f642d475c20bd9e85787_b.png"/></figure><p>相当于是 performance measure。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-5e8d20af09ce48d3d843003d06821ea3_b.png" data-caption="" data-size="normal" data-rawwidth="1122" data-rawheight="156" class="origin_image zh-lightbox-thumb" width="1122" data-original="https://pic4.zhimg.com/v2-5e8d20af09ce48d3d843003d06821ea3_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1122&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1122" data-rawheight="156" class="origin_image zh-lightbox-thumb lazy" width="1122" data-original="https://pic4.zhimg.com/v2-5e8d20af09ce48d3d843003d06821ea3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-5e8d20af09ce48d3d843003d06821ea3_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-294a05a32bd30d83e3081a079083b67d_b.png" data-caption="" data-size="normal" data-rawwidth="1096" data-rawheight="66" class="origin_image zh-lightbox-thumb" width="1096" data-original="https://pic2.zhimg.com/v2-294a05a32bd30d83e3081a079083b67d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1096&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1096" data-rawheight="66" class="origin_image zh-lightbox-thumb lazy" width="1096" data-original="https://pic2.zhimg.com/v2-294a05a32bd30d83e3081a079083b67d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-294a05a32bd30d83e3081a079083b67d_b.png"/></figure><p>V 函数和 Q 函数的定义。</p><h3>4. Regret bound：重要的部分</h3><p>第一步，对于 regret 进行如下拆分。这也算是分析的常见思路，要 bound 一个随机变量，把它拆成“均值”和“随机变量 - 均值”的形式，NNQL 分析也是类似的思路。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-e6c0413ee0f9b6432e64e4159a30e4df_b.jpg" data-caption="" data-size="normal" data-rawwidth="1402" data-rawheight="506" class="origin_image zh-lightbox-thumb" width="1402" data-original="https://pic4.zhimg.com/v2-e6c0413ee0f9b6432e64e4159a30e4df_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1402&#39; height=&#39;506&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1402" data-rawheight="506" class="origin_image zh-lightbox-thumb lazy" width="1402" data-original="https://pic4.zhimg.com/v2-e6c0413ee0f9b6432e64e4159a30e4df_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e6c0413ee0f9b6432e64e4159a30e4df_b.jpg"/></figure><p>“随机变量 - 均值”总是能在各种假设条件被 bound 的，因此这里面最关键的还是需要 bound <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Cmathfrak%7BR%7D%7D_T" alt="\bar{\mathfrak{R}}_T" eeimg="1"/> 。关于这一项的上界，先放出结论：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-98cccdffee6196336d040c93afaf8a7a_b.jpg" data-caption="" data-size="normal" data-rawwidth="1352" data-rawheight="820" class="origin_image zh-lightbox-thumb" width="1352" data-original="https://pic3.zhimg.com/v2-98cccdffee6196336d040c93afaf8a7a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1352&#39; height=&#39;820&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1352" data-rawheight="820" class="origin_image zh-lightbox-thumb lazy" width="1352" data-original="https://pic3.zhimg.com/v2-98cccdffee6196336d040c93afaf8a7a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-98cccdffee6196336d040c93afaf8a7a_b.jpg"/></figure><p>假设部分显然是必要的，因为 regret 的衡量标准是基于策略在 MDP 上性能的，而策略的输入只是对于性能的估计 <img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D" alt="\hat{Q}" eeimg="1"/> ，因此要有有用的结论，起码需要假设性能的估计比较准确。这里使用 <img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon%28%5Cdelta%2C+%5Ctau%29" alt="\varepsilon(\delta, \tau)" eeimg="1"/> 来表征其估计的准确性， <img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="\delta" eeimg="1"/> 表示相应的概率， <img src="https://www.zhihu.com/equation?tex=%5Ctau" alt="\tau" eeimg="1"/> 表示估计 Q 函数所使用的样本长度，显然，样本越多，estimation error 越小，估计越准确。</p><p>下面开始证明：</p><p>首先观察到</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-5680739f420329b38df284948b2f3893_b.png" data-caption="" data-size="normal" data-rawwidth="1280" data-rawheight="96" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic4.zhimg.com/v2-5680739f420329b38df284948b2f3893_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1280&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1280" data-rawheight="96" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic4.zhimg.com/v2-5680739f420329b38df284948b2f3893_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-5680739f420329b38df284948b2f3893_b.png"/></figure><p>在 discounted reward setting 下，该公式我们非常的熟悉，就是 TRPO paper 里面的 (1) 式，在 02 年 Kakade&amp;Lanford 的 paper 里面也有提到。考虑到策略使用的是 <img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D" alt="\hat{Q}" eeimg="1"/> ，只有替换成 <img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D" alt="\hat{Q}" eeimg="1"/> 才能讨论 EWA 相关的结论。因此，做如下拆分：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-402df1b8a74894805fc2d433ac1a5a22_b.jpg" data-caption="" data-size="normal" data-rawwidth="1266" data-rawheight="428" class="origin_image zh-lightbox-thumb" width="1266" data-original="https://pic3.zhimg.com/v2-402df1b8a74894805fc2d433ac1a5a22_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1266&#39; height=&#39;428&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1266" data-rawheight="428" class="origin_image zh-lightbox-thumb lazy" width="1266" data-original="https://pic3.zhimg.com/v2-402df1b8a74894805fc2d433ac1a5a22_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-402df1b8a74894805fc2d433ac1a5a22_b.jpg"/></figure><p>第一项可以套用 EWA 的结论，第二项可以套用定理中的假设。</p><p>对于第二项，使用定理中的假设，再加上 union bound（注意到，要求对于 <img src="https://www.zhihu.com/equation?tex=E" alt="E" eeimg="1"/> 个不同的策略，要求<i>每一个</i>都以大概率小于某个值），即可得到：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-e92c217d573cea3c7590f35430c3e9ff_b.png" data-caption="" data-size="normal" data-rawwidth="1224" data-rawheight="68" class="origin_image zh-lightbox-thumb" width="1224" data-original="https://pic4.zhimg.com/v2-e92c217d573cea3c7590f35430c3e9ff_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1224&#39; height=&#39;68&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1224" data-rawheight="68" class="origin_image zh-lightbox-thumb lazy" width="1224" data-original="https://pic4.zhimg.com/v2-e92c217d573cea3c7590f35430c3e9ff_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e92c217d573cea3c7590f35430c3e9ff_b.png"/></figure><p>对于第一项，目标是套用 EWA 的结论。EWA 中损失函数需要在 <img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[0,1]" eeimg="1"/> 区间内，因此这里对 <img src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D" alt="\hat{Q}" eeimg="1"/> 做 scale；同时每个 iteration 的 <img src="https://www.zhihu.com/equation?tex=%5Ctau" alt="\tau" eeimg="1"/> 步都是一样的，因此可以拆为 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bt%3D1%7D%5ET+%5Cto+%5Cle+%5Ctau+%5Csum_%7Bi%3D1%7D%5EE" alt="\sum_{t=1}^T \to \le \tau \sum_{i=1}^E" eeimg="1"/> 。令， <img src="https://www.zhihu.com/equation?tex=l_%7Bi%2Ca%7D%3D%28%5Chat%7BQ%7D_i%28x%2Ca%29+-+b%29%2FQ_%5Cmax" alt="l_{i,a}=(\hat{Q}_i(x,a) - b)/Q_\max" eeimg="1"/> ，套用 EWA 结论，有：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-ba02f90dceb75c06c99dfba26cd5f7b9_b.jpg" data-caption="" data-size="normal" data-rawwidth="1228" data-rawheight="222" class="origin_image zh-lightbox-thumb" width="1228" data-original="https://pic2.zhimg.com/v2-ba02f90dceb75c06c99dfba26cd5f7b9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1228&#39; height=&#39;222&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1228" data-rawheight="222" class="origin_image zh-lightbox-thumb lazy" width="1228" data-original="https://pic2.zhimg.com/v2-ba02f90dceb75c06c99dfba26cd5f7b9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ba02f90dceb75c06c99dfba26cd5f7b9_b.jpg"/></figure><p>下面是个人感觉比较难的一步（费了我一上午。。文章就单说使用 union bound。。）：对于每个状态 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"/> 都大概率有关于 <img src="https://www.zhihu.com/equation?tex=R%28x%29" alt="R(x)" eeimg="1"/> 的上界，特别地， <img src="https://www.zhihu.com/equation?tex=Pr%5BR%28x%29+%5Cge+%5Cepsilon%5D+%5Cle+%5Cexp%28-%5Cepsilon%5E2%29" alt="Pr[R(x) \ge \epsilon] \le \exp(-\epsilon^2)" eeimg="1"/> ，那么对于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bx%5Csim+%5Cmu%7D+%5BR%28x%29%5D" alt="\mathbb{E}_{x\sim \mu} [R(x)]" eeimg="1"/> 的上界应为多少？</p><p>首先，对于任意的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon%28x%29%2C+s.t.+%5Csum_x+%5Cepsilon%28x%29+%5Cmu%28x%29+%3D+%5Cepsilon" alt="\epsilon(x), s.t. \sum_x \epsilon(x) \mu(x) = \epsilon" eeimg="1"/> ，都有</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+Pr%5B%5Cmathbb%7BE%7D_%7Bx%5Csim+%5Cmu%7D+R%28x%29+%5Cge+%5Cepsilon%5D+%26+%5Cle+Pr%5B%5Cexists+x%2C+%5Cmu%28x%29+%3E0%2C+R%28x%29+%5Cge+%5Cepsilon%28x%29%5D+%5C%5C+%26+%5Cle++%5Csum_x+Pr%5BR%28x%29+%5Cge+%5Cepsilon%28x%29%5D+%5Cquad+%5Ctext%7B%28union+bound%29%7D%5C%5C++%26+%5Cle+%5Csum_x+%5Cexp%28-%5Cepsilon%5E2%28x%29%29+%3D+%5Cdelta+%3D+%5Csum_x+%5Cmu%28x%29+%5Cdelta+%5C%5C+%26+%5Cle+%5Cmathbb%7BE%7D_%7Bx%5Csim+%5Cmu%7D%5B%5Csqrt%7B%5Clog+%281%2F%5Cmu%28x%29%5Cdelta%29%7D%5D+%5Cquad+%5Ctext%7B%28Let+%7D+%5Cepsilon%28x%29+%3D+%5Csqrt%7B%5Clog+%281%2F%5Cmu%28x%29%5Cdelta%29%7D+%5Ctext%7B%29%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} Pr[\mathbb{E}_{x\sim \mu} R(x) \ge \epsilon] &amp; \le Pr[\exists x, \mu(x) &gt;0, R(x) \ge \epsilon(x)] \\ &amp; \le  \sum_x Pr[R(x) \ge \epsilon(x)] \quad \text{(union bound)}\\  &amp; \le \sum_x \exp(-\epsilon^2(x)) = \delta = \sum_x \mu(x) \delta \\ &amp; \le \mathbb{E}_{x\sim \mu}[\sqrt{\log (1/\mu(x)\delta)}] \quad \text{(Let } \epsilon(x) = \sqrt{\log (1/\mu(x)\delta)} \text{)} \end{aligned}" eeimg="1"/> </p><p>最后可以得到</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-b3fdbb0a1b8291e8188c9bf491fae0db_b.png" data-caption="" data-size="normal" data-rawwidth="1408" data-rawheight="166" class="origin_image zh-lightbox-thumb" width="1408" data-original="https://pic4.zhimg.com/v2-b3fdbb0a1b8291e8188c9bf491fae0db_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1408&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1408" data-rawheight="166" class="origin_image zh-lightbox-thumb lazy" width="1408" data-original="https://pic4.zhimg.com/v2-b3fdbb0a1b8291e8188c9bf491fae0db_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b3fdbb0a1b8291e8188c9bf491fae0db_b.png"/></figure><h3>5. Regret bound：次要的部分</h3><p>次要的部分就是要 bound <img src="https://www.zhihu.com/equation?tex=V_T%2C+W_T" alt="V_T, W_T" eeimg="1"/> 两项：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-6a7d7c680daa0fa36de8ef4be0bdf3e7_b.jpg" data-caption="" data-size="normal" data-rawwidth="1372" data-rawheight="276" class="origin_image zh-lightbox-thumb" width="1372" data-original="https://pic4.zhimg.com/v2-6a7d7c680daa0fa36de8ef4be0bdf3e7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1372&#39; height=&#39;276&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1372" data-rawheight="276" class="origin_image zh-lightbox-thumb lazy" width="1372" data-original="https://pic4.zhimg.com/v2-6a7d7c680daa0fa36de8ef4be0bdf3e7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-6a7d7c680daa0fa36de8ef4be0bdf3e7_b.jpg"/></figure><p>从 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> 的定义可以看到：如果每一轮中样本数目趋向无穷的时候， <img src="https://www.zhihu.com/equation?tex=V_T" alt="V_T" eeimg="1"/> 应该趋向零；如果总样本数 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 趋向无穷的时候， <img src="https://www.zhihu.com/equation?tex=W_T" alt="W_T" eeimg="1"/> 趋向零。在样本有限的时候，状态分布更多地类似于初始状态分布（暂态）可能有别于稳态状态，这会导致 <img src="https://www.zhihu.com/equation?tex=V_T%2C+W_T" alt="V_T, W_T" eeimg="1"/> 两项绝对数值较大。因此，需要做暂态状态消失地足够快的假设：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-fce7c98f8748d8694a40de20a15ef4c2_b.png" data-caption="" data-size="normal" data-rawwidth="1416" data-rawheight="216" class="origin_image zh-lightbox-thumb" width="1416" data-original="https://pic3.zhimg.com/v2-fce7c98f8748d8694a40de20a15ef4c2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1416&#39; height=&#39;216&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1416" data-rawheight="216" class="origin_image zh-lightbox-thumb lazy" width="1416" data-original="https://pic3.zhimg.com/v2-fce7c98f8748d8694a40de20a15ef4c2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-fce7c98f8748d8694a40de20a15ef4c2_b.png"/></figure><p>在此假设下能够得到结论：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-503cb2d8e43180e6519b202cc0d9728a_b.jpg" data-caption="" data-size="normal" data-rawwidth="1476" data-rawheight="274" class="origin_image zh-lightbox-thumb" width="1476" data-original="https://pic3.zhimg.com/v2-503cb2d8e43180e6519b202cc0d9728a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1476&#39; height=&#39;274&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1476" data-rawheight="274" class="origin_image zh-lightbox-thumb lazy" width="1476" data-original="https://pic3.zhimg.com/v2-503cb2d8e43180e6519b202cc0d9728a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-503cb2d8e43180e6519b202cc0d9728a_b.jpg"/></figure><p>暂态消失地足够快时， <img src="https://www.zhihu.com/equation?tex=%5Ckappa" alt="\kappa" eeimg="1"/> 比较小， <img src="https://www.zhihu.com/equation?tex=V_T%2C+W_T" alt="V_T, W_T" eeimg="1"/> 两项的数值也比较小；同时该误差也随着轨迹长度的增长而 sublinear 地增长，即步数越长，平均每步产生的误差越小；注意到 <img src="https://www.zhihu.com/equation?tex=V_T" alt="V_T" eeimg="1"/> 描述的策略，每隔 <img src="https://www.zhihu.com/equation?tex=%5Ctau" alt="\tau" eeimg="1"/> 步都会变化，因此还需要套一个 union bound，得到的结果含有 <img src="https://www.zhihu.com/equation?tex=E" alt="E" eeimg="1"/> 。</p><h3>6. 如何估计 Q 函数</h3><p>注意到，策略需要使用以前所有的 Q 函数估计，这一点不很友好。但是如果价值函数的估计使用的是线性函数拟合，就可以累积地维护历史上 Q 估计的和了。文章分析了一个特殊的线性函数估计方法 LSPE，得到了前面公式中 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon%28%5Cdelta%2C+%5Ctau%29" alt="\epsilon(\delta, \tau)" eeimg="1"/> 的具体表达形式。联合起来，得到了 LSPE + POLITEX 的 regret bound：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-80a4714ed83e70a400415143d289b3d3_b.jpg" data-caption="" data-size="normal" data-rawwidth="1408" data-rawheight="416" class="origin_image zh-lightbox-thumb" width="1408" data-original="https://pic4.zhimg.com/v2-80a4714ed83e70a400415143d289b3d3_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1408&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1408" data-rawheight="416" class="origin_image zh-lightbox-thumb lazy" width="1408" data-original="https://pic4.zhimg.com/v2-80a4714ed83e70a400415143d289b3d3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-80a4714ed83e70a400415143d289b3d3_b.jpg"/></figure><p>其中一直存在的误差 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon_0" alt="\epsilon_0" eeimg="1"/> 表示的是线性函数拟合的 approximation error；除此之外，regret 的增长速度大致为 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BO%7D%28T%5E%7B3%2F4%7D%29" alt="\tilde{O}(T^{3/4})" eeimg="1"/> 。这一部分的分析在附录中，很长，个人不太感兴趣，就没仔细看。</p><p>在实际中，可以使用神经网络来作为 Q 的估计，这样需要把历史上的神经网络全部存下来。实际操作中可以只存最近的若干个。</p><p>回头看一下 POLITEX，如果策略不使用历史上所有 Q 函数估计的和，而是只使用上一轮的 Q 函数估计，该方法几乎是一个 soft 版本（使用 Boltzmann policy）的 Q-learning。相比于 Q-learning，它使用了前些轮估计的 Q 函数，实验效果看，更稳定。</p><p>在实验上，使用神经网络估计 Q 函数，相比于 DQN 效果更好。实验环境为 Ms Pacman。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-91301ea01df32cd14640e5c4d7f24ee4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1276" data-rawheight="740" class="origin_image zh-lightbox-thumb" width="1276" data-original="https://pic1.zhimg.com/v2-91301ea01df32cd14640e5c4d7f24ee4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1276&#39; height=&#39;740&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1276" data-rawheight="740" class="origin_image zh-lightbox-thumb lazy" width="1276" data-original="https://pic1.zhimg.com/v2-91301ea01df32cd14640e5c4d7f24ee4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-91301ea01df32cd14640e5c4d7f24ee4_b.jpg"/></figure><p></p></div></div><div class="ContentItem-time">发布于 2019-07-21</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 12 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 12</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="01acffa8-62f5-48cc-b327-796f1aa90be7" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="01acffa8-62f5-48cc-b327-796f1aa90be7">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"74512862":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":74512862,"title":"【强化学习 82】POLITEX","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F74512862","imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8ebbeb1c6b7a7ba82b472a58a7d51e9f_b.jpg","titleImage":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8ebbeb1c6b7a7ba82b472a58a7d51e9f_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7247a2ec850918124ae61a026750c567_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1458\" data-rawheight=\"270\" data-watermark=\"watermark\" data-original-src=\"v2-7247a2ec850918124ae61a026750c567\" data-watermark-src=\"v2-d7b86740e296113b999b50cc1a04cbe4\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7247a2ec850918124ae61a026750c567_r.png\"\u002F\u003EPOLITEX 全称是 POLicy ITeration with EXpert advice。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fproceedings.mlr.press\u002Fv97\u002Flazic19a\u002Flazic19a-supp.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EAbbasi-Yadkori, Yasin, et al. &#34;Politex: Regret Bounds for Policy Iteration using Expert Prediction.&#34; (2019).\u003C\u002Fa\u003E特色使用 expert problem 研究中的 exponential weighted average（EWA）…","created":1563677754,"updated":1563677754,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":2180,"imageHeight":626,"content":"\u003Cp\u003EPOLITEX 全称是 POLicy ITeration with EXpert advice。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fproceedings.mlr.press\u002Fv97\u002Flazic19a\u002Flazic19a-supp.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EAbbasi-Yadkori, Yasin, et al. &#34;Politex: Regret Bounds for Policy Iteration using Expert Prediction.&#34; (2019).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E使用 expert problem 研究中的 exponential weighted average（EWA）算法，得到了一个强化学习策略，并且得到了该算法的 regret bound。注意到对 regret bound 的分析等价于 finite sample analysis。该算法可以使用任意的 Q 函数估计。文章还证明了对于 Q 函数的线性函数拟合，使用 least-squares policy evaluation（LSPE）方法来估计参数，有相应的估计误差的上界。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1. Expert problem and EWA\u003C\u002Fh3\u003E\u003Cp\u003E这里的 expert problem 设定如下。这是一个多轮的游戏，每一轮分为如下几步： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A\" alt=\"A\" eeimg=\"1\"\u002F\u003E 个专家分别给出对于未来的预测建议（advice） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f_%7Bi%2Ct%7D+%5Cin+%5Cmathcal%7BD%7D\" alt=\"f_{i,t} \\in \\mathcal{D}\" eeimg=\"1\"\u002F\u003E ，属于 decision space；玩家依照一定的策略，挑选出一个专家 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=I_t%5Cin%5BA%5D\" alt=\"I_t\\in[A]\" eeimg=\"1\"\u002F\u003E 并且采取它给出的建议；环境揭示该时刻的情形 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y_t+%5Cin+%5Cmathcal%7BO%7D\" alt=\"y_t \\in \\mathcal{O}\" eeimg=\"1\"\u002F\u003E ，属于 outcome space；接下来，对于专家给出的每一个建议，都能够通过损失函数计算出相应的损失，该损失可以使用一个向量来表示 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l_t%5Cin%5B0%2C+1%5D%5EA\" alt=\"l_t\\in[0, 1]^A\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003EExpert problem 一般考虑环境可能是 adversarial，甚至 expert 也是不靠谱的，如果目标是减小玩家的损失，那么只要 expert 总是给出不靠谱的“建议”，那么玩家不可能有很小的损失。然而，退一步地，expert problem 的目标是最小化玩家损失和任意一个专家（或者可以理解为最好的那个专家）损失差距。注意到，我们一开始并不知道那个专家是最好的，甚至专家也可能是 adversarial ，根据你做的决策而改变。具体来说，优化目标为最小化 regret：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathfrak%7BR%7D_%7BT%2Cj%7D%3D%5Csum_%7Bt%3D1%7D%5ET+%28l_%7Bt%2C+I_t%7D+-+l_%7Bt%2Cj%7D%29\" alt=\"\\mathfrak{R}_{T,j}=\\sum_{t=1}^T (l_{t, I_t} - l_{t,j})\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E表示截止第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 轮时，玩家累积受到的损失相比于第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E 个专家受到的累积损失的差距。\u003C\u002Fp\u003E\u003Cp\u003EEWA 每次按照一定概率选择相应的专家，即使用一个随机策略，选择第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i\" alt=\"i\" eeimg=\"1\"\u002F\u003E 个专家的概率为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_t%28i%29+%5Cpropto+%5Cexp%28-%5Ceta+%5Csum_%7Bs%3D1%7D%5E%7Bt-1%7D+l_%7Bs%2Ci%7D%29\" alt=\"\\pi_t(i) \\propto \\exp(-\\eta \\sum_{s=1}^{t-1} l_{s,i})\" eeimg=\"1\"\u002F\u003E 。即，如果该专家历史上表现都比较好，就会以比较高的概率选择该专家。这个策略也叫做 Boltzmann policy。如果使用该策略，可以证明，当时间较长的时候，玩家遭受的损失基本上和最好专家遭受的损失差不多。具体地，见如下定理：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d7b86740e296113b999b50cc1a04cbe4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1458\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"1458\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d7b86740e296113b999b50cc1a04cbe4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1458&#39; height=&#39;270&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1458\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1458\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d7b86740e296113b999b50cc1a04cbe4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d7b86740e296113b999b50cc1a04cbe4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到，参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"\u002F\u003E 的选择要求我们实现知道游戏需要玩多少轮，这个条件可以被放松，代价是大概会给 regret bound 带来常数倍的影响。只要 regret 增长速度小于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E （比如这里是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csqrt%7BT%7D\" alt=\"\\sqrt{T}\" eeimg=\"1\"\u002F\u003E ），就说明当轮数变多的时候，每轮和最好专家之间产生的 regret 差距逐渐趋向于零。\u003C\u002Fp\u003E\u003Ch3\u003E2. POLITEX 算法框架\u003C\u002Fh3\u003E\u003Cp\u003E受到 EWA 算法的启发，文章也希望采取类似的 Boltzmann policy，并且试图分析一下能不能得到相应的 regret bound。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a29aecc2f4c941346391a8ef6343e7a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1382\" data-rawheight=\"676\" class=\"origin_image zh-lightbox-thumb\" width=\"1382\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a29aecc2f4c941346391a8ef6343e7a0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1382&#39; height=&#39;676&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1382\" data-rawheight=\"676\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1382\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a29aecc2f4c941346391a8ef6343e7a0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a29aecc2f4c941346391a8ef6343e7a0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到，这里研究的是离散动作空间的问题，可以认为每个 action 都是一个专家。在某状态下，选择某个 action（专家）的概率取决于该 action 的历史表现。\u003C\u002Fp\u003E\u003Cp\u003E该算法分为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i%5Cin%5BE%5D\" alt=\"i\\in[E]\" eeimg=\"1\"\u002F\u003E 个 iteration，每个 iteration 都需要采样 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"\u002F\u003E 个样本，假设总共采样 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 个样本。采样到第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t%5Cin%5BT%5D\" alt=\"t\\in[T]\" eeimg=\"1\"\u002F\u003E 个样本的时候，对应的策略记为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_%7B%28t%29%7D\" alt=\"\\pi_{(t)}\" eeimg=\"1\"\u002F\u003E ；第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i%5Cin%5BE%5D\" alt=\"i\\in[E]\" eeimg=\"1\"\u002F\u003E 个 iteration 的策略记为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_i\" alt=\"\\pi_i\" eeimg=\"1\"\u002F\u003E 。按照上述算法，采样 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 步，每一步的 cost 记做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c%28x_t%2Ca_t%29\" alt=\"c(x_t,a_t)\" eeimg=\"1\"\u002F\u003E 。假设有一个最优策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%5E%2A\" alt=\"\\pi^*\" eeimg=\"1\"\u002F\u003E ，采样 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 步，每一步的 cost 记做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c%28x%5E%2A_t%2Ca%5E%2A_t%29\" alt=\"c(x^*_t,a^*_t)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E和 expert problem 不同的是：在 expert problem 里面，即使没有选择其他的专家，也告知假如选择其他专家而产生的 loss；而在强化学习里面，并不会告知其他 action 产生的 cumulative cost。因此这里会使用估计的价值函数来代替实际的 cumulative cost，即算法中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D\" alt=\"\\hat{Q}\" eeimg=\"1\"\u002F\u003E 。每一轮， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D_i\" alt=\"\\hat{Q}_i\" eeimg=\"1\"\u002F\u003E 的目标是估计相对于策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_i\" alt=\"\\pi_i\" eeimg=\"1\"\u002F\u003E 的价值函数。\u003C\u002Fp\u003E\u003Cp\u003E该算法的目标就是最小化如下 regret：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathfrak%7BR%7D_T+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5B+c%28x_t%2C+a_t%29+-+c%28x_t%5E%2A%2C+a_t%5E%2A%29%5D\" alt=\"\\mathfrak{R}_T = \\sum_{t=1}^T [ c(x_t, a_t) - c(x_t^*, a_t^*)]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Ch3\u003E3. MDP 设定\u003C\u002Fh3\u003E\u003Cp\u003E该工作使用的是 infinite horizon average cost 的设定，而不是我个人比较习惯的 infinite horizon discounted reward 的设定，不过两者基本上是等价的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-08e4bdf28342f642d475c20bd9e85787_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"1116\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-08e4bdf28342f642d475c20bd9e85787_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1116&#39; height=&#39;146&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1116\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-08e4bdf28342f642d475c20bd9e85787_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-08e4bdf28342f642d475c20bd9e85787_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E相当于是 performance measure。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5e8d20af09ce48d3d843003d06821ea3_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1122\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5e8d20af09ce48d3d843003d06821ea3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1122&#39; height=&#39;156&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1122\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5e8d20af09ce48d3d843003d06821ea3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5e8d20af09ce48d3d843003d06821ea3_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-294a05a32bd30d83e3081a079083b67d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1096\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"1096\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-294a05a32bd30d83e3081a079083b67d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1096&#39; height=&#39;66&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1096\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1096\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-294a05a32bd30d83e3081a079083b67d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-294a05a32bd30d83e3081a079083b67d_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EV 函数和 Q 函数的定义。\u003C\u002Fp\u003E\u003Ch3\u003E4. Regret bound：重要的部分\u003C\u002Fh3\u003E\u003Cp\u003E第一步，对于 regret 进行如下拆分。这也算是分析的常见思路，要 bound 一个随机变量，把它拆成“均值”和“随机变量 - 均值”的形式，NNQL 分析也是类似的思路。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e6c0413ee0f9b6432e64e4159a30e4df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb\" width=\"1402\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e6c0413ee0f9b6432e64e4159a30e4df_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1402&#39; height=&#39;506&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1402\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e6c0413ee0f9b6432e64e4159a30e4df_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e6c0413ee0f9b6432e64e4159a30e4df_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E“随机变量 - 均值”总是能在各种假设条件被 bound 的，因此这里面最关键的还是需要 bound \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Cmathfrak%7BR%7D%7D_T\" alt=\"\\bar{\\mathfrak{R}}_T\" eeimg=\"1\"\u002F\u003E 。关于这一项的上界，先放出结论：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98cccdffee6196336d040c93afaf8a7a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb\" width=\"1352\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98cccdffee6196336d040c93afaf8a7a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1352&#39; height=&#39;820&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1352\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98cccdffee6196336d040c93afaf8a7a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-98cccdffee6196336d040c93afaf8a7a_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E假设部分显然是必要的，因为 regret 的衡量标准是基于策略在 MDP 上性能的，而策略的输入只是对于性能的估计 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D\" alt=\"\\hat{Q}\" eeimg=\"1\"\u002F\u003E ，因此要有有用的结论，起码需要假设性能的估计比较准确。这里使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cvarepsilon%28%5Cdelta%2C+%5Ctau%29\" alt=\"\\varepsilon(\\delta, \\tau)\" eeimg=\"1\"\u002F\u003E 来表征其估计的准确性， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta\" alt=\"\\delta\" eeimg=\"1\"\u002F\u003E 表示相应的概率， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"\u002F\u003E 表示估计 Q 函数所使用的样本长度，显然，样本越多，estimation error 越小，估计越准确。\u003C\u002Fp\u003E\u003Cp\u003E下面开始证明：\u003C\u002Fp\u003E\u003Cp\u003E首先观察到\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5680739f420329b38df284948b2f3893_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5680739f420329b38df284948b2f3893_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1280&#39; height=&#39;96&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1280\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5680739f420329b38df284948b2f3893_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5680739f420329b38df284948b2f3893_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在 discounted reward setting 下，该公式我们非常的熟悉，就是 TRPO paper 里面的 (1) 式，在 02 年 Kakade&amp;Lanford 的 paper 里面也有提到。考虑到策略使用的是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D\" alt=\"\\hat{Q}\" eeimg=\"1\"\u002F\u003E ，只有替换成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D\" alt=\"\\hat{Q}\" eeimg=\"1\"\u002F\u003E 才能讨论 EWA 相关的结论。因此，做如下拆分：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-402df1b8a74894805fc2d433ac1a5a22_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb\" width=\"1266\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-402df1b8a74894805fc2d433ac1a5a22_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1266&#39; height=&#39;428&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1266\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-402df1b8a74894805fc2d433ac1a5a22_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-402df1b8a74894805fc2d433ac1a5a22_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E第一项可以套用 EWA 的结论，第二项可以套用定理中的假设。\u003C\u002Fp\u003E\u003Cp\u003E对于第二项，使用定理中的假设，再加上 union bound（注意到，要求对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=E\" alt=\"E\" eeimg=\"1\"\u002F\u003E 个不同的策略，要求\u003Ci\u003E每一个\u003C\u002Fi\u003E都以大概率小于某个值），即可得到：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e92c217d573cea3c7590f35430c3e9ff_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb\" width=\"1224\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e92c217d573cea3c7590f35430c3e9ff_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1224&#39; height=&#39;68&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1224\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e92c217d573cea3c7590f35430c3e9ff_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e92c217d573cea3c7590f35430c3e9ff_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E对于第一项，目标是套用 EWA 的结论。EWA 中损失函数需要在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5B0%2C1%5D\" alt=\"[0,1]\" eeimg=\"1\"\u002F\u003E 区间内，因此这里对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BQ%7D\" alt=\"\\hat{Q}\" eeimg=\"1\"\u002F\u003E 做 scale；同时每个 iteration 的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"\u002F\u003E 步都是一样的，因此可以拆为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_%7Bt%3D1%7D%5ET+%5Cto+%5Cle+%5Ctau+%5Csum_%7Bi%3D1%7D%5EE\" alt=\"\\sum_{t=1}^T \\to \\le \\tau \\sum_{i=1}^E\" eeimg=\"1\"\u002F\u003E 。令， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=l_%7Bi%2Ca%7D%3D%28%5Chat%7BQ%7D_i%28x%2Ca%29+-+b%29%2FQ_%5Cmax\" alt=\"l_{i,a}=(\\hat{Q}_i(x,a) - b)\u002FQ_\\max\" eeimg=\"1\"\u002F\u003E ，套用 EWA 结论，有：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba02f90dceb75c06c99dfba26cd5f7b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1228\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb\" width=\"1228\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba02f90dceb75c06c99dfba26cd5f7b9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1228&#39; height=&#39;222&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1228\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1228\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba02f90dceb75c06c99dfba26cd5f7b9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba02f90dceb75c06c99dfba26cd5f7b9_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E下面是个人感觉比较难的一步（费了我一上午。。文章就单说使用 union bound。。）：对于每个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 都大概率有关于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28x%29\" alt=\"R(x)\" eeimg=\"1\"\u002F\u003E 的上界，特别地， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Pr%5BR%28x%29+%5Cge+%5Cepsilon%5D+%5Cle+%5Cexp%28-%5Cepsilon%5E2%29\" alt=\"Pr[R(x) \\ge \\epsilon] \\le \\exp(-\\epsilon^2)\" eeimg=\"1\"\u002F\u003E ，那么对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BE%7D_%7Bx%5Csim+%5Cmu%7D+%5BR%28x%29%5D\" alt=\"\\mathbb{E}_{x\\sim \\mu} [R(x)]\" eeimg=\"1\"\u002F\u003E 的上界应为多少？\u003C\u002Fp\u003E\u003Cp\u003E首先，对于任意的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon%28x%29%2C+s.t.+%5Csum_x+%5Cepsilon%28x%29+%5Cmu%28x%29+%3D+%5Cepsilon\" alt=\"\\epsilon(x), s.t. \\sum_x \\epsilon(x) \\mu(x) = \\epsilon\" eeimg=\"1\"\u002F\u003E ，都有\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+Pr%5B%5Cmathbb%7BE%7D_%7Bx%5Csim+%5Cmu%7D+R%28x%29+%5Cge+%5Cepsilon%5D+%26+%5Cle+Pr%5B%5Cexists+x%2C+%5Cmu%28x%29+%3E0%2C+R%28x%29+%5Cge+%5Cepsilon%28x%29%5D+%5C%5C+%26+%5Cle++%5Csum_x+Pr%5BR%28x%29+%5Cge+%5Cepsilon%28x%29%5D+%5Cquad+%5Ctext%7B%28union+bound%29%7D%5C%5C++%26+%5Cle+%5Csum_x+%5Cexp%28-%5Cepsilon%5E2%28x%29%29+%3D+%5Cdelta+%3D+%5Csum_x+%5Cmu%28x%29+%5Cdelta+%5C%5C+%26+%5Cle+%5Cmathbb%7BE%7D_%7Bx%5Csim+%5Cmu%7D%5B%5Csqrt%7B%5Clog+%281%2F%5Cmu%28x%29%5Cdelta%29%7D%5D+%5Cquad+%5Ctext%7B%28Let+%7D+%5Cepsilon%28x%29+%3D+%5Csqrt%7B%5Clog+%281%2F%5Cmu%28x%29%5Cdelta%29%7D+%5Ctext%7B%29%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} Pr[\\mathbb{E}_{x\\sim \\mu} R(x) \\ge \\epsilon] &amp; \\le Pr[\\exists x, \\mu(x) &gt;0, R(x) \\ge \\epsilon(x)] \\\\ &amp; \\le  \\sum_x Pr[R(x) \\ge \\epsilon(x)] \\quad \\text{(union bound)}\\\\  &amp; \\le \\sum_x \\exp(-\\epsilon^2(x)) = \\delta = \\sum_x \\mu(x) \\delta \\\\ &amp; \\le \\mathbb{E}_{x\\sim \\mu}[\\sqrt{\\log (1\u002F\\mu(x)\\delta)}] \\quad \\text{(Let } \\epsilon(x) = \\sqrt{\\log (1\u002F\\mu(x)\\delta)} \\text{)} \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E最后可以得到\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3fdbb0a1b8291e8188c9bf491fae0db_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1408\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb\" width=\"1408\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3fdbb0a1b8291e8188c9bf491fae0db_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1408&#39; height=&#39;166&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1408\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1408\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3fdbb0a1b8291e8188c9bf491fae0db_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3fdbb0a1b8291e8188c9bf491fae0db_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E5. Regret bound：次要的部分\u003C\u002Fh3\u003E\u003Cp\u003E次要的部分就是要 bound \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T%2C+W_T\" alt=\"V_T, W_T\" eeimg=\"1\"\u002F\u003E 两项：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6a7d7c680daa0fa36de8ef4be0bdf3e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1372\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"1372\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6a7d7c680daa0fa36de8ef4be0bdf3e7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1372&#39; height=&#39;276&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1372\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1372\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6a7d7c680daa0fa36de8ef4be0bdf3e7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6a7d7c680daa0fa36de8ef4be0bdf3e7_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E 的定义可以看到：如果每一轮中样本数目趋向无穷的时候， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T\" alt=\"V_T\" eeimg=\"1\"\u002F\u003E 应该趋向零；如果总样本数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 趋向无穷的时候， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W_T\" alt=\"W_T\" eeimg=\"1\"\u002F\u003E 趋向零。在样本有限的时候，状态分布更多地类似于初始状态分布（暂态）可能有别于稳态状态，这会导致 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T%2C+W_T\" alt=\"V_T, W_T\" eeimg=\"1\"\u002F\u003E 两项绝对数值较大。因此，需要做暂态状态消失地足够快的假设：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fce7c98f8748d8694a40de20a15ef4c2_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1416\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb\" width=\"1416\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fce7c98f8748d8694a40de20a15ef4c2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1416&#39; height=&#39;216&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1416\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1416\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fce7c98f8748d8694a40de20a15ef4c2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-fce7c98f8748d8694a40de20a15ef4c2_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在此假设下能够得到结论：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-503cb2d8e43180e6519b202cc0d9728a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1476\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb\" width=\"1476\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-503cb2d8e43180e6519b202cc0d9728a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1476&#39; height=&#39;274&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1476\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1476\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-503cb2d8e43180e6519b202cc0d9728a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-503cb2d8e43180e6519b202cc0d9728a_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E暂态消失地足够快时， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ckappa\" alt=\"\\kappa\" eeimg=\"1\"\u002F\u003E 比较小， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T%2C+W_T\" alt=\"V_T, W_T\" eeimg=\"1\"\u002F\u003E 两项的数值也比较小；同时该误差也随着轨迹长度的增长而 sublinear 地增长，即步数越长，平均每步产生的误差越小；注意到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T\" alt=\"V_T\" eeimg=\"1\"\u002F\u003E 描述的策略，每隔 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"\u002F\u003E 步都会变化，因此还需要套一个 union bound，得到的结果含有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=E\" alt=\"E\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Ch3\u003E6. 如何估计 Q 函数\u003C\u002Fh3\u003E\u003Cp\u003E注意到，策略需要使用以前所有的 Q 函数估计，这一点不很友好。但是如果价值函数的估计使用的是线性函数拟合，就可以累积地维护历史上 Q 估计的和了。文章分析了一个特殊的线性函数估计方法 LSPE，得到了前面公式中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon%28%5Cdelta%2C+%5Ctau%29\" alt=\"\\epsilon(\\delta, \\tau)\" eeimg=\"1\"\u002F\u003E 的具体表达形式。联合起来，得到了 LSPE + POLITEX 的 regret bound：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-80a4714ed83e70a400415143d289b3d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1408\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"1408\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-80a4714ed83e70a400415143d289b3d3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1408&#39; height=&#39;416&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1408\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1408\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-80a4714ed83e70a400415143d289b3d3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-80a4714ed83e70a400415143d289b3d3_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中一直存在的误差 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon_0\" alt=\"\\epsilon_0\" eeimg=\"1\"\u002F\u003E 表示的是线性函数拟合的 approximation error；除此之外，regret 的增长速度大致为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%28T%5E%7B3%2F4%7D%29\" alt=\"\\tilde{O}(T^{3\u002F4})\" eeimg=\"1\"\u002F\u003E 。这一部分的分析在附录中，很长，个人不太感兴趣，就没仔细看。\u003C\u002Fp\u003E\u003Cp\u003E在实际中，可以使用神经网络来作为 Q 的估计，这样需要把历史上的神经网络全部存下来。实际操作中可以只存最近的若干个。\u003C\u002Fp\u003E\u003Cp\u003E回头看一下 POLITEX，如果策略不使用历史上所有 Q 函数估计的和，而是只使用上一轮的 Q 函数估计，该方法几乎是一个 soft 版本（使用 Boltzmann policy）的 Q-learning。相比于 Q-learning，它使用了前些轮估计的 Q 函数，实验效果看，更稳定。\u003C\u002Fp\u003E\u003Cp\u003E在实验上，使用神经网络估计 Q 函数，相比于 DQN 效果更好。实验环境为 Ms Pacman。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-91301ea01df32cd14640e5c4d7f24ee4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1276\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb\" width=\"1276\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-91301ea01df32cd14640e5c4d7f24ee4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1276&#39; height=&#39;740&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1276\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1276\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-91301ea01df32cd14640e5c4d7f24ee4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-91301ea01df32cd14640e5c4d7f24ee4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":12,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":2,"contributions":[{"id":21322672,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 82】POLITEX - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F74512862 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_bullet_gui-4","expPrefix":"vd_bullet_gui","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-2","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"gw_mweb_launch-2","expPrefix":"gw_mweb_launch","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"web_ask","type":"String","value":"0"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"点我，做第一个上屏的人"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F74512862","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F74512862","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>