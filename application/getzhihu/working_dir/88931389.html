<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 97】Linear MDP - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="机器学习,强化学习 (Reinforcement Learning),深度学习（Deep Learning）"/><meta data-react-helmet="true" name="description" content="对于线性 MDP 模型，可以做线性函数拟合。原文传送门Jin, Chi, et al. &amp;#34;Provably efficient reinforcement learning with linear function approximation.&amp;#34; arXiv preprint arXiv:1907.05388 (2019).特色 …"/><meta data-react-helmet="true" property="og:title" content="【强化学习 97】Linear MDP"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/88931389"/><meta data-react-helmet="true" property="og:description" content="对于线性 MDP 模型，可以做线性函数拟合。原文传送门Jin, Chi, et al. &amp;#34;Provably efficient reinforcement learning with linear function approximation.&amp;#34; arXiv preprint arXiv:1907.05388 (2019).特色 …"/><meta data-react-helmet="true" property="og:image" content="https://pic4.zhimg.com/v2-a08c7211bce17cd63dcfc91c6b3ff6c7_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:88931389,&quot;title&quot;:&quot;【强化学习 97】Linear MDP&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic4.zhimg.com/v2-a08c7211bce17cd63dcfc91c6b3ff6c7_1200x500.jpg" alt="【强化学习 97】Linear MDP"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 97】Linear MDP</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">19 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>对于线性 MDP 模型，可以做线性函数拟合。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1907.05388.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Jin, Chi, et al. &#34;Provably efficient reinforcement learning with linear function approximation.&#34; arXiv preprint arXiv:1907.05388 (2019).</a></p><h2>特色</h2><p> 考虑对价值函数做函数拟合（function approximation）。当函数拟合使用的函数 capacity 大的时候，容易遇到 sparsity 的问题，即所遇到的大多数状态的附近都没有其他样本，则很难估计出一个准确的价值函数（variance 大）；当函数拟合使用的函数 capacity 小的时候，容易遇到 misspecification 的问题，即所使用的函数族不足以表示真实的价值函数（bias 大）。为此，文章提出了线性 MDP 模型，在线性 MDP 模型下，线性的函数拟合不会产生 bias，同时也定量分析了如果稍微偏离线性 MDP 模型，线性函数拟合会产生的 bias 大小。值得注意的是，文章提出的线性 MDP 模型能够概括 tabular case。</p><p>同时，要设计一个有效的（probably efficient）算法，还需要做探索（或称作 exploration-exploitation tradeoff）。探索问题可以想象成如何有效地估计到 transition function / matrix。</p><ul><li>有些算法假设能有一个 simulator，这样能够设置成任意的状态（比如，最简单的那种 tabular Q-learning），这等于是能够直接把你带到 transition matrix 的任意行中。</li><li>有些算法假设稍微不那么强，它们假设有一个 restart distribution（比如专栏里面讲到的 Kakade 02 和 19 年的两篇工作），这样能够从一个在状态空间中分布比较均匀的初始分布出发来采样，这等于是从一定程度上保证了能比较均匀地访问到 transition matrix 的所有行。</li><li>还有些算法假设了比较简单的 transition function，比如 transition function 是（比较）确定性的或者自由度较低的（本文引用的若干文献），这等于是对 transition function 加了一些先验，从而用更少的样本能够估计到 transition function。</li><li>本文使用 UCB 方法来解决探索问题，在初始状态甚至可以是对手故意选取的情况下，能够达到 <img src="https://www.zhihu.com/equation?tex=%5Ctilde+O%28%5Csqrt%7Bd%5E3+H%5E3T%7D%29" alt="\tilde O(\sqrt{d^3 H^3T})" eeimg="1"/> 。其中 d 是线性化特征的维度（后面会详讲）</li><ul><li><b>为什么在这种情形下，regret 会有一个上界呢？</b>可以这么理解，如果对手挑选了一个从来没见过的状态，我没什么信息都没有，容易产生一个较大的 regret，但通过遭受这个 regret 我们获取了更多的信息；最多在所有的状态上都通过遭受 regret 来学习，因此总是有一个 regret 的上限的。</li><li><b>为什么 regret 上界和状态空间的大小没关系，而只和 d 有关呢？</b>因为这里不是直接对 transition matrix 估计，而是估计它朝 d 维的『基底』上的投影；只要对每一维度上的投影估计准确即能得到最优策略。</li></ul></ul><h2>过程</h2><h3>一、Linear MDP Model </h3><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-bfb1c34d988e85fc96629585186a55dd_b.jpg" data-caption="" data-size="normal" data-rawwidth="1912" data-rawheight="446" class="origin_image zh-lightbox-thumb" width="1912" data-original="https://pic2.zhimg.com/v2-bfb1c34d988e85fc96629585186a55dd_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1912&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1912" data-rawheight="446" class="origin_image zh-lightbox-thumb lazy" width="1912" data-original="https://pic2.zhimg.com/v2-bfb1c34d988e85fc96629585186a55dd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-bfb1c34d988e85fc96629585186a55dd_b.jpg"/></figure><p>需要注意几件事情：</p><ul><li>虽然特征向量只有 d 维，但是 transition function 中 <img src="https://www.zhihu.com/equation?tex=%7B%5Cbf+%5Cmu%7D_h%28%5Ccdot%29" alt="{\bf \mu}_h(\cdot)" eeimg="1"/> 可以是任意在状态空间上的函数，因此在该模型下 transition function 的自由度仍然为无穷维；显然，reward function 的自由度是 d 维。</li><li>Linear MDP 看起来做了比较强的假设，要求这些过程都是线性的。但是只要 d 足够大，该模型可以完全概括 tabular MDP 的情形。参见下图的 Example 2.1。</li><li>要使得 transition function 是一个合法的函数，在特征向量 <img src="https://www.zhihu.com/equation?tex=%5Cphi%28x%2Ca%29" alt="\phi(x,a)" eeimg="1"/> 给定的情况下， <img src="https://www.zhihu.com/equation?tex=%7B%5Cbf+%5Cmu%7D_h%28%5Ccdot%29" alt="{\bf \mu}_h(\cdot)" eeimg="1"/> 的选取需要有一定的限制。下图的 Example 2.2 给出了一个合法的例子。下图的 Proposition A.1. 给出了具体的限制条件。</li><li>最后这件事情最为重要：<b>Linear MDP 和 linear function approximation 之间具有必然联系！</b>具体来说，</li><ul><li>如果模型为线性的，那么任意策略的价值函数就可以用线性函数拟合来准确表示（Proposition 2.3.）；</li><li>如果价值函数用线性函数拟合来表示，要希望任意策略的 Bellman 算子作用到某个线性价值函数上之后还是线性价值函数，那么 MDP 模型必须为线性（Proposition 5.1.）。</li></ul></ul><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-261d099404aa7dc6d95461ffbee3fba5_b.jpg" data-caption="" data-size="normal" data-rawwidth="1920" data-rawheight="494" class="origin_image zh-lightbox-thumb" width="1920" data-original="https://pic2.zhimg.com/v2-261d099404aa7dc6d95461ffbee3fba5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1920&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1920" data-rawheight="494" class="origin_image zh-lightbox-thumb lazy" width="1920" data-original="https://pic2.zhimg.com/v2-261d099404aa7dc6d95461ffbee3fba5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-261d099404aa7dc6d95461ffbee3fba5_b.jpg"/></figure><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-4454d75df5a5f62f968e2e43bb87dc87_b.png" data-caption="" data-size="normal" data-rawwidth="1944" data-rawheight="172" class="origin_image zh-lightbox-thumb" width="1944" data-original="https://pic4.zhimg.com/v2-4454d75df5a5f62f968e2e43bb87dc87_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1944&#39; height=&#39;172&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1944" data-rawheight="172" class="origin_image zh-lightbox-thumb lazy" width="1944" data-original="https://pic4.zhimg.com/v2-4454d75df5a5f62f968e2e43bb87dc87_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4454d75df5a5f62f968e2e43bb87dc87_b.png"/></figure><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-358ed7532534423a1b1a1eabd5611aa7_b.jpg" data-caption="" data-size="normal" data-rawwidth="1934" data-rawheight="472" class="origin_image zh-lightbox-thumb" width="1934" data-original="https://pic4.zhimg.com/v2-358ed7532534423a1b1a1eabd5611aa7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1934&#39; height=&#39;472&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1934" data-rawheight="472" class="origin_image zh-lightbox-thumb lazy" width="1934" data-original="https://pic4.zhimg.com/v2-358ed7532534423a1b1a1eabd5611aa7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-358ed7532534423a1b1a1eabd5611aa7_b.jpg"/></figure><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-bad0ae79e12755f829de33592fa015a7_b.png" data-caption="" data-size="normal" data-rawwidth="1964" data-rawheight="224" class="origin_image zh-lightbox-thumb" width="1964" data-original="https://pic4.zhimg.com/v2-bad0ae79e12755f829de33592fa015a7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1964&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1964" data-rawheight="224" class="origin_image zh-lightbox-thumb lazy" width="1964" data-original="https://pic4.zhimg.com/v2-bad0ae79e12755f829de33592fa015a7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-bad0ae79e12755f829de33592fa015a7_b.png"/></figure><h3>二、算法</h3><p>算法使用 Least-Square Value Iteration （LSVI）+ Upper-Confidence Bound（UCB）。每一轮迭代分为两个步骤：</p><ul><li>第一个步骤是基于历史上所有的样本来估计 Q 函数的参数，该 Q 函数是增加了 UCB bonus 的 Q 函数置信上界；</li><li>第二个步骤是基于估计到的 Q 函数，直接采用 argmax 来做 rollout（注意到在标准的 Q-learning 中，这一步需要加 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy 来做探索，但是这里由于使用了更有效的 UCB 因此代替了 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy）。</li></ul><p>先给出算法框图，然后再详细讲解。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-128ff63c620567140be7d1b06fae5c27_b.jpg" data-caption="" data-size="normal" data-rawwidth="1898" data-rawheight="596" class="origin_image zh-lightbox-thumb" width="1898" data-original="https://pic4.zhimg.com/v2-128ff63c620567140be7d1b06fae5c27_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1898&#39; height=&#39;596&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1898" data-rawheight="596" class="origin_image zh-lightbox-thumb lazy" width="1898" data-original="https://pic4.zhimg.com/v2-128ff63c620567140be7d1b06fae5c27_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-128ff63c620567140be7d1b06fae5c27_b.jpg"/></figure><p><b><i>Least-Square Value Iteration （LSVI）</i></b></p><p>假设 transition function 已知，那么要求解 optimal Q function，只需要依照如下更新方式按 <img src="https://www.zhihu.com/equation?tex=h%3DH%2C+%5Ccdots%2C+1" alt="h=H, \cdots, 1" eeimg="1"/> 撸一遍就好了（动态规划）。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-3ec223f7b826e7d8cf03b75984273604_b.png" data-caption="" data-size="normal" data-rawwidth="1892" data-rawheight="88" class="origin_image zh-lightbox-thumb" width="1892" data-original="https://pic1.zhimg.com/v2-3ec223f7b826e7d8cf03b75984273604_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1892&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1892" data-rawheight="88" class="origin_image zh-lightbox-thumb lazy" width="1892" data-original="https://pic1.zhimg.com/v2-3ec223f7b826e7d8cf03b75984273604_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-3ec223f7b826e7d8cf03b75984273604_b.png"/></figure><p>但是实际上 transition function 是不知道的，因此只能在估计的样本上最小化上式左边和右边之间的 MSE；同时，当状态空间、动作空间很大时，很难对于每一个 <img src="https://www.zhihu.com/equation?tex=%28x%2Ca%29" alt="(x,a)" eeimg="1"/> pair 都有数据，因此把它们往 d 维空间上做一个投影 <img src="https://www.zhihu.com/equation?tex=%5Cphi%28x%2Ca%29" alt="\phi(x,a)" eeimg="1"/> ，并限定线性模型。得到以下优化问题</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-62cd77840b10c71c25c2f7504d20d0de_b.png" data-caption="" data-size="normal" data-rawwidth="1912" data-rawheight="178" class="origin_image zh-lightbox-thumb" width="1912" data-original="https://pic3.zhimg.com/v2-62cd77840b10c71c25c2f7504d20d0de_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1912&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1912" data-rawheight="178" class="origin_image zh-lightbox-thumb lazy" width="1912" data-original="https://pic3.zhimg.com/v2-62cd77840b10c71c25c2f7504d20d0de_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-62cd77840b10c71c25c2f7504d20d0de_b.png"/></figure><p>其中，L2 正则项可以避免 over-fitting（我记得至少 PRML 书里面讲过其原理）。对上述待优化函数写下来，求导，并且令导数为零，可以直接写出闭式解（见算法框图的第 4-5 行）。</p><p><b><i>Upper-Confidence Bound（UCB）</i></b></p><p>首先， <img src="https://www.zhihu.com/equation?tex=m%28x%2Ca%29%3A%3D%28%5Cphi%5ET%28x%2Ca%29+%5CLambda_h%5E%7B-1%7D+%5Cphi%28x%2Ca%29%29%5E%7B-1%7D" alt="m(x,a):=(\phi^T(x,a) \Lambda_h^{-1} \phi(x,a))^{-1}" eeimg="1"/> （参考算法框图第 6 行）表示了 <img src="https://www.zhihu.com/equation?tex=%28x%2Ca%29" alt="(x,a)" eeimg="1"/> 被等效地访问了多少次。考虑 Example 2.1. 中的 tabular MDP 的情形。 <img src="https://www.zhihu.com/equation?tex=%5CLambda_h" alt="\Lambda_h" eeimg="1"/> 是一个 SAxSA 的对角矩阵，对角元上的元素为该 <img src="https://www.zhihu.com/equation?tex=%28x%2Ca%29" alt="(x,a)" eeimg="1"/> pair 在历史数据中被访问了多少次（为了防止某些 <img src="https://www.zhihu.com/equation?tex=%28x%2Ca%29" alt="(x,a)" eeimg="1"/> pair 一次都没有访问到，会不能取逆，因此加了正则）。而接下来的操作则提取出来具体的某个 <img src="https://www.zhihu.com/equation?tex=%28x%2Ca%29" alt="(x,a)" eeimg="1"/> 访问的次数。在非 tabular MDP 的情形下，该计数等于是把历史数据的特征向量朝 <img src="https://www.zhihu.com/equation?tex=%5Cphi%28x%2Ca%29" alt="\phi(x,a)" eeimg="1"/> 做投影并且计数。</p><p>最后，考虑到 UCB bonus 的公式为 <img src="https://www.zhihu.com/equation?tex=%5Cbeta%2F%5Csqrt%7Bm%7D" alt="\beta/\sqrt{m}" eeimg="1"/> ，由此可以写出算法框图第 6 行的公式。</p><h3>三、理论结果</h3><p>如果实际 MDP 满足前述 Linear MDP 的要求，则有如下 regret bound。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-0e4f5e09f24f9aa1511bfbe60fa83498_b.png" data-caption="" data-size="normal" data-rawwidth="1938" data-rawheight="238" class="origin_image zh-lightbox-thumb" width="1938" data-original="https://pic1.zhimg.com/v2-0e4f5e09f24f9aa1511bfbe60fa83498_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1938&#39; height=&#39;238&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1938" data-rawheight="238" class="origin_image zh-lightbox-thumb lazy" width="1938" data-original="https://pic1.zhimg.com/v2-0e4f5e09f24f9aa1511bfbe60fa83498_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0e4f5e09f24f9aa1511bfbe60fa83498_b.png"/></figure><ul><li><b>该 regret bound 可以被转化为 PAC 的结果</b>：考虑固定初始状态 <img src="https://www.zhihu.com/equation?tex=x_1" alt="x_1" eeimg="1"/> （而不再是每轮对手选定），根据 regret 的定义有 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bk%3D1%7D%5EK+%28V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5Ek%7D%28x_1%29%29+%3D+%5Ctilde+O%28%5Csqrt%7Bd%5E3+H%5E3+T%7D%29+%3D+%5Ctilde+O%28%5Csqrt%7Bd%5E3+H%5E4+K%7D%29" alt="\sum_{k=1}^K (V^*(x_1) - V^{\pi^k}(x_1)) = \tilde O(\sqrt{d^3 H^3 T}) = \tilde O(\sqrt{d^3 H^4 K})" eeimg="1"/> ，取 <img src="https://www.zhihu.com/equation?tex=K%3D%5Ctilde+O%28d%5E3H%5E4%2F%5Cepsilon%5E2%29" alt="K=\tilde O(d^3H^4/\epsilon^2)" eeimg="1"/> ，并且随机取一个 <img src="https://www.zhihu.com/equation?tex=k%27%5Cin+%5BK%5D" alt="k&#39;\in [K]" eeimg="1"/> ，则有 <img src="https://www.zhihu.com/equation?tex=K+%28V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5E%7Bk%27%7D%7D%28x_1%29%29+%5Capprox+%5Csum_%7Bk%3D1%7D%5EK+%28V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5Ek%7D%28x_1%29%29+%3D+%5Ctilde+O%28d%5E3+H%5E4%2F%5Cepsilon%29" alt="K (V^*(x_1) - V^{\pi^{k&#39;}}(x_1)) \approx \sum_{k=1}^K (V^*(x_1) - V^{\pi^k}(x_1)) = \tilde O(d^3 H^4/\epsilon)" eeimg="1"/> ，其中约等号表示随机选取可以大概率取到低于均值的情形，在这种情况下有 <img src="https://www.zhihu.com/equation?tex=V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5E%7Bk%27%7D%7D%28x_1%29+%5Capprox+%5Cepsilon" alt="V^*(x_1) - V^{\pi^{k&#39;}}(x_1) \approx \epsilon" eeimg="1"/> ，即需要 <img src="https://www.zhihu.com/equation?tex=KH+%3D+%5Ctilde+O%28d%5E3+H%5E5+%2F%5Cepsilon%5E2%29" alt="KH = \tilde O(d^3 H^5 /\epsilon^2)" eeimg="1"/> 的样本以拿到一个 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -optimal 的策略。（跟文中算出来的差了个 H。。）</li><li><b>空间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28d%5E2H%2BdAT%29" alt="O(d^2H+dAT)" eeimg="1"/> </b>：储存 <img src="https://www.zhihu.com/equation?tex=%5CLambda_h" alt="\Lambda_h" eeimg="1"/> 需要 <img src="https://www.zhihu.com/equation?tex=d%5E2+H" alt="d^2 H" eeimg="1"/> ，储存 <img src="https://www.zhihu.com/equation?tex=w_h" alt="w_h" eeimg="1"/> 需要 <img src="https://www.zhihu.com/equation?tex=dH" alt="dH" eeimg="1"/> ，储存所有的 reward 需要 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> ，储存所有遇到的 <img src="https://www.zhihu.com/equation?tex=%5Cphi%28x%2Ca%29" alt="\phi(x,a)" eeimg="1"/> pair 以及所遇到状态下所有其他行动的 <img src="https://www.zhihu.com/equation?tex=%5Cphi%28x%2Ca%27%29" alt="\phi(x,a&#39;)" eeimg="1"/> ，它们需要 <img src="https://www.zhihu.com/equation?tex=d+A+T" alt="d A T" eeimg="1"/> 。</li><li><b>计算复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28d%5E2AKT%29" alt="O(d^2AKT)" eeimg="1"/> </b>：有点没算出来，跟文中的差了一个 d。。</li></ul><p>如果实际 MDP 和前述 Linear MDP 的假设有一定的差距（misspecification），差距定义为</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-3c473e35f2280ad74634cff97ac50d99_b.jpg" data-caption="" data-size="normal" data-rawwidth="1924" data-rawheight="470" class="origin_image zh-lightbox-thumb" width="1924" data-original="https://pic2.zhimg.com/v2-3c473e35f2280ad74634cff97ac50d99_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1924&#39; height=&#39;470&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1924" data-rawheight="470" class="origin_image zh-lightbox-thumb lazy" width="1924" data-original="https://pic2.zhimg.com/v2-3c473e35f2280ad74634cff97ac50d99_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-3c473e35f2280ad74634cff97ac50d99_b.jpg"/></figure><p>则有如下 regret bound。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-45dc54f585bcc0310d0f11f9dfe5e6fc_b.png" data-caption="" data-size="normal" data-rawwidth="1922" data-rawheight="178" class="origin_image zh-lightbox-thumb" width="1922" data-original="https://pic1.zhimg.com/v2-45dc54f585bcc0310d0f11f9dfe5e6fc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1922&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1922" data-rawheight="178" class="origin_image zh-lightbox-thumb lazy" width="1922" data-original="https://pic1.zhimg.com/v2-45dc54f585bcc0310d0f11f9dfe5e6fc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-45dc54f585bcc0310d0f11f9dfe5e6fc_b.png"/></figure><p>不难理解，但存在 misspecification 的时候，最后学习到的模型一定存在 bias，因此 regret 中肯定会有关于 T 线性的项。</p><h3>四、算法机制</h3><p>算法中有有两点最为关键</p><ul><li>探索的实质就是说明要如何估计 transition function，因此这里证明的重点之一就是说明 LSVI 是如何估计到 transition function 的。</li><li>UCB 的作用机理可以概括为：加上 UCB bonus 之后，估计的价值函数 &gt; 最优价值函数；通过采样样本的增多，估计的价值函数 -&gt; 实际的策略性能；而最优价值函数 &gt; 实际的策略性能。最后的结果是实际的策略性能最后能够逼近最优价值函数。需要注意的一点是由于价值函数的更新是 bootstrapped，因此分析上也是后一层的误差会传递到前一层。</li><ul><li>关于 UCB 的机理，可以具体参考该作者前面的一篇工作 <a href="https://zhuanlan.zhihu.com/p/82857779" class="internal">UCB+Q-learning</a>。注意到前面的算法是 online 的，因此 Q 值的更新设置了一个 learning rate <img src="https://www.zhihu.com/equation?tex=%5Calpha_t" alt="\alpha_t" eeimg="1"/> 。这里比较粗暴，每次都对于所有的样本一锅端，直接计算 optimal Bellman operator 作用之后的闭式解。</li></ul></ul><p>这里主要来讲第一点。即，算法中使用样本估计 transition function 的方法能够逼近一个完美的 optimal Bellman operation。为了简化分析，可以 1）扔掉 UCB 项；2）扔掉 smoothing 的项（即 <img src="https://www.zhihu.com/equation?tex=%5Clambda+%3D0" alt="\lambda =0" eeimg="1"/> ）。在第二个简化下，有</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-211884ee2d9e0bd8102ebb52eb269e7b_b.png" data-caption="" data-size="normal" data-rawwidth="1708" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="1708" data-original="https://pic4.zhimg.com/v2-211884ee2d9e0bd8102ebb52eb269e7b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1708&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1708" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="1708" data-original="https://pic4.zhimg.com/v2-211884ee2d9e0bd8102ebb52eb269e7b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-211884ee2d9e0bd8102ebb52eb269e7b_b.png"/></figure><p>一个 optimal Bellman operation，对于所有的 <img src="https://www.zhihu.com/equation?tex=%28x%2Ca%29" alt="(x,a)" eeimg="1"/> 的 Q 值都做如下更新：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-f20a8dd7e64eedda09e1cd70d2008f6e_b.png" data-caption="" data-size="normal" data-rawwidth="1920" data-rawheight="86" class="origin_image zh-lightbox-thumb" width="1920" data-original="https://pic3.zhimg.com/v2-f20a8dd7e64eedda09e1cd70d2008f6e_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1920&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1920" data-rawheight="86" class="origin_image zh-lightbox-thumb lazy" width="1920" data-original="https://pic3.zhimg.com/v2-f20a8dd7e64eedda09e1cd70d2008f6e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-f20a8dd7e64eedda09e1cd70d2008f6e_b.png"/></figure><p>算法中的基于样本的更新如下，即对于 <img src="https://www.zhihu.com/equation?tex=w_h" alt="w_h" eeimg="1"/> 的所有维度都做如下更新：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-d114eb6d0ccaa5b82b9b9ced9a6980b7_b.png" data-caption="" data-size="normal" data-rawwidth="1900" data-rawheight="156" class="origin_image zh-lightbox-thumb" width="1900" data-original="https://pic4.zhimg.com/v2-d114eb6d0ccaa5b82b9b9ced9a6980b7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1900&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1900" data-rawheight="156" class="origin_image zh-lightbox-thumb lazy" width="1900" data-original="https://pic4.zhimg.com/v2-d114eb6d0ccaa5b82b9b9ced9a6980b7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-d114eb6d0ccaa5b82b9b9ced9a6980b7_b.png"/></figure><p>其中记 <img src="https://www.zhihu.com/equation?tex=V%28x_%7Bh%2B1%7D%29+%3D+%5Cmax_a+Q%28x_%7Bh%2B1%7D%2C+a%29" alt="V(x_{h+1}) = \max_a Q(x_{h+1}, a)" eeimg="1"/> 。</p><p><b><i>1. Reward Function Term</i></b></p><p>根据 Linear MDP 的假设，奖励函数是确定性的，因此很容易得到</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Cphi%28x%2Ca%29%5ET+%5CLambda_h%5E%7B-1%7D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bk-1%7D+%5Cphi%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29r%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29+%5C%5C+%3D+%26+%5Cphi%28x%2Ca%29%5ET+%5CLambda_h%5E%7B-1%7D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bk-1%7D+%5Cphi%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29+%5Cphi%5ET%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29+%5Ctheta_h+%5C%5C+%3D+%26+%5Cphi%28x%2Ca%29%5ET+%5Ctheta_h+%5C%5C+%3D+%26+r%28x%2Ca%29+%5C%5C+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \phi(x,a)^T \Lambda_h^{-1} \sum_{\tau=1}^{k-1} \phi(x_h^\tau, a_h^\tau)r(x_h^\tau, a_h^\tau) \\ = &amp; \phi(x,a)^T \Lambda_h^{-1} \sum_{\tau=1}^{k-1} \phi(x_h^\tau, a_h^\tau) \phi^T(x_h^\tau, a_h^\tau) \theta_h \\ = &amp; \phi(x,a)^T \theta_h \\ = &amp; r(x,a) \\ \end{aligned}" eeimg="1"/> </p><p><b><i>2. Transition Function Term</i></b></p><p>定义以下两个算子</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-af331eed4e3484279ae880518b440de2_b.png" data-caption="" data-size="normal" data-rawwidth="1914" data-rawheight="150" class="origin_image zh-lightbox-thumb" width="1914" data-original="https://pic3.zhimg.com/v2-af331eed4e3484279ae880518b440de2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1914&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1914" data-rawheight="150" class="origin_image zh-lightbox-thumb lazy" width="1914" data-original="https://pic3.zhimg.com/v2-af331eed4e3484279ae880518b440de2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-af331eed4e3484279ae880518b440de2_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c7e61778b23f3df1bf9c0d4136045c75_b.png" data-caption="" data-size="normal" data-rawwidth="1898" data-rawheight="140" class="origin_image zh-lightbox-thumb" width="1898" data-original="https://pic2.zhimg.com/v2-c7e61778b23f3df1bf9c0d4136045c75_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1898&#39; height=&#39;140&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1898" data-rawheight="140" class="origin_image zh-lightbox-thumb lazy" width="1898" data-original="https://pic2.zhimg.com/v2-c7e61778b23f3df1bf9c0d4136045c75_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c7e61778b23f3df1bf9c0d4136045c75_b.png"/></figure><p>不难看出，算法中的 transition function term 为 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D" alt="\hat{\mathbb{P}}_h V_{h+1}" eeimg="1"/> ，而在线性模型下 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D+%3D+%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D" alt="\bar{\mathbb{P}}_h V_{h+1} = {\mathbb{P}}_h V_{h+1}" eeimg="1"/> ，而 <img src="https://www.zhihu.com/equation?tex=%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D" alt="{\mathbb{P}}_h V_{h+1}" eeimg="1"/> 就是 optimal Bellman operation 中对应的项。这一点容易看出：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-ad0d88ba166f4f3d1044584ec6706c1c_b.png" data-caption="" data-size="normal" data-rawwidth="1884" data-rawheight="138" class="origin_image zh-lightbox-thumb" width="1884" data-original="https://pic1.zhimg.com/v2-ad0d88ba166f4f3d1044584ec6706c1c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1884&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1884" data-rawheight="138" class="origin_image zh-lightbox-thumb lazy" width="1884" data-original="https://pic1.zhimg.com/v2-ad0d88ba166f4f3d1044584ec6706c1c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-ad0d88ba166f4f3d1044584ec6706c1c_b.png"/></figure><p>下面的重点就是</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-0524006b2412db7a4eef394864c8f724_b.png" data-caption="" data-size="normal" data-rawwidth="1908" data-rawheight="74" class="origin_image zh-lightbox-thumb" width="1908" data-original="https://pic1.zhimg.com/v2-0524006b2412db7a4eef394864c8f724_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1908&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1908" data-rawheight="74" class="origin_image zh-lightbox-thumb lazy" width="1908" data-original="https://pic1.zhimg.com/v2-0524006b2412db7a4eef394864c8f724_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0524006b2412db7a4eef394864c8f724_b.png"/></figure><p>注意到</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-0cbf20e1e4530aaf5cb35d011c495ffa_b.png" data-caption="" data-size="normal" data-rawwidth="1922" data-rawheight="74" class="origin_image zh-lightbox-thumb" width="1922" data-original="https://pic3.zhimg.com/v2-0cbf20e1e4530aaf5cb35d011c495ffa_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1922&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1922" data-rawheight="74" class="origin_image zh-lightbox-thumb lazy" width="1922" data-original="https://pic3.zhimg.com/v2-0cbf20e1e4530aaf5cb35d011c495ffa_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-0cbf20e1e4530aaf5cb35d011c495ffa_b.png"/></figure><p>由于 <img src="https://www.zhihu.com/equation?tex=x_%7Bh%2B1%7D%5E%5Ctau+%5Csim+%5Cmathbb%7BP%7D_h%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29" alt="x_{h+1}^\tau \sim \mathbb{P}_h(x_h^\tau, a_h^\tau)" eeimg="1"/> ，因此可以使用 concentration inequality 来 bound。但是这里面有一些难处理的地方，这就是 <img src="https://www.zhihu.com/equation?tex=V_%7Bh%2B1%7D" alt="V_{h+1}" eeimg="1"/> 每一轮是在变化的，但是这个变化的过程有很难去 track，因此文章直接在如下函数族上做 concentration。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0a720ed2e841aa80f769dc41d2c66529_b.png" data-caption="" data-size="normal" data-rawwidth="1930" data-rawheight="108" class="origin_image zh-lightbox-thumb" width="1930" data-original="https://pic2.zhimg.com/v2-0a720ed2e841aa80f769dc41d2c66529_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1930&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1930" data-rawheight="108" class="origin_image zh-lightbox-thumb lazy" width="1930" data-original="https://pic2.zhimg.com/v2-0a720ed2e841aa80f769dc41d2c66529_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-0a720ed2e841aa80f769dc41d2c66529_b.png"/></figure><p>这一个部分给我们的启示是，虽然要准确估计 transition function 很难，但是通过把它往线性基底上面投影，可以使用更少的样本来对其有准确估计。</p><hr/><p>UCB 的探索奖励是 Hoeffding type，如果改用 Bernstein type 的 regret 可能可以去掉一个 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BH%7D" alt="\sqrt{H}" eeimg="1"/> 。</p></div></div><div class="ContentItem-time">发布于 2019-10-29</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">机器学习</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19813032" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">深度学习（Deep Learning）</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 19 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 19</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>1 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="7beb7fa5-183a-4fbf-aae1-119b11745556" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="7beb7fa5-183a-4fbf-aae1-119b11745556">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"88931389":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":88931389,"title":"【强化学习 97】Linear MDP","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F88931389","imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a08c7211bce17cd63dcfc91c6b3ff6c7_b.jpg","titleImage":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a08c7211bce17cd63dcfc91c6b3ff6c7_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e412521fe644ed168f2cb5267014e0e3_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1912\" data-rawheight=\"446\" data-watermark=\"watermark\" data-original-src=\"v2-e412521fe644ed168f2cb5267014e0e3\" data-watermark-src=\"v2-bfb1c34d988e85fc96629585186a55dd\" data-private-watermark-src=\"\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e412521fe644ed168f2cb5267014e0e3_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"\u002F\u003E对于线性 MDP 模型，可以做线性函数拟合。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1907.05388.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EJin, Chi, et al. &#34;Provably efficient reinforcement learning with linear function approximation.&#34; arXiv preprint arXiv:1907.05388 (2019).\u003C\u002Fa\u003E特色 考虑对价值函数做函数拟合（function approximation…","created":1572341035,"updated":1572341035,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":2018,"imageHeight":896,"content":"\u003Cp\u003E对于线性 MDP 模型，可以做线性函数拟合。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1907.05388.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EJin, Chi, et al. &#34;Provably efficient reinforcement learning with linear function approximation.&#34; arXiv preprint arXiv:1907.05388 (2019).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E 考虑对价值函数做函数拟合（function approximation）。当函数拟合使用的函数 capacity 大的时候，容易遇到 sparsity 的问题，即所遇到的大多数状态的附近都没有其他样本，则很难估计出一个准确的价值函数（variance 大）；当函数拟合使用的函数 capacity 小的时候，容易遇到 misspecification 的问题，即所使用的函数族不足以表示真实的价值函数（bias 大）。为此，文章提出了线性 MDP 模型，在线性 MDP 模型下，线性的函数拟合不会产生 bias，同时也定量分析了如果稍微偏离线性 MDP 模型，线性函数拟合会产生的 bias 大小。值得注意的是，文章提出的线性 MDP 模型能够概括 tabular case。\u003C\u002Fp\u003E\u003Cp\u003E同时，要设计一个有效的（probably efficient）算法，还需要做探索（或称作 exploration-exploitation tradeoff）。探索问题可以想象成如何有效地估计到 transition function \u002F matrix。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E有些算法假设能有一个 simulator，这样能够设置成任意的状态（比如，最简单的那种 tabular Q-learning），这等于是能够直接把你带到 transition matrix 的任意行中。\u003C\u002Fli\u003E\u003Cli\u003E有些算法假设稍微不那么强，它们假设有一个 restart distribution（比如专栏里面讲到的 Kakade 02 和 19 年的两篇工作），这样能够从一个在状态空间中分布比较均匀的初始分布出发来采样，这等于是从一定程度上保证了能比较均匀地访问到 transition matrix 的所有行。\u003C\u002Fli\u003E\u003Cli\u003E还有些算法假设了比较简单的 transition function，比如 transition function 是（比较）确定性的或者自由度较低的（本文引用的若干文献），这等于是对 transition function 加了一些先验，从而用更少的样本能够估计到 transition function。\u003C\u002Fli\u003E\u003Cli\u003E本文使用 UCB 方法来解决探索问题，在初始状态甚至可以是对手故意选取的情况下，能够达到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde+O%28%5Csqrt%7Bd%5E3+H%5E3T%7D%29\" alt=\"\\tilde O(\\sqrt{d^3 H^3T})\" eeimg=\"1\"\u002F\u003E 。其中 d 是线性化特征的维度（后面会详讲）\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli\u003E\u003Cb\u003E为什么在这种情形下，regret 会有一个上界呢？\u003C\u002Fb\u003E可以这么理解，如果对手挑选了一个从来没见过的状态，我没什么信息都没有，容易产生一个较大的 regret，但通过遭受这个 regret 我们获取了更多的信息；最多在所有的状态上都通过遭受 regret 来学习，因此总是有一个 regret 的上限的。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E为什么 regret 上界和状态空间的大小没关系，而只和 d 有关呢？\u003C\u002Fb\u003E因为这里不是直接对 transition matrix 估计，而是估计它朝 d 维的『基底』上的投影；只要对每一维度上的投影估计准确即能得到最优策略。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E一、Linear MDP Model \u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bfb1c34d988e85fc96629585186a55dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1912\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"1912\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bfb1c34d988e85fc96629585186a55dd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1912&#39; height=&#39;446&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1912\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1912\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bfb1c34d988e85fc96629585186a55dd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bfb1c34d988e85fc96629585186a55dd_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E需要注意几件事情：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E虽然特征向量只有 d 维，但是 transition function 中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7B%5Cbf+%5Cmu%7D_h%28%5Ccdot%29\" alt=\"{\\bf \\mu}_h(\\cdot)\" eeimg=\"1\"\u002F\u003E 可以是任意在状态空间上的函数，因此在该模型下 transition function 的自由度仍然为无穷维；显然，reward function 的自由度是 d 维。\u003C\u002Fli\u003E\u003Cli\u003ELinear MDP 看起来做了比较强的假设，要求这些过程都是线性的。但是只要 d 足够大，该模型可以完全概括 tabular MDP 的情形。参见下图的 Example 2.1。\u003C\u002Fli\u003E\u003Cli\u003E要使得 transition function 是一个合法的函数，在特征向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%28x%2Ca%29\" alt=\"\\phi(x,a)\" eeimg=\"1\"\u002F\u003E 给定的情况下， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7B%5Cbf+%5Cmu%7D_h%28%5Ccdot%29\" alt=\"{\\bf \\mu}_h(\\cdot)\" eeimg=\"1\"\u002F\u003E 的选取需要有一定的限制。下图的 Example 2.2 给出了一个合法的例子。下图的 Proposition A.1. 给出了具体的限制条件。\u003C\u002Fli\u003E\u003Cli\u003E最后这件事情最为重要：\u003Cb\u003ELinear MDP 和 linear function approximation 之间具有必然联系！\u003C\u002Fb\u003E具体来说，\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli\u003E如果模型为线性的，那么任意策略的价值函数就可以用线性函数拟合来准确表示（Proposition 2.3.）；\u003C\u002Fli\u003E\u003Cli\u003E如果价值函数用线性函数拟合来表示，要希望任意策略的 Bellman 算子作用到某个线性价值函数上之后还是线性价值函数，那么 MDP 模型必须为线性（Proposition 5.1.）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-261d099404aa7dc6d95461ffbee3fba5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-261d099404aa7dc6d95461ffbee3fba5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1920&#39; height=&#39;494&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1920\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-261d099404aa7dc6d95461ffbee3fba5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-261d099404aa7dc6d95461ffbee3fba5_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4454d75df5a5f62f968e2e43bb87dc87_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1944\" data-rawheight=\"172\" class=\"origin_image zh-lightbox-thumb\" width=\"1944\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4454d75df5a5f62f968e2e43bb87dc87_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1944&#39; height=&#39;172&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1944\" data-rawheight=\"172\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1944\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4454d75df5a5f62f968e2e43bb87dc87_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-4454d75df5a5f62f968e2e43bb87dc87_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-358ed7532534423a1b1a1eabd5611aa7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1934\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb\" width=\"1934\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-358ed7532534423a1b1a1eabd5611aa7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1934&#39; height=&#39;472&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1934\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1934\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-358ed7532534423a1b1a1eabd5611aa7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-358ed7532534423a1b1a1eabd5611aa7_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bad0ae79e12755f829de33592fa015a7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1964\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"1964\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bad0ae79e12755f829de33592fa015a7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1964&#39; height=&#39;224&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1964\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1964\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bad0ae79e12755f829de33592fa015a7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bad0ae79e12755f829de33592fa015a7_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E二、算法\u003C\u002Fh3\u003E\u003Cp\u003E算法使用 Least-Square Value Iteration （LSVI）+ Upper-Confidence Bound（UCB）。每一轮迭代分为两个步骤：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E第一个步骤是基于历史上所有的样本来估计 Q 函数的参数，该 Q 函数是增加了 UCB bonus 的 Q 函数置信上界；\u003C\u002Fli\u003E\u003Cli\u003E第二个步骤是基于估计到的 Q 函数，直接采用 argmax 来做 rollout（注意到在标准的 Q-learning 中，这一步需要加 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy 来做探索，但是这里由于使用了更有效的 UCB 因此代替了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E先给出算法框图，然后再详细讲解。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-128ff63c620567140be7d1b06fae5c27_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1898\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb\" width=\"1898\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-128ff63c620567140be7d1b06fae5c27_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1898&#39; height=&#39;596&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1898\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1898\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-128ff63c620567140be7d1b06fae5c27_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-128ff63c620567140be7d1b06fae5c27_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003ELeast-Square Value Iteration （LSVI）\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E假设 transition function 已知，那么要求解 optimal Q function，只需要依照如下更新方式按 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%3DH%2C+%5Ccdots%2C+1\" alt=\"h=H, \\cdots, 1\" eeimg=\"1\"\u002F\u003E 撸一遍就好了（动态规划）。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3ec223f7b826e7d8cf03b75984273604_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1892\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb\" width=\"1892\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3ec223f7b826e7d8cf03b75984273604_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1892&#39; height=&#39;88&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1892\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1892\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3ec223f7b826e7d8cf03b75984273604_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3ec223f7b826e7d8cf03b75984273604_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E但是实际上 transition function 是不知道的，因此只能在估计的样本上最小化上式左边和右边之间的 MSE；同时，当状态空间、动作空间很大时，很难对于每一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E pair 都有数据，因此把它们往 d 维空间上做一个投影 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%28x%2Ca%29\" alt=\"\\phi(x,a)\" eeimg=\"1\"\u002F\u003E ，并限定线性模型。得到以下优化问题\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62cd77840b10c71c25c2f7504d20d0de_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1912\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1912\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62cd77840b10c71c25c2f7504d20d0de_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1912&#39; height=&#39;178&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1912\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1912\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62cd77840b10c71c25c2f7504d20d0de_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62cd77840b10c71c25c2f7504d20d0de_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中，L2 正则项可以避免 over-fitting（我记得至少 PRML 书里面讲过其原理）。对上述待优化函数写下来，求导，并且令导数为零，可以直接写出闭式解（见算法框图的第 4-5 行）。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EUpper-Confidence Bound（UCB）\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E首先， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m%28x%2Ca%29%3A%3D%28%5Cphi%5ET%28x%2Ca%29+%5CLambda_h%5E%7B-1%7D+%5Cphi%28x%2Ca%29%29%5E%7B-1%7D\" alt=\"m(x,a):=(\\phi^T(x,a) \\Lambda_h^{-1} \\phi(x,a))^{-1}\" eeimg=\"1\"\u002F\u003E （参考算法框图第 6 行）表示了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E 被等效地访问了多少次。考虑 Example 2.1. 中的 tabular MDP 的情形。 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CLambda_h\" alt=\"\\Lambda_h\" eeimg=\"1\"\u002F\u003E 是一个 SAxSA 的对角矩阵，对角元上的元素为该 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E pair 在历史数据中被访问了多少次（为了防止某些 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E pair 一次都没有访问到，会不能取逆，因此加了正则）。而接下来的操作则提取出来具体的某个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E 访问的次数。在非 tabular MDP 的情形下，该计数等于是把历史数据的特征向量朝 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%28x%2Ca%29\" alt=\"\\phi(x,a)\" eeimg=\"1\"\u002F\u003E 做投影并且计数。\u003C\u002Fp\u003E\u003Cp\u003E最后，考虑到 UCB bonus 的公式为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbeta%2F%5Csqrt%7Bm%7D\" alt=\"\\beta\u002F\\sqrt{m}\" eeimg=\"1\"\u002F\u003E ，由此可以写出算法框图第 6 行的公式。\u003C\u002Fp\u003E\u003Ch3\u003E三、理论结果\u003C\u002Fh3\u003E\u003Cp\u003E如果实际 MDP 满足前述 Linear MDP 的要求，则有如下 regret bound。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e4f5e09f24f9aa1511bfbe60fa83498_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1938\" data-rawheight=\"238\" class=\"origin_image zh-lightbox-thumb\" width=\"1938\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e4f5e09f24f9aa1511bfbe60fa83498_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1938&#39; height=&#39;238&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1938\" data-rawheight=\"238\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1938\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e4f5e09f24f9aa1511bfbe60fa83498_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e4f5e09f24f9aa1511bfbe60fa83498_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003E\u003Cb\u003E该 regret bound 可以被转化为 PAC 的结果\u003C\u002Fb\u003E：考虑固定初始状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x_1\" alt=\"x_1\" eeimg=\"1\"\u002F\u003E （而不再是每轮对手选定），根据 regret 的定义有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_%7Bk%3D1%7D%5EK+%28V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5Ek%7D%28x_1%29%29+%3D+%5Ctilde+O%28%5Csqrt%7Bd%5E3+H%5E3+T%7D%29+%3D+%5Ctilde+O%28%5Csqrt%7Bd%5E3+H%5E4+K%7D%29\" alt=\"\\sum_{k=1}^K (V^*(x_1) - V^{\\pi^k}(x_1)) = \\tilde O(\\sqrt{d^3 H^3 T}) = \\tilde O(\\sqrt{d^3 H^4 K})\" eeimg=\"1\"\u002F\u003E ，取 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K%3D%5Ctilde+O%28d%5E3H%5E4%2F%5Cepsilon%5E2%29\" alt=\"K=\\tilde O(d^3H^4\u002F\\epsilon^2)\" eeimg=\"1\"\u002F\u003E ，并且随机取一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k%27%5Cin+%5BK%5D\" alt=\"k&#39;\\in [K]\" eeimg=\"1\"\u002F\u003E ，则有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K+%28V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5E%7Bk%27%7D%7D%28x_1%29%29+%5Capprox+%5Csum_%7Bk%3D1%7D%5EK+%28V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5Ek%7D%28x_1%29%29+%3D+%5Ctilde+O%28d%5E3+H%5E4%2F%5Cepsilon%29\" alt=\"K (V^*(x_1) - V^{\\pi^{k&#39;}}(x_1)) \\approx \\sum_{k=1}^K (V^*(x_1) - V^{\\pi^k}(x_1)) = \\tilde O(d^3 H^4\u002F\\epsilon)\" eeimg=\"1\"\u002F\u003E ，其中约等号表示随机选取可以大概率取到低于均值的情形，在这种情况下有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%2A%28x_1%29+-+V%5E%7B%5Cpi%5E%7Bk%27%7D%7D%28x_1%29+%5Capprox+%5Cepsilon\" alt=\"V^*(x_1) - V^{\\pi^{k&#39;}}(x_1) \\approx \\epsilon\" eeimg=\"1\"\u002F\u003E ，即需要 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=KH+%3D+%5Ctilde+O%28d%5E3+H%5E5+%2F%5Cepsilon%5E2%29\" alt=\"KH = \\tilde O(d^3 H^5 \u002F\\epsilon^2)\" eeimg=\"1\"\u002F\u003E 的样本以拿到一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -optimal 的策略。（跟文中算出来的差了个 H。。）\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E空间复杂度为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=O%28d%5E2H%2BdAT%29\" alt=\"O(d^2H+dAT)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fb\u003E：储存 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CLambda_h\" alt=\"\\Lambda_h\" eeimg=\"1\"\u002F\u003E 需要 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d%5E2+H\" alt=\"d^2 H\" eeimg=\"1\"\u002F\u003E ，储存 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w_h\" alt=\"w_h\" eeimg=\"1\"\u002F\u003E 需要 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=dH\" alt=\"dH\" eeimg=\"1\"\u002F\u003E ，储存所有的 reward 需要 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E ，储存所有遇到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%28x%2Ca%29\" alt=\"\\phi(x,a)\" eeimg=\"1\"\u002F\u003E pair 以及所遇到状态下所有其他行动的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi%28x%2Ca%27%29\" alt=\"\\phi(x,a&#39;)\" eeimg=\"1\"\u002F\u003E ，它们需要 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d+A+T\" alt=\"d A T\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cb\u003E计算复杂度为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=O%28d%5E2AKT%29\" alt=\"O(d^2AKT)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fb\u003E：有点没算出来，跟文中的差了一个 d。。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E如果实际 MDP 和前述 Linear MDP 的假设有一定的差距（misspecification），差距定义为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3c473e35f2280ad74634cff97ac50d99_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1924\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb\" width=\"1924\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3c473e35f2280ad74634cff97ac50d99_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1924&#39; height=&#39;470&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1924\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1924\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3c473e35f2280ad74634cff97ac50d99_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3c473e35f2280ad74634cff97ac50d99_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E则有如下 regret bound。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45dc54f585bcc0310d0f11f9dfe5e6fc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1922\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1922\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45dc54f585bcc0310d0f11f9dfe5e6fc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1922&#39; height=&#39;178&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1922\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1922\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45dc54f585bcc0310d0f11f9dfe5e6fc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45dc54f585bcc0310d0f11f9dfe5e6fc_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E不难理解，但存在 misspecification 的时候，最后学习到的模型一定存在 bias，因此 regret 中肯定会有关于 T 线性的项。\u003C\u002Fp\u003E\u003Ch3\u003E四、算法机制\u003C\u002Fh3\u003E\u003Cp\u003E算法中有有两点最为关键\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E探索的实质就是说明要如何估计 transition function，因此这里证明的重点之一就是说明 LSVI 是如何估计到 transition function 的。\u003C\u002Fli\u003E\u003Cli\u003EUCB 的作用机理可以概括为：加上 UCB bonus 之后，估计的价值函数 &gt; 最优价值函数；通过采样样本的增多，估计的价值函数 -&gt; 实际的策略性能；而最优价值函数 &gt; 实际的策略性能。最后的结果是实际的策略性能最后能够逼近最优价值函数。需要注意的一点是由于价值函数的更新是 bootstrapped，因此分析上也是后一层的误差会传递到前一层。\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli\u003E关于 UCB 的机理，可以具体参考该作者前面的一篇工作 \u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F82857779\" class=\"internal\"\u003EUCB+Q-learning\u003C\u002Fa\u003E。注意到前面的算法是 online 的，因此 Q 值的更新设置了一个 learning rate \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t\" alt=\"\\alpha_t\" eeimg=\"1\"\u002F\u003E 。这里比较粗暴，每次都对于所有的样本一锅端，直接计算 optimal Bellman operator 作用之后的闭式解。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp\u003E这里主要来讲第一点。即，算法中使用样本估计 transition function 的方法能够逼近一个完美的 optimal Bellman operation。为了简化分析，可以 1）扔掉 UCB 项；2）扔掉 smoothing 的项（即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda+%3D0\" alt=\"\\lambda =0\" eeimg=\"1\"\u002F\u003E ）。在第二个简化下，有\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-211884ee2d9e0bd8102ebb52eb269e7b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"1708\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-211884ee2d9e0bd8102ebb52eb269e7b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1708&#39; height=&#39;106&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1708\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-211884ee2d9e0bd8102ebb52eb269e7b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-211884ee2d9e0bd8102ebb52eb269e7b_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E一个 optimal Bellman operation，对于所有的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E 的 Q 值都做如下更新：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f20a8dd7e64eedda09e1cd70d2008f6e_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f20a8dd7e64eedda09e1cd70d2008f6e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1920&#39; height=&#39;86&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1920\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f20a8dd7e64eedda09e1cd70d2008f6e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f20a8dd7e64eedda09e1cd70d2008f6e_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E算法中的基于样本的更新如下，即对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w_h\" alt=\"w_h\" eeimg=\"1\"\u002F\u003E 的所有维度都做如下更新：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d114eb6d0ccaa5b82b9b9ced9a6980b7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1900\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1900\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d114eb6d0ccaa5b82b9b9ced9a6980b7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1900&#39; height=&#39;156&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1900\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1900\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d114eb6d0ccaa5b82b9b9ced9a6980b7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d114eb6d0ccaa5b82b9b9ced9a6980b7_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中记 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28x_%7Bh%2B1%7D%29+%3D+%5Cmax_a+Q%28x_%7Bh%2B1%7D%2C+a%29\" alt=\"V(x_{h+1}) = \\max_a Q(x_{h+1}, a)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E1. Reward Function Term\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E根据 Linear MDP 的假设，奖励函数是确定性的，因此很容易得到\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Cphi%28x%2Ca%29%5ET+%5CLambda_h%5E%7B-1%7D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bk-1%7D+%5Cphi%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29r%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29+%5C%5C+%3D+%26+%5Cphi%28x%2Ca%29%5ET+%5CLambda_h%5E%7B-1%7D+%5Csum_%7B%5Ctau%3D1%7D%5E%7Bk-1%7D+%5Cphi%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29+%5Cphi%5ET%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29+%5Ctheta_h+%5C%5C+%3D+%26+%5Cphi%28x%2Ca%29%5ET+%5Ctheta_h+%5C%5C+%3D+%26+r%28x%2Ca%29+%5C%5C+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\phi(x,a)^T \\Lambda_h^{-1} \\sum_{\\tau=1}^{k-1} \\phi(x_h^\\tau, a_h^\\tau)r(x_h^\\tau, a_h^\\tau) \\\\ = &amp; \\phi(x,a)^T \\Lambda_h^{-1} \\sum_{\\tau=1}^{k-1} \\phi(x_h^\\tau, a_h^\\tau) \\phi^T(x_h^\\tau, a_h^\\tau) \\theta_h \\\\ = &amp; \\phi(x,a)^T \\theta_h \\\\ = &amp; r(x,a) \\\\ \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2. Transition Function Term\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E定义以下两个算子\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af331eed4e3484279ae880518b440de2_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1914\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1914\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af331eed4e3484279ae880518b440de2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1914&#39; height=&#39;150&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1914\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1914\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af331eed4e3484279ae880518b440de2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-af331eed4e3484279ae880518b440de2_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c7e61778b23f3df1bf9c0d4136045c75_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1898\" data-rawheight=\"140\" class=\"origin_image zh-lightbox-thumb\" width=\"1898\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c7e61778b23f3df1bf9c0d4136045c75_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1898&#39; height=&#39;140&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1898\" data-rawheight=\"140\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1898\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c7e61778b23f3df1bf9c0d4136045c75_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c7e61778b23f3df1bf9c0d4136045c75_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E不难看出，算法中的 transition function term 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D\" alt=\"\\hat{\\mathbb{P}}_h V_{h+1}\" eeimg=\"1\"\u002F\u003E ，而在线性模型下 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D+%3D+%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D\" alt=\"\\bar{\\mathbb{P}}_h V_{h+1} = {\\mathbb{P}}_h V_{h+1}\" eeimg=\"1\"\u002F\u003E ，而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7B%5Cmathbb%7BP%7D%7D_h+V_%7Bh%2B1%7D\" alt=\"{\\mathbb{P}}_h V_{h+1}\" eeimg=\"1\"\u002F\u003E 就是 optimal Bellman operation 中对应的项。这一点容易看出：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ad0d88ba166f4f3d1044584ec6706c1c_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1884\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb\" width=\"1884\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ad0d88ba166f4f3d1044584ec6706c1c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1884&#39; height=&#39;138&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1884\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1884\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ad0d88ba166f4f3d1044584ec6706c1c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ad0d88ba166f4f3d1044584ec6706c1c_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E下面的重点就是\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0524006b2412db7a4eef394864c8f724_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1908\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb\" width=\"1908\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0524006b2412db7a4eef394864c8f724_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1908&#39; height=&#39;74&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1908\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1908\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0524006b2412db7a4eef394864c8f724_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0524006b2412db7a4eef394864c8f724_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0cbf20e1e4530aaf5cb35d011c495ffa_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1922\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb\" width=\"1922\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0cbf20e1e4530aaf5cb35d011c495ffa_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1922&#39; height=&#39;74&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1922\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1922\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0cbf20e1e4530aaf5cb35d011c495ffa_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0cbf20e1e4530aaf5cb35d011c495ffa_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E由于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x_%7Bh%2B1%7D%5E%5Ctau+%5Csim+%5Cmathbb%7BP%7D_h%28x_h%5E%5Ctau%2C+a_h%5E%5Ctau%29\" alt=\"x_{h+1}^\\tau \\sim \\mathbb{P}_h(x_h^\\tau, a_h^\\tau)\" eeimg=\"1\"\u002F\u003E ，因此可以使用 concentration inequality 来 bound。但是这里面有一些难处理的地方，这就是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%7Bh%2B1%7D\" alt=\"V_{h+1}\" eeimg=\"1\"\u002F\u003E 每一轮是在变化的，但是这个变化的过程有很难去 track，因此文章直接在如下函数族上做 concentration。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0a720ed2e841aa80f769dc41d2c66529_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1930\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1930\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0a720ed2e841aa80f769dc41d2c66529_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1930&#39; height=&#39;108&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1930\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1930\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0a720ed2e841aa80f769dc41d2c66529_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0a720ed2e841aa80f769dc41d2c66529_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这一个部分给我们的启示是，虽然要准确估计 transition function 很难，但是通过把它往线性基底上面投影，可以使用更少的样本来对其有准确估计。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Cp\u003EUCB 的探索奖励是 Hoeffding type，如果改用 Bernstein type 的 regret 可能可以去掉一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csqrt%7BH%7D\" alt=\"\\sqrt{H}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","type":"topic","id":"19559450","name":"机器学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","type":"topic","id":"19813032","name":"深度学习（Deep Learning）"}],"voteupCount":19,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":1,"contributions":[{"id":22094009,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 97】Linear MDP - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F88931389 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_bullet_gui-4","expPrefix":"vd_bullet_gui","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"gw_mweb_launch-2","expPrefix":"gw_mweb_launch","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"点我，做第一个上屏的人"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F88931389","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F88931389","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>