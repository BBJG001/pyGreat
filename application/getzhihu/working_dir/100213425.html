<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 101】Representation Lower Bound - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="清华大学,计算机科学"/><meta data-react-helmet="true" name="description" content="今天本文的作者之一，姚班毕业的同学，Ruosong Wang 来讲他们的一篇理论工作，还挺有意思的。中关村海华信息技术前沿研究院：讲座预告| 姚班校友王若松将主讲第十七场交叉信息院--海华研究院前沿讲座原文传送门Du,…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 101】Representation Lower Bound"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/100213425"/><meta data-react-helmet="true" property="og:description" content="今天本文的作者之一，姚班毕业的同学，Ruosong Wang 来讲他们的一篇理论工作，还挺有意思的。中关村海华信息技术前沿研究院：讲座预告| 姚班校友王若松将主讲第十七场交叉信息院--海华研究院前沿讲座原文传送门Du,…"/><meta data-react-helmet="true" property="og:image" content="https://pic3.zhimg.com/v2-b1039e03b20b77e927662099d1ebdb50_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:100213425,&quot;title&quot;:&quot;【强化学习 101】Representation Lower Bound&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic3.zhimg.com/v2-b1039e03b20b77e927662099d1ebdb50_1200x500.jpg" alt="【强化学习 101】Representation Lower Bound"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 101】Representation Lower Bound</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">80 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>今天本文的作者之一，姚班毕业的同学，Ruosong Wang 来讲他们的一篇理论工作，还挺有意思的。</p><a target="_blank" href="https://zhuanlan.zhihu.com/p/99340215" data-draft-node="block" data-draft-type="link-card" data-image="https://pic2.zhimg.com/v2-9c0c59db9947d114a2aa43d445074431_180x120.jpg" data-image-width="1920" data-image-height="1080" class="LinkCard LinkCard--hasImage"><span class="LinkCard-backdrop" style="background-image:url(https://pic2.zhimg.com/v2-9c0c59db9947d114a2aa43d445074431_180x120.jpg)"></span><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">中关村海华信息技术前沿研究院：讲座预告| 姚班校友王若松将主讲第十七场交叉信息院--海华研究院前沿讲座</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>zhuanlan.zhihu.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--horizontal" alt="图标" src="https://pic2.zhimg.com/v2-9c0c59db9947d114a2aa43d445074431_180x120.jpg"/></span></span></a><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.03016.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Du, Simon S., et al. &#34;Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?.&#34; arXiv preprint arXiv:1910.03016 (2019).</a></p><h2>特色</h2><p>理论方面的工作，大家都基于很多种不同的假设设计了不同的 provably efficient 算法，给出了相应的 upper bound，这相当于找到了 efficient algorithm 的许多充分条件。但是不同论文里面又都用到了许多很不一样的假设，它们之间的关系又都很难说得清楚。这篇论文反过来思考哪些条件是 efficient algorithm 的必要条件，即如果缺少哪些条件，就不可能设计出 efficient algorithm。</p><p>讲座结束之后跟王若松同学聊了一下，整理梳理了一下最近读到的一些强化学习理论方向的相关工作；另外我发现这篇 paper 的 related work 写的也很棒，因此特别记录下来。特别注意到有哪些已经 solved，有哪些仍然是 open problem， 不同的流派和做法可以按照一些什么样的维度去区分他们的工作。</p><h2>强化学习理论进展</h2><p>首先，强化学习理论问题可以被划分为两大块：</p><p>第一块是 tabular case，即状态空间是离散的情形。在这种情形下，相关的 RL 问题都基本上被解决了，理论分析允许 stochastic transition、stochastic reward、unknown dynamics、arbitrary initial state distribution 等。已知的 upper bound 和 lower bound 只相差了一个 H（planning horizon），大致上 sample complexity 基本为 <img src="https://www.zhihu.com/equation?tex=%5CTheta%28%7CS%7C%7CA%7C%5Ctext%7Bpoly%7D%28H%29%2F%5Cepsilon%5E2%29" alt="\Theta(|S||A|\text{poly}(H)/\epsilon^2)" eeimg="1"/> ，具体的可以参见姜楠老师的 paper [1]。</p><p>第二块是目前大家比较关心的，approximated case，即当状态空间非常大或者连续的时候，我们就不能允许在 sample complexity 里面出现 <img src="https://www.zhihu.com/equation?tex=%7C%5Cmathcal%7BS%7D%7C" alt="|\mathcal{S}|" eeimg="1"/> 项了，因此需要考虑做 function approximation。这一块主要有三个流派：</p><ul><li>Uncertainty-bonus-based algorithm：主要是 bandit problem 的研究思路，对 dynamics 和 function class 做一些假设，然后按照 UCB 或者 Elimination 的思路设计算法。</li><li>Approximate dynamic programming-based algorithm：主要基于 Bellman operator 的 contraction 性质，然后仔细处理 approximation error，对 transition 有一些限制。</li><li>Direct policy search-based algorithm：需要假设有比较均匀的初始状态分布，这样就能利用 gradient domination 和优化领域的一些结论来证明 sample complexity。</li></ul><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-89a732db473a15c79a93abc965cf8b03_b.jpg" data-size="normal" data-rawwidth="2560" data-rawheight="1600" class="origin_image zh-lightbox-thumb" width="2560" data-original="https://pic4.zhimg.com/v2-89a732db473a15c79a93abc965cf8b03_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="2560" data-rawheight="1600" class="origin_image zh-lightbox-thumb lazy" width="2560" data-original="https://pic4.zhimg.com/v2-89a732db473a15c79a93abc965cf8b03_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-89a732db473a15c79a93abc965cf8b03_b.jpg"/><figcaption>绿色标注了我比较熟悉的 paper。最近读 paper 都没有写知乎，这里统一标注一下，欢迎大家一起交流讨论！</figcaption></figure><p>这里的总结说明：目前大家需要对 transition 做各种 low-rank 的假设（第一大块），或者在状态分布上做一些超出 RL 范围的假设（第二、三大块）；如果仅仅只假设有一个好的特征，但是对 dynamics 不做假设（第四大块，本工作），是不可能存在有效的 RL 算法的。</p><p>这篇文章的贡献和之前工作的关系如下</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-d33bc9df40aa11ed5b00ad8d35fa77a3_b.jpg" data-caption="" data-size="normal" data-rawwidth="2028" data-rawheight="1054" class="origin_image zh-lightbox-thumb" width="2028" data-original="https://pic4.zhimg.com/v2-d33bc9df40aa11ed5b00ad8d35fa77a3_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2028&#39; height=&#39;1054&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2028" data-rawheight="1054" class="origin_image zh-lightbox-thumb lazy" width="2028" data-original="https://pic4.zhimg.com/v2-d33bc9df40aa11ed5b00ad8d35fa77a3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-d33bc9df40aa11ed5b00ad8d35fa77a3_b.jpg"/></figure><p>注意到 RL（指定的初始状态分布，只能通过动作/策略来转移到各个不同的状态） &gt; generative model（可以设置为任意的状态） &gt; known transition（整个转移概率函数都完全知道）。因此只要证明前面的某个设定下的 upper bound，即在后面设定有相应的 upper bound；只要证明后面某个设定下的 lower bound，即在前面的设定下有相应的 lower bound。</p><h2>过程</h2><h3>1. 定义</h3><p>这篇文章假设如果有某种比较好的特征，使得 function approximation 能以最简单的线性模型表示的话，那么会有怎样的 upper bound 和 lower bound。那么一个好的特征具体是什么呢？对于 value-based 方法来说，一个好的特征能够使得最优价值函数（或者任意价值函数）都能够被线性表示出来，即</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-076ea36c0cf253bb0b2bad207ba96c10_b.png" data-caption="" data-size="normal" data-rawwidth="1840" data-rawheight="114" class="origin_image zh-lightbox-thumb" width="1840" data-original="https://pic1.zhimg.com/v2-076ea36c0cf253bb0b2bad207ba96c10_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1840&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1840" data-rawheight="114" class="origin_image zh-lightbox-thumb lazy" width="1840" data-original="https://pic1.zhimg.com/v2-076ea36c0cf253bb0b2bad207ba96c10_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-076ea36c0cf253bb0b2bad207ba96c10_b.png"/></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8b7f2d042d14fa7a9db2cfe63984c39c_b.png" data-caption="" data-size="normal" data-rawwidth="1812" data-rawheight="116" class="origin_image zh-lightbox-thumb" width="1812" data-original="https://pic1.zhimg.com/v2-8b7f2d042d14fa7a9db2cfe63984c39c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1812&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1812" data-rawheight="116" class="origin_image zh-lightbox-thumb lazy" width="1812" data-original="https://pic1.zhimg.com/v2-8b7f2d042d14fa7a9db2cfe63984c39c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8b7f2d042d14fa7a9db2cfe63984c39c_b.png"/></figure><p>对于 policy-based 方法来说，一个好的特征是能够表示出最优策略，即</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-a0873c3ddbf305cc3c8a5c8d2d055907_b.png" data-caption="" data-size="normal" data-rawwidth="1798" data-rawheight="126" class="origin_image zh-lightbox-thumb" width="1798" data-original="https://pic4.zhimg.com/v2-a0873c3ddbf305cc3c8a5c8d2d055907_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1798&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1798" data-rawheight="126" class="origin_image zh-lightbox-thumb lazy" width="1798" data-original="https://pic4.zhimg.com/v2-a0873c3ddbf305cc3c8a5c8d2d055907_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-a0873c3ddbf305cc3c8a5c8d2d055907_b.png"/></figure><p>这样的假设显然是比 Q* realizability 弱的。如果把前面的假设看做是 regression 存在合适的 regression 的话，这里相当于是假设在一个 classification 问题中存在一个分界面。考虑到有监督学习的一些通用假设，通常还会假设该分界面存在一个 margin，即</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-c357bfe8cef713f00e5e7ec69d55b442_b.png" data-caption="" data-size="normal" data-rawwidth="1796" data-rawheight="228" class="origin_image zh-lightbox-thumb" width="1796" data-original="https://pic3.zhimg.com/v2-c357bfe8cef713f00e5e7ec69d55b442_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1796&#39; height=&#39;228&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1796" data-rawheight="228" class="origin_image zh-lightbox-thumb lazy" width="1796" data-original="https://pic3.zhimg.com/v2-c357bfe8cef713f00e5e7ec69d55b442_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-c357bfe8cef713f00e5e7ec69d55b442_b.png"/></figure><p>如果有两个 action 太过相似，那么 optimal policy 和 suboptimal policy 就变得难以分辨，所以一般还会假设存在一个 gap。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-609d826d29e2d7a240c86272ef4ff3fc_b.png" data-caption="" data-size="normal" data-rawwidth="1814" data-rawheight="64" class="origin_image zh-lightbox-thumb" width="1814" data-original="https://pic1.zhimg.com/v2-609d826d29e2d7a240c86272ef4ff3fc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1814&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1814" data-rawheight="64" class="origin_image zh-lightbox-thumb lazy" width="1814" data-original="https://pic1.zhimg.com/v2-609d826d29e2d7a240c86272ef4ff3fc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-609d826d29e2d7a240c86272ef4ff3fc_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-d0ddb1b0216d945fbd0e0dace9147505_b.png" data-caption="" data-size="normal" data-rawwidth="1786" data-rawheight="134" class="origin_image zh-lightbox-thumb" width="1786" data-original="https://pic2.zhimg.com/v2-d0ddb1b0216d945fbd0e0dace9147505_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1786&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1786" data-rawheight="134" class="origin_image zh-lightbox-thumb lazy" width="1786" data-original="https://pic2.zhimg.com/v2-d0ddb1b0216d945fbd0e0dace9147505_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-d0ddb1b0216d945fbd0e0dace9147505_b.png"/></figure><h3>2. 算法思路（Upper bound）</h3><p>下面我们证明表中的『exact Linear Q* + gap + generative model』 存在有效的算法。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-be07135b557efe6a4deca6c9c4ceff90_b.png" data-size="normal" data-rawwidth="1854" data-rawheight="156" class="origin_image zh-lightbox-thumb" width="1854" data-original="https://pic1.zhimg.com/v2-be07135b557efe6a4deca6c9c4ceff90_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1854&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1854" data-rawheight="156" class="origin_image zh-lightbox-thumb lazy" width="1854" data-original="https://pic1.zhimg.com/v2-be07135b557efe6a4deca6c9c4ceff90_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-be07135b557efe6a4deca6c9c4ceff90_b.png"/><figcaption>注意到 d 是 feature 的维度</figcaption></figure><p>当 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D0" alt="\delta=0" eeimg="1"/> 并且存在一个 gap， 考虑到 optimal policy 能用 d 维特征的线性组合表示出来，那么用 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bpoly%7D%28d%2C%5Cfrac%7B1%7D%7B%5Crho%7D%29" alt="\text{poly}(d,\frac{1}{\rho})" eeimg="1"/> 的样本就能以 <img src="https://www.zhihu.com/equation?tex=1-%5Cdfrac%7B0.01%7D%7BH%7D" alt="1-\dfrac{0.01}{H}" eeimg="1"/> 的概率分辨出某一层 h 上的 optimal policy；从 H-1 到 1 来跑这个算法；在第 h 层的时候，它后面 Q 的估计可以用已经学到的 h+1 到 H-1 层的 optimal policy 来 rollout。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-7befdd7b581e7c6c60f708dc6244776c_b.png" data-caption="" data-size="normal" data-rawwidth="1818" data-rawheight="122" class="origin_image zh-lightbox-thumb" width="1818" data-original="https://pic1.zhimg.com/v2-7befdd7b581e7c6c60f708dc6244776c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1818&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1818" data-rawheight="122" class="origin_image zh-lightbox-thumb lazy" width="1818" data-original="https://pic1.zhimg.com/v2-7befdd7b581e7c6c60f708dc6244776c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-7befdd7b581e7c6c60f708dc6244776c_b.png"/></figure><p>思路差不多。</p><h3>3. Lower bound</h3><p>下面我们来说明 Approximate Linear Q* + DetMDP 时为什么不能有一个多项式复杂度的算法？（sample complexity 会和 planning horizon H 成指数关系）</p><p><b><i>一个例子</i></b></p><p>先看一个直观的难的例子，说明了即使 feature 造的足够好，以至于线性函数拟合就能够表示真实的价值函数或者策略（value completeness assumption），也还是会有关于 planning horizon H 呈指数的困难。</p><ul><li>Dynamics：考虑有两个 action 的 DetMDP，如果选第一个 action 就走到左子节点，如果选择第二个就走到右子节点。 </li><li>Reward：在最后一层的某一个状态上 reward=1，其他状态 reward=0。</li><li>初步分析：直观来说，由于必须至少把最后一层所有状态都遍历一遍，才能够知道是那个状态上有 reward，因此至少需要 <img src="https://www.zhihu.com/equation?tex=%5COmega%282%5EH%29" alt="\Omega(2^H)" eeimg="1"/> 的样本才够。这是由于，虽然假设了 DetMDP，其中 reward 虽然是确定性的，但是是未知的。</li></ul><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-4bd08df6d61967ce11b2d2c77fd84292_b.jpg" data-caption="" data-size="normal" data-rawwidth="1882" data-rawheight="928" class="origin_image zh-lightbox-thumb" width="1882" data-original="https://pic3.zhimg.com/v2-4bd08df6d61967ce11b2d2c77fd84292_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1882&#39; height=&#39;928&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1882" data-rawheight="928" class="origin_image zh-lightbox-thumb lazy" width="1882" data-original="https://pic3.zhimg.com/v2-4bd08df6d61967ce11b2d2c77fd84292_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4bd08df6d61967ce11b2d2c77fd84292_b.jpg"/></figure><p>注意到，tabular case 下认为状态数目是固定的，这里认为状态会很多（比如这里就有 <img src="https://www.zhihu.com/equation?tex=2%5E%7BH%2B1%7D-1" alt="2^{H+1}-1" eeimg="1"/> 个状态），但是可以用有限维的 feature 来表示。</p><p>先看一个简单的设定：假设如果只有一维 feature，并且这个 feature 是 binary 的，那么假设到达一个状态，其 feature=1，如果它上面 reward=0，那么就可以排除其他所有 feature=1 的状态。这个想法比较粗糙，但是说明一个问题，如果 representation 维度比较低， 就可以通过 representation 的相似程度来基于已知的样本来做泛化，这使得我们不需要把每个状态都访问到就可以探索到 reward function 的形态。</p><p>有了 value completeness assumption 之后真的可以减少我们对于每个状态的访问次数么？最关键的来了，答案是否！我们可以找到一个低维的特征表示 <img src="https://www.zhihu.com/equation?tex=d%3D%5COmega%28H%2F%5Cdelta%5E2%29" alt="d=\Omega(H/\delta^2)" eeimg="1"/> 使得其中能包含 <img src="https://www.zhihu.com/equation?tex=2%5EH" alt="2^H" eeimg="1"/> 个近似的标准正交基！注意到，一个近似的标准正交基一定能对于任意的 reward function 都满足 value completeness assumption；同时，如果为标准正交基，那么探索到一个标准正交向量对应的状态对于标准正交基中其他的向量并不会带来任何的信息量。因此，我们即使有了这样的一个满足 value completeness assumption 的表示，也仍然需要 <img src="https://www.zhihu.com/equation?tex=2%5EH" alt="2^H" eeimg="1"/> 次查询，才能够找到有 reward=1 的那个状态。</p><p><b><i>分析</i></b></p><p>下面从另外一个角度来分析。</p><p>前面讲了当 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D0" alt="\delta=0" eeimg="1"/> 时的一个算法设计思路，最重要的是它利用了 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D0" alt="\delta=0" eeimg="1"/> 这样一个已知条件。这种情况下，考虑一个 d 维的特征，只要我们知道 d 个线性无关的特征和对应的 Q 值，我们就能知道，它们拟合得到的 <img src="https://www.zhihu.com/equation?tex=%5Chat+Q" alt="\hat Q" eeimg="1"/> 一定等于真实的 <img src="https://www.zhihu.com/equation?tex=Q%5E%2A" alt="Q^*" eeimg="1"/> 。</p><p>但是这样情况并不适用于 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%3E+0" alt="\delta &gt; 0" eeimg="1"/> 的情形，考虑下图中左边的例子。考虑 d=2，在 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D0" alt="\delta=0" eeimg="1"/> 和  DetMDP 的情况，如果知道了橙色的两个向量的特征和 Q 值，那么我们就能拟合出线性函数的系数，从而非常安全地去预测任意一个特征（比如绿色的特征）对应的 Q 值。但当 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%3E+0" alt="\delta &gt; 0" eeimg="1"/> 时，我们只知道这样最优的一个拟合对于橙色两个特征的拟合精度在 <img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="\delta" eeimg="1"/> 范围之内，这样系数 w 的取值范围就在一个狭长的区域内了（大家在纸上笔画一下就知道了）。这种情况下，预测一个和橙色向量垂直的向量时，就会产生任意大的误差。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-86bb5a54d1c83af688ba87bc541505f3_b.jpg" data-caption="" data-size="normal" data-rawwidth="1570" data-rawheight="576" class="origin_image zh-lightbox-thumb" width="1570" data-original="https://pic4.zhimg.com/v2-86bb5a54d1c83af688ba87bc541505f3_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1570&#39; height=&#39;576&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1570" data-rawheight="576" class="origin_image zh-lightbox-thumb lazy" width="1570" data-original="https://pic4.zhimg.com/v2-86bb5a54d1c83af688ba87bc541505f3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-86bb5a54d1c83af688ba87bc541505f3_b.jpg"/></figure><p>考虑一个更为一般的情形，看上图右边的例子。在 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D0" alt="\delta=0" eeimg="1"/> 时，采集到差不多 d 个点就能够非常安全地在整个空间上泛化了；但是在 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3E0" alt="\delta&gt;0" eeimg="1"/> 情况下，如果数据采样的分布在上图橙色椭球范围内，我们只能在椭球的主轴方向有比较好的泛化，但是在其垂直的方向上，泛化误差就会很大。</p><p>考虑有 H 层的情形。在第 h 层的时候，各个 state-action pair 对应的 Q 函数是通过已经学习好的 <img src="https://www.zhihu.com/equation?tex=h%2B1%3A+H" alt="h+1: H" eeimg="1"/> 的 Q* 函数得到的，考虑此时在 <img src="https://www.zhihu.com/equation?tex=h%2B1%3A+H" alt="h+1: H" eeimg="1"/> 上已经有了 <img src="https://www.zhihu.com/equation?tex=%5Cdelta" alt="\delta" eeimg="1"/> 的误差。考虑到上面所讲的原因，对于这一层上 <img src="https://www.zhihu.com/equation?tex=%28s%27%2Ca%27%29" alt="(s&#39;,a&#39;)" eeimg="1"/> 的估计误差 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%27" alt="\delta&#39;" eeimg="1"/> 可以写作</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-39ecef293daaf561414179ac189d52f7_b.png" data-caption="" data-size="normal" data-rawwidth="1614" data-rawheight="86" class="origin_image zh-lightbox-thumb" width="1614" data-original="https://pic4.zhimg.com/v2-39ecef293daaf561414179ac189d52f7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1614&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1614" data-rawheight="86" class="origin_image zh-lightbox-thumb lazy" width="1614" data-original="https://pic4.zhimg.com/v2-39ecef293daaf561414179ac189d52f7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-39ecef293daaf561414179ac189d52f7_b.png"/></figure><p>其中 C 表示样本点的 covariance matrix。为了控制红色的这一项比较小（红色的部分小于 1），我们需要采集的样本数目差不多为 <img src="https://www.zhihu.com/equation?tex=O%28d%29" alt="O(d)" eeimg="1"/> 。这样每一层的误差都会被放大 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7Bd%7D" alt="\sqrt{d}" eeimg="1"/> ，这样误差就会呈指数增长。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-89df8d20c783d2cc1b6eb6cc124cd0f5_b.jpg" data-caption="" data-size="normal" data-rawwidth="1760" data-rawheight="512" class="origin_image zh-lightbox-thumb" width="1760" data-original="https://pic2.zhimg.com/v2-89df8d20c783d2cc1b6eb6cc124cd0f5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1760&#39; height=&#39;512&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1760" data-rawheight="512" class="origin_image zh-lightbox-thumb lazy" width="1760" data-original="https://pic2.zhimg.com/v2-89df8d20c783d2cc1b6eb6cc124cd0f5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-89df8d20c783d2cc1b6eb6cc124cd0f5_b.jpg"/></figure><h3>4. 总结</h3><ul><li>在一些 model-based 的假设下（比如前面蓝色表中列的一些工作），已有一些算法能够有效对抗拟合误差。</li><li>困难并不来自于有监督学习的过程，比如我们可以假设 gap、margin 等，但是仍然不解决问题。</li><li>困难也并不来自于环境的 dynamics 未知。如果一个环境的 dynamics 难（比如前面提到的那个二叉树例子），即使把它告诉你，但是不告诉你 reward function，你也需要指数级的样本去探索 reward function。</li><li>最大的困难来自于 distribution mismatch。如果把 RL 看做是 SL（有监督学习），那么智能体一开始并不知道要在哪个分布上做优化。这一点在 Agarwal et al 2019 上也可以看得到，只要做了一个关于 distribution 的简单假设，很多问题都能迎刃而解。</li></ul><hr/><h3>参考文献</h3><p>[1] <a href="https://link.zhihu.com/?target=https%3A//nanjiang.cs.illinois.edu/files/colt18_open_problem.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Jiang, Nan, and Alekh Agarwal. &#34;Open problem: The dependence of sample complexity lower bounds on planning horizon.&#34; Conference On Learning Theory. 2018.</a></p><hr/><h3>Acknowledgement</h3><p>感谢王若松同学的 PPT 和 talk！</p><hr/><h2>关于 <a class="member_mention" href="https://www.zhihu.com/people/9ca670d990e1758b498eb7a4ca3edbba" data-hash="9ca670d990e1758b498eb7a4ca3edbba" data-hovercard="p$b$9ca670d990e1758b498eb7a4ca3edbba">@李英儒</a> 的疑问</h2><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-3accb3a8503081d19fd3e8d52d39e5e5_b.jpg" data-caption="" data-size="normal" data-rawwidth="1085" data-rawheight="1056" class="origin_image zh-lightbox-thumb" width="1085" data-original="https://pic2.zhimg.com/v2-3accb3a8503081d19fd3e8d52d39e5e5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1085&#39; height=&#39;1056&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1085" data-rawheight="1056" class="origin_image zh-lightbox-thumb lazy" width="1085" data-original="https://pic2.zhimg.com/v2-3accb3a8503081d19fd3e8d52d39e5e5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-3accb3a8503081d19fd3e8d52d39e5e5_b.jpg"/></figure><p></p></div></div><div class="ContentItem-time">编辑于 2020-01-04</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19563245" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">清华大学</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19580349" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">计算机科学</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 80 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 80</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>8 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="210d288e-d16e-4a8b-acf4-3caf48a4965b" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="210d288e-d16e-4a8b-acf4-3caf48a4965b">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"100213425":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":100213425,"title":"【强化学习 101】Representation Lower Bound","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F100213425","imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-b1039e03b20b77e927662099d1ebdb50_b.jpg","titleImage":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-b1039e03b20b77e927662099d1ebdb50_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-489d1c971d1319ac0b06ba6b88f6aad6_200x112.png\" data-caption=\"绿色标注了我比较熟悉的 paper。最近读 paper 都没有写知乎，这里统一标注一下，欢迎大家一起交流讨论！\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" data-watermark=\"watermark\" data-original-src=\"v2-489d1c971d1319ac0b06ba6b88f6aad6\" data-watermark-src=\"v2-89a732db473a15c79a93abc965cf8b03\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-489d1c971d1319ac0b06ba6b88f6aad6_r.png\"\u002F\u003E今天本文的作者之一，姚班毕业的同学，Ruosong Wang 来讲他们的一篇理论工作，还挺有意思的。\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F99340215\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9c0c59db9947d114a2aa43d445074431_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\"internal\"\u003E中关村海华信息技术前沿研究院：讲座预告| 姚班校友王若松将主讲第十七场交叉信息院--海华研究院前沿讲座\u003C\u002Fa\u003E原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1910.03016.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EDu, Simon S., et al. &#34;Is a Good Representat…\u003C\u002Fa\u003E","created":1577927904,"updated":1578127204,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1726,"imageHeight":730,"content":"\u003Cp\u003E今天本文的作者之一，姚班毕业的同学，Ruosong Wang 来讲他们的一篇理论工作，还挺有意思的。\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F99340215\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-9c0c59db9947d114a2aa43d445074431_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\"internal\"\u003E中关村海华信息技术前沿研究院：讲座预告| 姚班校友王若松将主讲第十七场交叉信息院--海华研究院前沿讲座\u003C\u002Fa\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1910.03016.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EDu, Simon S., et al. &#34;Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?.&#34; arXiv preprint arXiv:1910.03016 (2019).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E理论方面的工作，大家都基于很多种不同的假设设计了不同的 provably efficient 算法，给出了相应的 upper bound，这相当于找到了 efficient algorithm 的许多充分条件。但是不同论文里面又都用到了许多很不一样的假设，它们之间的关系又都很难说得清楚。这篇论文反过来思考哪些条件是 efficient algorithm 的必要条件，即如果缺少哪些条件，就不可能设计出 efficient algorithm。\u003C\u002Fp\u003E\u003Cp\u003E讲座结束之后跟王若松同学聊了一下，整理梳理了一下最近读到的一些强化学习理论方向的相关工作；另外我发现这篇 paper 的 related work 写的也很棒，因此特别记录下来。特别注意到有哪些已经 solved，有哪些仍然是 open problem， 不同的流派和做法可以按照一些什么样的维度去区分他们的工作。\u003C\u002Fp\u003E\u003Ch2\u003E强化学习理论进展\u003C\u002Fh2\u003E\u003Cp\u003E首先，强化学习理论问题可以被划分为两大块：\u003C\u002Fp\u003E\u003Cp\u003E第一块是 tabular case，即状态空间是离散的情形。在这种情形下，相关的 RL 问题都基本上被解决了，理论分析允许 stochastic transition、stochastic reward、unknown dynamics、arbitrary initial state distribution 等。已知的 upper bound 和 lower bound 只相差了一个 H（planning horizon），大致上 sample complexity 基本为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CTheta%28%7CS%7C%7CA%7C%5Ctext%7Bpoly%7D%28H%29%2F%5Cepsilon%5E2%29\" alt=\"\\Theta(|S||A|\\text{poly}(H)\u002F\\epsilon^2)\" eeimg=\"1\"\u002F\u003E ，具体的可以参见姜楠老师的 paper [1]。\u003C\u002Fp\u003E\u003Cp\u003E第二块是目前大家比较关心的，approximated case，即当状态空间非常大或者连续的时候，我们就不能允许在 sample complexity 里面出现 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%5Cmathcal%7BS%7D%7C\" alt=\"|\\mathcal{S}|\" eeimg=\"1\"\u002F\u003E 项了，因此需要考虑做 function approximation。这一块主要有三个流派：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EUncertainty-bonus-based algorithm：主要是 bandit problem 的研究思路，对 dynamics 和 function class 做一些假设，然后按照 UCB 或者 Elimination 的思路设计算法。\u003C\u002Fli\u003E\u003Cli\u003EApproximate dynamic programming-based algorithm：主要基于 Bellman operator 的 contraction 性质，然后仔细处理 approximation error，对 transition 有一些限制。\u003C\u002Fli\u003E\u003Cli\u003EDirect policy search-based algorithm：需要假设有比较均匀的初始状态分布，这样就能利用 gradient domination 和优化领域的一些结论来证明 sample complexity。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-89a732db473a15c79a93abc965cf8b03_b.jpg\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-89a732db473a15c79a93abc965cf8b03_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-89a732db473a15c79a93abc965cf8b03_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-89a732db473a15c79a93abc965cf8b03_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E绿色标注了我比较熟悉的 paper。最近读 paper 都没有写知乎，这里统一标注一下，欢迎大家一起交流讨论！\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这里的总结说明：目前大家需要对 transition 做各种 low-rank 的假设（第一大块），或者在状态分布上做一些超出 RL 范围的假设（第二、三大块）；如果仅仅只假设有一个好的特征，但是对 dynamics 不做假设（第四大块，本工作），是不可能存在有效的 RL 算法的。\u003C\u002Fp\u003E\u003Cp\u003E这篇文章的贡献和之前工作的关系如下\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d33bc9df40aa11ed5b00ad8d35fa77a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2028\" data-rawheight=\"1054\" class=\"origin_image zh-lightbox-thumb\" width=\"2028\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d33bc9df40aa11ed5b00ad8d35fa77a3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2028&#39; height=&#39;1054&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2028\" data-rawheight=\"1054\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2028\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d33bc9df40aa11ed5b00ad8d35fa77a3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d33bc9df40aa11ed5b00ad8d35fa77a3_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到 RL（指定的初始状态分布，只能通过动作\u002F策略来转移到各个不同的状态） &gt; generative model（可以设置为任意的状态） &gt; known transition（整个转移概率函数都完全知道）。因此只要证明前面的某个设定下的 upper bound，即在后面设定有相应的 upper bound；只要证明后面某个设定下的 lower bound，即在前面的设定下有相应的 lower bound。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1. 定义\u003C\u002Fh3\u003E\u003Cp\u003E这篇文章假设如果有某种比较好的特征，使得 function approximation 能以最简单的线性模型表示的话，那么会有怎样的 upper bound 和 lower bound。那么一个好的特征具体是什么呢？对于 value-based 方法来说，一个好的特征能够使得最优价值函数（或者任意价值函数）都能够被线性表示出来，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-076ea36c0cf253bb0b2bad207ba96c10_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1840\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1840\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-076ea36c0cf253bb0b2bad207ba96c10_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1840&#39; height=&#39;114&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1840\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1840\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-076ea36c0cf253bb0b2bad207ba96c10_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-076ea36c0cf253bb0b2bad207ba96c10_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8b7f2d042d14fa7a9db2cfe63984c39c_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1812\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1812\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8b7f2d042d14fa7a9db2cfe63984c39c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1812&#39; height=&#39;116&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1812\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1812\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8b7f2d042d14fa7a9db2cfe63984c39c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8b7f2d042d14fa7a9db2cfe63984c39c_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E对于 policy-based 方法来说，一个好的特征是能够表示出最优策略，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a0873c3ddbf305cc3c8a5c8d2d055907_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1798\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"1798\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a0873c3ddbf305cc3c8a5c8d2d055907_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1798&#39; height=&#39;126&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1798\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1798\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a0873c3ddbf305cc3c8a5c8d2d055907_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-a0873c3ddbf305cc3c8a5c8d2d055907_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这样的假设显然是比 Q* realizability 弱的。如果把前面的假设看做是 regression 存在合适的 regression 的话，这里相当于是假设在一个 classification 问题中存在一个分界面。考虑到有监督学习的一些通用假设，通常还会假设该分界面存在一个 margin，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c357bfe8cef713f00e5e7ec69d55b442_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1796\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb\" width=\"1796\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c357bfe8cef713f00e5e7ec69d55b442_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1796&#39; height=&#39;228&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1796\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1796\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c357bfe8cef713f00e5e7ec69d55b442_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c357bfe8cef713f00e5e7ec69d55b442_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E如果有两个 action 太过相似，那么 optimal policy 和 suboptimal policy 就变得难以分辨，所以一般还会假设存在一个 gap。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-609d826d29e2d7a240c86272ef4ff3fc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1814\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"1814\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-609d826d29e2d7a240c86272ef4ff3fc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1814&#39; height=&#39;64&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1814\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1814\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-609d826d29e2d7a240c86272ef4ff3fc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-609d826d29e2d7a240c86272ef4ff3fc_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d0ddb1b0216d945fbd0e0dace9147505_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1786\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"1786\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d0ddb1b0216d945fbd0e0dace9147505_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1786&#39; height=&#39;134&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1786\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1786\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d0ddb1b0216d945fbd0e0dace9147505_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d0ddb1b0216d945fbd0e0dace9147505_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E2. 算法思路（Upper bound）\u003C\u002Fh3\u003E\u003Cp\u003E下面我们证明表中的『exact Linear Q* + gap + generative model』 存在有效的算法。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-be07135b557efe6a4deca6c9c4ceff90_b.png\" data-size=\"normal\" data-rawwidth=\"1854\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1854\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-be07135b557efe6a4deca6c9c4ceff90_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1854&#39; height=&#39;156&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1854\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1854\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-be07135b557efe6a4deca6c9c4ceff90_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-be07135b557efe6a4deca6c9c4ceff90_b.png\"\u002F\u003E\u003Cfigcaption\u003E注意到 d 是 feature 的维度\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta%3D0\" alt=\"\\delta=0\" eeimg=\"1\"\u002F\u003E 并且存在一个 gap， 考虑到 optimal policy 能用 d 维特征的线性组合表示出来，那么用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctext%7Bpoly%7D%28d%2C%5Cfrac%7B1%7D%7B%5Crho%7D%29\" alt=\"\\text{poly}(d,\\frac{1}{\\rho})\" eeimg=\"1\"\u002F\u003E 的样本就能以 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1-%5Cdfrac%7B0.01%7D%7BH%7D\" alt=\"1-\\dfrac{0.01}{H}\" eeimg=\"1\"\u002F\u003E 的概率分辨出某一层 h 上的 optimal policy；从 H-1 到 1 来跑这个算法；在第 h 层的时候，它后面 Q 的估计可以用已经学到的 h+1 到 H-1 层的 optimal policy 来 rollout。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7befdd7b581e7c6c60f708dc6244776c_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1818\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"1818\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7befdd7b581e7c6c60f708dc6244776c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1818&#39; height=&#39;122&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1818\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1818\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7befdd7b581e7c6c60f708dc6244776c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7befdd7b581e7c6c60f708dc6244776c_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E思路差不多。\u003C\u002Fp\u003E\u003Ch3\u003E3. Lower bound\u003C\u002Fh3\u003E\u003Cp\u003E下面我们来说明 Approximate Linear Q* + DetMDP 时为什么不能有一个多项式复杂度的算法？（sample complexity 会和 planning horizon H 成指数关系）\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E一个例子\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E先看一个直观的难的例子，说明了即使 feature 造的足够好，以至于线性函数拟合就能够表示真实的价值函数或者策略（value completeness assumption），也还是会有关于 planning horizon H 呈指数的困难。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EDynamics：考虑有两个 action 的 DetMDP，如果选第一个 action 就走到左子节点，如果选择第二个就走到右子节点。 \u003C\u002Fli\u003E\u003Cli\u003EReward：在最后一层的某一个状态上 reward=1，其他状态 reward=0。\u003C\u002Fli\u003E\u003Cli\u003E初步分析：直观来说，由于必须至少把最后一层所有状态都遍历一遍，才能够知道是那个状态上有 reward，因此至少需要 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5COmega%282%5EH%29\" alt=\"\\Omega(2^H)\" eeimg=\"1\"\u002F\u003E 的样本才够。这是由于，虽然假设了 DetMDP，其中 reward 虽然是确定性的，但是是未知的。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4bd08df6d61967ce11b2d2c77fd84292_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1882\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb\" width=\"1882\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4bd08df6d61967ce11b2d2c77fd84292_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1882&#39; height=&#39;928&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1882\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1882\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4bd08df6d61967ce11b2d2c77fd84292_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4bd08df6d61967ce11b2d2c77fd84292_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到，tabular case 下认为状态数目是固定的，这里认为状态会很多（比如这里就有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=2%5E%7BH%2B1%7D-1\" alt=\"2^{H+1}-1\" eeimg=\"1\"\u002F\u003E 个状态），但是可以用有限维的 feature 来表示。\u003C\u002Fp\u003E\u003Cp\u003E先看一个简单的设定：假设如果只有一维 feature，并且这个 feature 是 binary 的，那么假设到达一个状态，其 feature=1，如果它上面 reward=0，那么就可以排除其他所有 feature=1 的状态。这个想法比较粗糙，但是说明一个问题，如果 representation 维度比较低， 就可以通过 representation 的相似程度来基于已知的样本来做泛化，这使得我们不需要把每个状态都访问到就可以探索到 reward function 的形态。\u003C\u002Fp\u003E\u003Cp\u003E有了 value completeness assumption 之后真的可以减少我们对于每个状态的访问次数么？最关键的来了，答案是否！我们可以找到一个低维的特征表示 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d%3D%5COmega%28H%2F%5Cdelta%5E2%29\" alt=\"d=\\Omega(H\u002F\\delta^2)\" eeimg=\"1\"\u002F\u003E 使得其中能包含 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=2%5EH\" alt=\"2^H\" eeimg=\"1\"\u002F\u003E 个近似的标准正交基！注意到，一个近似的标准正交基一定能对于任意的 reward function 都满足 value completeness assumption；同时，如果为标准正交基，那么探索到一个标准正交向量对应的状态对于标准正交基中其他的向量并不会带来任何的信息量。因此，我们即使有了这样的一个满足 value completeness assumption 的表示，也仍然需要 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=2%5EH\" alt=\"2^H\" eeimg=\"1\"\u002F\u003E 次查询，才能够找到有 reward=1 的那个状态。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E分析\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E下面从另外一个角度来分析。\u003C\u002Fp\u003E\u003Cp\u003E前面讲了当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta%3D0\" alt=\"\\delta=0\" eeimg=\"1\"\u002F\u003E 时的一个算法设计思路，最重要的是它利用了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta%3D0\" alt=\"\\delta=0\" eeimg=\"1\"\u002F\u003E 这样一个已知条件。这种情况下，考虑一个 d 维的特征，只要我们知道 d 个线性无关的特征和对应的 Q 值，我们就能知道，它们拟合得到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat+Q\" alt=\"\\hat Q\" eeimg=\"1\"\u002F\u003E 一定等于真实的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%2A\" alt=\"Q^*\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E但是这样情况并不适用于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta+%3E+0\" alt=\"\\delta &gt; 0\" eeimg=\"1\"\u002F\u003E 的情形，考虑下图中左边的例子。考虑 d=2，在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta%3D0\" alt=\"\\delta=0\" eeimg=\"1\"\u002F\u003E 和  DetMDP 的情况，如果知道了橙色的两个向量的特征和 Q 值，那么我们就能拟合出线性函数的系数，从而非常安全地去预测任意一个特征（比如绿色的特征）对应的 Q 值。但当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta+%3E+0\" alt=\"\\delta &gt; 0\" eeimg=\"1\"\u002F\u003E 时，我们只知道这样最优的一个拟合对于橙色两个特征的拟合精度在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta\" alt=\"\\delta\" eeimg=\"1\"\u002F\u003E 范围之内，这样系数 w 的取值范围就在一个狭长的区域内了（大家在纸上笔画一下就知道了）。这种情况下，预测一个和橙色向量垂直的向量时，就会产生任意大的误差。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-86bb5a54d1c83af688ba87bc541505f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb\" width=\"1570\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-86bb5a54d1c83af688ba87bc541505f3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1570&#39; height=&#39;576&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1570\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-86bb5a54d1c83af688ba87bc541505f3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-86bb5a54d1c83af688ba87bc541505f3_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E考虑一个更为一般的情形，看上图右边的例子。在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta%3D0\" alt=\"\\delta=0\" eeimg=\"1\"\u002F\u003E 时，采集到差不多 d 个点就能够非常安全地在整个空间上泛化了；但是在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta%3E0\" alt=\"\\delta&gt;0\" eeimg=\"1\"\u002F\u003E 情况下，如果数据采样的分布在上图橙色椭球范围内，我们只能在椭球的主轴方向有比较好的泛化，但是在其垂直的方向上，泛化误差就会很大。\u003C\u002Fp\u003E\u003Cp\u003E考虑有 H 层的情形。在第 h 层的时候，各个 state-action pair 对应的 Q 函数是通过已经学习好的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%2B1%3A+H\" alt=\"h+1: H\" eeimg=\"1\"\u002F\u003E 的 Q* 函数得到的，考虑此时在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%2B1%3A+H\" alt=\"h+1: H\" eeimg=\"1\"\u002F\u003E 上已经有了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta\" alt=\"\\delta\" eeimg=\"1\"\u002F\u003E 的误差。考虑到上面所讲的原因，对于这一层上 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28s%27%2Ca%27%29\" alt=\"(s&#39;,a&#39;)\" eeimg=\"1\"\u002F\u003E 的估计误差 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdelta%27\" alt=\"\\delta&#39;\" eeimg=\"1\"\u002F\u003E 可以写作\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-39ecef293daaf561414179ac189d52f7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-39ecef293daaf561414179ac189d52f7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1614&#39; height=&#39;86&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1614\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-39ecef293daaf561414179ac189d52f7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-39ecef293daaf561414179ac189d52f7_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中 C 表示样本点的 covariance matrix。为了控制红色的这一项比较小（红色的部分小于 1），我们需要采集的样本数目差不多为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=O%28d%29\" alt=\"O(d)\" eeimg=\"1\"\u002F\u003E 。这样每一层的误差都会被放大 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csqrt%7Bd%7D\" alt=\"\\sqrt{d}\" eeimg=\"1\"\u002F\u003E ，这样误差就会呈指数增长。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-89df8d20c783d2cc1b6eb6cc124cd0f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1760\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb\" width=\"1760\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-89df8d20c783d2cc1b6eb6cc124cd0f5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1760&#39; height=&#39;512&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1760\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1760\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-89df8d20c783d2cc1b6eb6cc124cd0f5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-89df8d20c783d2cc1b6eb6cc124cd0f5_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E4. 总结\u003C\u002Fh3\u003E\u003Cul\u003E\u003Cli\u003E在一些 model-based 的假设下（比如前面蓝色表中列的一些工作），已有一些算法能够有效对抗拟合误差。\u003C\u002Fli\u003E\u003Cli\u003E困难并不来自于有监督学习的过程，比如我们可以假设 gap、margin 等，但是仍然不解决问题。\u003C\u002Fli\u003E\u003Cli\u003E困难也并不来自于环境的 dynamics 未知。如果一个环境的 dynamics 难（比如前面提到的那个二叉树例子），即使把它告诉你，但是不告诉你 reward function，你也需要指数级的样本去探索 reward function。\u003C\u002Fli\u003E\u003Cli\u003E最大的困难来自于 distribution mismatch。如果把 RL 看做是 SL（有监督学习），那么智能体一开始并不知道要在哪个分布上做优化。这一点在 Agarwal et al 2019 上也可以看得到，只要做了一个关于 distribution 的简单假设，很多问题都能迎刃而解。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Chr\u002F\u003E\u003Ch3\u003E参考文献\u003C\u002Fh3\u003E\u003Cp\u003E[1] \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fnanjiang.cs.illinois.edu\u002Ffiles\u002Fcolt18_open_problem.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EJiang, Nan, and Alekh Agarwal. &#34;Open problem: The dependence of sample complexity lower bounds on planning horizon.&#34; Conference On Learning Theory. 2018.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch3\u003EAcknowledgement\u003C\u002Fh3\u003E\u003Cp\u003E感谢王若松同学的 PPT 和 talk！\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E关于 \u003Ca class=\"member_mention\" href=\"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002F9ca670d990e1758b498eb7a4ca3edbba\" data-hash=\"9ca670d990e1758b498eb7a4ca3edbba\" data-hovercard=\"p$b$9ca670d990e1758b498eb7a4ca3edbba\"\u003E@李英儒\u003C\u002Fa\u003E 的疑问\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3accb3a8503081d19fd3e8d52d39e5e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1085\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb\" width=\"1085\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3accb3a8503081d19fd3e8d52d39e5e5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1085&#39; height=&#39;1056&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1085\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1085\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3accb3a8503081d19fd3e8d52d39e5e5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3accb3a8503081d19fd3e8d52d39e5e5_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19563245","type":"topic","id":"19563245","name":"清华大学"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19580349","type":"topic","id":"19580349","name":"计算机科学"}],"voteupCount":80,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":8,"contributions":[{"id":22693022,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 101】Representation Lower Bound - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F100213425 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-3","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"1","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F100213425","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F100213425","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>