<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 91】Kakade&amp;Langford 02&#x27; - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="这么有名的一篇工作竟然没有笔记，今天重新看了一下，补一下笔记。原文传送门Kakade, Sham, and John Langford. &amp;#34;Approximately optimal approximate reinforcement learning.&amp;#34; ICML. Vol. 2. 2002.特色提…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 91】Kakade&amp;Langford 02&#x27;"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/79478298"/><meta data-react-helmet="true" property="og:description" content="这么有名的一篇工作竟然没有笔记，今天重新看了一下，补一下笔记。原文传送门Kakade, Sham, and John Langford. &amp;#34;Approximately optimal approximate reinforcement learning.&amp;#34; ICML. Vol. 2. 2002.特色提…"/><meta data-react-helmet="true" property="og:image" content="https://pic1.zhimg.com/v2-e2f9fe9a8661630e18826bc8c9290571_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:79478298,&quot;title&quot;:&quot;【强化学习 91】Kakade&amp;Langford 02&#x27;&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic1.zhimg.com/v2-e2f9fe9a8661630e18826bc8c9290571_1200x500.jpg" alt="【强化学习 91】Kakade&amp;Langford 02&#x27;"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 91】Kakade&amp;Langford 02&#x27;</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">20 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>这么有名的一篇工作竟然没有笔记，今天重新看了一下，补一下笔记。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=http%3A//homes.cs.washington.edu/~sham/papers/rl/aoarl.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Kakade, Sham, and John Langford. &#34;Approximately optimal approximate reinforcement learning.&#34; ICML. Vol. 2. 2002.</a></p><h2>特色</h2><p>提出了 conservative policy iteration（CPI），在理论分析上使用 restart distribution 来处理探索难的问题，并且把策略评估步（policy evaluation）和策略改进步（policy improvement ）打包形成一个 greedy policy chooser。规定这个 greedy policy chooser 的误差为 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> ，代表这两步综合的 approximation error。文章证明了，当给定 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 时，CPI 方法：1）能在某种 metric 下保证 policy improvement；2）能在有限次调用 greedy policy chooser 之后结束；3）最终的策略 near-optimal。</p><h2>过程</h2><h3>1. 主要想法</h3><p>这篇文章提出 restart distribution 和 greedy policy chooser。具体细节后面都会再提到。</p><ul><li>Restart distribution：策略梯度方法把 exploration 和 exploitation 交杂在一起，即使用当前的（随机）策略进行 rollout 产生样本，然后计算在样本上的策略梯度，并用于更新当前样本。因此对于状态空间的探索全靠当前策略产生，如果当前策略使得某一些重要的状态很难被探索到，则很难学习到最优策略。Restart distribution 的主要想法是，既然全凭当前策略无法充分覆盖状态空间，那么可以让 rollout 的初始状态分布尽可能覆盖到整个状态空间，这样就从另一个角度解决了探索的难题（至少在理论分析上）。</li><li>Greedy policy chooser：如果策略估计步估计准确（exact case），那么每次就选择相对于所估计的价值函数的 greedy policy 即可（one step improvement），这样的操作可以保证 policy improvement 和 optimality，这其实就是 dynamic programming。但是如果策略估计步不准确，那么每次还选择相对于这个不准确估计的 greedy policy，就不能再保证 policy improvement 了。如果策略估计步使用函数近似（approximate case），那么就不能保证最后选择出来的 greedy policy 对应最大的 one step improvement。文章假设在每一步上，该 greedy policy chooser 选择到的策略距离最大的 one step improvement 相差小于 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 。</li><li>考虑以上两个部分之后，文章设计了 CPI 使得：1）能在某种 metric 下保证 policy improvement；2）能在有限次调用 greedy policy chooser 之后结束；3）最终的策略 near-optimal。</li></ul><h3>2. 之前的方法</h3><p>文章说，希望能够设计一个算法使得理论上能够对于以下三个问题有保障：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-24956bcaf4f6e90093ade3c304ecdb32_b.jpg" data-caption="" data-size="normal" data-rawwidth="1230" data-rawheight="276" class="origin_image zh-lightbox-thumb" width="1230" data-original="https://pic3.zhimg.com/v2-24956bcaf4f6e90093ade3c304ecdb32_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1230&#39; height=&#39;276&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1230" data-rawheight="276" class="origin_image zh-lightbox-thumb lazy" width="1230" data-original="https://pic3.zhimg.com/v2-24956bcaf4f6e90093ade3c304ecdb32_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-24956bcaf4f6e90093ade3c304ecdb32_b.jpg"/></figure><p>第一个问题是讲，能不能保障 policy improvement，该 policy improvement 可以不是最后关心的那个 policy performance，而是自己定义的某种 policy performance measure。第二个问题是讲，如果能够保障 policy improvement，那么有没有什么判定依据使得我们知道这一轮还能不能产生相应的 policy improvement。它是在第一个问题基础上提出的，即只有第一个问题有肯定的回答，才会有第二个问题。第三个问题是说，能不能在有限次更新之后得到一个 optimality 的保障，即希望有一个 finite iteration analysis 而不是一个笼统的 convergence and asymptotic analysis。</p><p><b><i>2.1 Exact value function methods (e.g. policy iteration)</i></b></p><p>对于一个策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> ，每次计算得到 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2Ca%29" alt="Q_\pi(s,a)" eeimg="1"/> ，然后把策略更新为相对于 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2Ca%29" alt="Q_\pi(s,a)" eeimg="1"/> 的 greedy policy</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-07eb69dff89dd082fcc561dbbaa37ef0_b.png" data-caption="" data-size="normal" data-rawwidth="1422" data-rawheight="48" class="origin_image zh-lightbox-thumb" width="1422" data-original="https://pic1.zhimg.com/v2-07eb69dff89dd082fcc561dbbaa37ef0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1422&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1422" data-rawheight="48" class="origin_image zh-lightbox-thumb lazy" width="1422" data-original="https://pic1.zhimg.com/v2-07eb69dff89dd082fcc561dbbaa37ef0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-07eb69dff89dd082fcc561dbbaa37ef0_b.png"/></figure><p>(Puterman, 1994) 书中说明这种方法能够保证收敛到 optimal。</p><p><b><i>2.2 Approximate value function methods</i></b></p><p>不再能够得到一个准确的价值函数估计，而是得到一个近似估计 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BV%7D%28s%29" alt="\tilde{V}(s)" eeimg="1"/> ，满足</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-41de3066b9c131c35a04250991a7c06f_b.png" data-caption="" data-size="normal" data-rawwidth="1216" data-rawheight="82" class="origin_image zh-lightbox-thumb" width="1216" data-original="https://pic4.zhimg.com/v2-41de3066b9c131c35a04250991a7c06f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1216&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1216" data-rawheight="82" class="origin_image zh-lightbox-thumb lazy" width="1216" data-original="https://pic4.zhimg.com/v2-41de3066b9c131c35a04250991a7c06f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-41de3066b9c131c35a04250991a7c06f_b.png"/></figure><p><img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> 是 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7BV%7D%28s%29" alt="\tilde{V}(s)" eeimg="1"/> 对应的 greedy policy （这里不太明白，V 函数如何对应相应的 greedy policy，大概认为这里是用的类似 Sutton 书 Chap 6.8 里面讲的 afterstate value function 吧）。这种情况下，不能保证 policy improvement，只能保证性能减小的不太多，即</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-84b3594bbcd885099db7eae0aa6a545b_b.png" data-caption="" data-size="normal" data-rawwidth="1280" data-rawheight="102" class="origin_image zh-lightbox-thumb" width="1280" data-original="https://pic4.zhimg.com/v2-84b3594bbcd885099db7eae0aa6a545b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1280&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1280" data-rawheight="102" class="origin_image zh-lightbox-thumb lazy" width="1280" data-original="https://pic4.zhimg.com/v2-84b3594bbcd885099db7eae0aa6a545b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-84b3594bbcd885099db7eae0aa6a545b_b.png"/></figure><p>从上式可以看出，exact case 时 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon%3D0" alt="\epsilon=0" eeimg="1"/> ，能保证 policy improvement。太</p><p>这说明，该方法不能回答前述第 1、2 个问题。</p><p><b><i>2.3 Policy gradient methods</i></b></p><p>文章中说明要准确估计 policy gradient 的方向所需要的样本会非常地多。</p><p>文章举了一个 sparse reward 的例子，即，除了一个目标状态之外， 其他状态上的奖励都为零。</p><ul><li>On-policy：在该例子中，在策略 far from optimal 的时候，如果让策略去随机采样产生 rollout，需要指数级多的样本才能够采样到目标状态。如果采样不到目标状态，那么估计的策略梯度为零，这样策略也不会做任何更新。</li><li>Off-policy：另外一种自然的想法就是使用更容易到达目标状态的 off-policy 的轨迹来计算策略梯度，同时为了弥补相应的分布的不同，再乘上 importance weight，这其中对应的 importance weight 就会指数级地小，这样在有用的策略梯度方向上只会产生一个很小的值。（文章中说这种情况下 importance weight 会指数级地大，我觉得是弄反了）</li></ul><p>有文章提到策略梯度能够较为准确地被估计，不过注意到在文章中的例子上，策略梯度为零其实是一个比较准确的估计，但是它可能使得收敛指数级地慢。</p><h3>3. Conservative policy iteration</h3><p><b><i>3.1 Conservative policy update and policy improvement lower bound</i></b></p><p>这篇文章最重要的观察是：对于一个任意的策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> ，只要它能产生一个正的 one step improvement，那么就能够找到一个相应的 conservative update <img src="https://www.zhihu.com/equation?tex=%5Cpi_%5Ctext%7Bnew%7D" alt="\pi_\text{new}" eeimg="1"/> ，使得新策略相比于旧策略有 policy improvement。具体的细节如下。</p><p>Policy improvement 可以写作（performance difference lemma）：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-e07eea4600e4b0b4fed7ecff38f95697_b.png" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="200" class="origin_image zh-lightbox-thumb" width="1380" data-original="https://pic4.zhimg.com/v2-e07eea4600e4b0b4fed7ecff38f95697_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1380&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="200" class="origin_image zh-lightbox-thumb lazy" width="1380" data-original="https://pic4.zhimg.com/v2-e07eea4600e4b0b4fed7ecff38f95697_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e07eea4600e4b0b4fed7ecff38f95697_b.png"/></figure><p>而 one step improvement（policy advantage）可以写作：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1b77b3fe10bcb2fa60bb3ae33fda087b_b.png" data-caption="" data-size="normal" data-rawwidth="1312" data-rawheight="68" class="origin_image zh-lightbox-thumb" width="1312" data-original="https://pic4.zhimg.com/v2-1b77b3fe10bcb2fa60bb3ae33fda087b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1312&#39; height=&#39;68&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1312" data-rawheight="68" class="origin_image zh-lightbox-thumb lazy" width="1312" data-original="https://pic4.zhimg.com/v2-1b77b3fe10bcb2fa60bb3ae33fda087b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1b77b3fe10bcb2fa60bb3ae33fda087b_b.png"/></figure><p>注意到两者的区别在于 state-action 的分布。在文中，最后关心的性能是在初始状态分布 <img src="https://www.zhihu.com/equation?tex=D" alt="D" eeimg="1"/> 下得到的性能，即 <img src="https://www.zhihu.com/equation?tex=%5Ceta_D%28%5Cpi%29" alt="\eta_D(\pi)" eeimg="1"/> ；这里考虑的是一个在状态空间中分布更为均匀的 restart distribution <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 下得到的性能，即 <img src="https://www.zhihu.com/equation?tex=%5Ceta_%5Cmu%28%5Cpi%29" alt="\eta_\mu(\pi)" eeimg="1"/> 。</p><p>考虑一个 conservative update rule</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-a3a7fc72861695e26a1b419e509f9508_b.png" data-caption="" data-size="normal" data-rawwidth="1336" data-rawheight="72" class="origin_image zh-lightbox-thumb" width="1336" data-original="https://pic1.zhimg.com/v2-a3a7fc72861695e26a1b419e509f9508_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1336&#39; height=&#39;72&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1336" data-rawheight="72" class="origin_image zh-lightbox-thumb lazy" width="1336" data-original="https://pic1.zhimg.com/v2-a3a7fc72861695e26a1b419e509f9508_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a3a7fc72861695e26a1b419e509f9508_b.png"/></figure><p>利用 policy gradient theorem （Sutton书）可以得到</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-e2ab77b9485c60efa067b0b435779d46_b.png" data-caption="" data-size="normal" data-rawwidth="984" data-rawheight="66" class="origin_image zh-lightbox-thumb" width="984" data-original="https://pic3.zhimg.com/v2-e2ab77b9485c60efa067b0b435779d46_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;984&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="984" data-rawheight="66" class="origin_image zh-lightbox-thumb lazy" width="984" data-original="https://pic3.zhimg.com/v2-e2ab77b9485c60efa067b0b435779d46_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-e2ab77b9485c60efa067b0b435779d46_b.png"/></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-41585f1c9356566b85af162261a02dc9_b.png" data-caption="" data-size="normal" data-rawwidth="1220" data-rawheight="98" class="origin_image zh-lightbox-thumb" width="1220" data-original="https://pic2.zhimg.com/v2-41585f1c9356566b85af162261a02dc9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1220&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1220" data-rawheight="98" class="origin_image zh-lightbox-thumb lazy" width="1220" data-original="https://pic2.zhimg.com/v2-41585f1c9356566b85af162261a02dc9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-41585f1c9356566b85af162261a02dc9_b.png"/></figure><p>这个性质告诉我们，只要这个任意的策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> 使得 one step improvement 大于零，那么我们至少能够用 <img src="https://www.zhihu.com/equation?tex=0%3C%5Calpha+%5Cll+1" alt="0&lt;\alpha \ll 1" eeimg="1"/> 构造一个 conservative policy 使得该策略有大于零的 policy improvement。</p><p>那么选择一个怎样的 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"/> 能够获取一个最大的 policy improvement lower bound 呢？ 顺着这个思路往下，考虑给定 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"/> ，计算 policy improvement 的下界。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-881806da094deab85008068160fb8692_b.jpg" data-caption="" data-size="normal" data-rawwidth="1338" data-rawheight="338" class="origin_image zh-lightbox-thumb" width="1338" data-original="https://pic3.zhimg.com/v2-881806da094deab85008068160fb8692_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1338&#39; height=&#39;338&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1338" data-rawheight="338" class="origin_image zh-lightbox-thumb lazy" width="1338" data-original="https://pic3.zhimg.com/v2-881806da094deab85008068160fb8692_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-881806da094deab85008068160fb8692_b.jpg"/></figure><p>由于这个定理比较重要，因此加了一个红框框。定理的推导也比较直观，policy improvement 和 one step policy improvement 差距产生的原因就在于策略不同导致相应的状态分布不一样。而根据 conservative policy update rule，这两个比较的策略每一步至少有 <img src="https://www.zhihu.com/equation?tex=1-%5Calpha" alt="1-\alpha" eeimg="1"/> 的概率选择相同的行动，从而产生相同的状态分布；而不同的那一部分遵循 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> ，而我们已知它所带来的 one step improvement；剔除这两部分之后，bound 剩余的项即可得到上述定理。</p><p>比较有意思的是当 <img src="https://www.zhihu.com/equation?tex=%5Calpha%3D1" alt="\alpha=1" eeimg="1"/> 时，上述定理变为</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-54ebb89b65c4e60056938e033831c73d_b.png" data-caption="" data-size="normal" data-rawwidth="1190" data-rawheight="98" class="origin_image zh-lightbox-thumb" width="1190" data-original="https://pic2.zhimg.com/v2-54ebb89b65c4e60056938e033831c73d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1190&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1190" data-rawheight="98" class="origin_image zh-lightbox-thumb lazy" width="1190" data-original="https://pic2.zhimg.com/v2-54ebb89b65c4e60056938e033831c73d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-54ebb89b65c4e60056938e033831c73d_b.png"/></figure><p>其形式上和 (3.1) 类似，不过两处地方 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 的含义不太一样，这里是 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon+%3D+%5Cmax_s+%7C%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%27%7D+%5BA_%5Cpi%28s%2Ca%29%5D%7C+%3D+%5Cmax_s+%7C%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%27%7D+%5BQ_%5Cpi%28s%2Ca%29%5D+-+V_%5Cpi%28s%29%7C" alt="\epsilon = \max_s |\mathbb{E}_{a\sim \pi&#39;} [A_\pi(s,a)]| = \max_s |\mathbb{E}_{a\sim \pi&#39;} [Q_\pi(s,a)] - V_\pi(s)|" eeimg="1"/> ，而 (3.1) 中为 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon+%3D+%5Cmax_s+%7C%5Ctilde%7BV%7D%28s%29+-+V_%5Cpi%28s%29%7C%2C+%5Ctext%7Bwhere+%7D+%5Cpi%27%3Dgreedy%28%5Ctilde%7BV%7D%29" alt="\epsilon = \max_s |\tilde{V}(s) - V_\pi(s)|, \text{where } \pi&#39;=greedy(\tilde{V})" eeimg="1"/> 。究竟是否讲的是一回事，有点分不清，原因是不太清楚 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27%3Dgreedy%28%5Ctilde%7BV%7D%29" alt="\pi&#39;=greedy(\tilde{V})" eeimg="1"/>  是怎么来的。</p><p>通过上述定理可以找到一个步长 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"/> 使得 policy improvement lower bound 最大。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d2800bcd8babca83f91ac894fd7f8d24_b.jpg" data-caption="" data-size="normal" data-rawwidth="1240" data-rawheight="330" class="origin_image zh-lightbox-thumb" width="1240" data-original="https://pic1.zhimg.com/v2-d2800bcd8babca83f91ac894fd7f8d24_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1240" data-rawheight="330" class="origin_image zh-lightbox-thumb lazy" width="1240" data-original="https://pic1.zhimg.com/v2-d2800bcd8babca83f91ac894fd7f8d24_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-d2800bcd8babca83f91ac894fd7f8d24_b.jpg"/></figure><p><b><i>3.2 Greedy policy chooser</i></b></p><p>假设我们能够获得一个比较好的找到策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> 的方法，即能找到一个 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> 比最好的能找到的情况差注意到不了多少，即</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-fac754d24501ecfa1ee33e8ff76c8adb_b.jpg" data-caption="" data-size="normal" data-rawwidth="1160" data-rawheight="234" class="origin_image zh-lightbox-thumb" width="1160" data-original="https://pic4.zhimg.com/v2-fac754d24501ecfa1ee33e8ff76c8adb_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1160&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1160" data-rawheight="234" class="origin_image zh-lightbox-thumb lazy" width="1160" data-original="https://pic4.zhimg.com/v2-fac754d24501ecfa1ee33e8ff76c8adb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-fac754d24501ecfa1ee33e8ff76c8adb_b.jpg"/></figure><p>注意，从这里开始一直到最后面， <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 都应该是代表 greedy policy chooser 的误差，和 (3.1) 中的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 以及 Theorem 4.1 中的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 都不一样。Theorem 4.1 中的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 用于得到 Corollary 4.2 之后就没再用到了。</p><p>该 greedy policy chooser 隐含了 policy evaluation 和 policy improvement 两项。它获取的方式可以先得到一个 value function approximator</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1ee9f0ed4f253f98d128dfbf749bd11b_b.png" data-caption="" data-size="normal" data-rawwidth="1086" data-rawheight="92" class="origin_image zh-lightbox-thumb" width="1086" data-original="https://pic4.zhimg.com/v2-1ee9f0ed4f253f98d128dfbf749bd11b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1086&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1086" data-rawheight="92" class="origin_image zh-lightbox-thumb lazy" width="1086" data-original="https://pic4.zhimg.com/v2-1ee9f0ed4f253f98d128dfbf749bd11b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1ee9f0ed4f253f98d128dfbf749bd11b_b.png"/></figure><p>如果它的误差能够控制到小于 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon%2F2" alt="\epsilon/2" eeimg="1"/> ，那么选取关于它的 greedy policy 就可以组成一个 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy policy chooser。显然 greedy policy chooser 也是需要一定的 sample complexity 来完成的，文章把它作为一个黑盒子，没有具体分析。</p><p><b><i>3.3 Sketch of CPI</i></b></p><p>CPI 的大致步骤如下：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-3487094c4bd4e1d29f08df1197bd40f1_b.jpg" data-caption="" data-size="normal" data-rawwidth="1236" data-rawheight="250" class="origin_image zh-lightbox-thumb" width="1236" data-original="https://pic2.zhimg.com/v2-3487094c4bd4e1d29f08df1197bd40f1_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1236&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1236" data-rawheight="250" class="origin_image zh-lightbox-thumb lazy" width="1236" data-original="https://pic2.zhimg.com/v2-3487094c4bd4e1d29f08df1197bd40f1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-3487094c4bd4e1d29f08df1197bd40f1_b.jpg"/></figure><p>由于对于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D" alt="\mathbb{A}" eeimg="1"/> 的估计可能会有一些误差，具体细节如下：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-35b6da90f1f6a976a54aa0b0ef20771c_b.jpg" data-caption="" data-size="normal" data-rawwidth="1452" data-rawheight="410" class="origin_image zh-lightbox-thumb" width="1452" data-original="https://pic1.zhimg.com/v2-35b6da90f1f6a976a54aa0b0ef20771c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1452&#39; height=&#39;410&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1452" data-rawheight="410" class="origin_image zh-lightbox-thumb lazy" width="1452" data-original="https://pic1.zhimg.com/v2-35b6da90f1f6a976a54aa0b0ef20771c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-35b6da90f1f6a976a54aa0b0ef20771c_b.jpg"/></figure><p>关于该算法有如下定理：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-34cbe297e1e1b320d71a81dc9fe3e56f_b.png" data-caption="" data-size="normal" data-rawwidth="1362" data-rawheight="224" class="origin_image zh-lightbox-thumb" width="1362" data-original="https://pic4.zhimg.com/v2-34cbe297e1e1b320d71a81dc9fe3e56f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1362&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1362" data-rawheight="224" class="origin_image zh-lightbox-thumb lazy" width="1362" data-original="https://pic4.zhimg.com/v2-34cbe297e1e1b320d71a81dc9fe3e56f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-34cbe297e1e1b320d71a81dc9fe3e56f_b.png"/></figure><p>该定理回答了之前的几个问题。</p><p>首先，虽然我们关心的是 <img src="https://www.zhihu.com/equation?tex=%5Ceta_D" alt="\eta_D" eeimg="1"/> 但是在使用restart distribution <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/>  之后，可以保证 <img src="https://www.zhihu.com/equation?tex=%5Ceta_%5Cmu" alt="\eta_\mu" eeimg="1"/> 有 policy improvement。</p><p>其次，第 2 步中的结论只需利用 Hoeffding 不等式即可。当第 4 步中的条件满足时，可以保证 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D" alt="\mathbb{A}" eeimg="1"/> 的真实值一定大于 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon+%2F+3" alt="\epsilon / 3" eeimg="1"/> ，这样能够保证每次 improve 的量都不小于 <img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cepsilon%5E2%7D%7B72+R%7D" alt="\dfrac{\epsilon^2}{72 R}" eeimg="1"/> ，其中 <img src="https://www.zhihu.com/equation?tex=R" alt="R" eeimg="1"/> 为最大的 cumulative reward。另外注意到总体 performance 有上界，因此可以算出迭代次数的上界。</p><p>最后，当第 3 步中的条件满足时，可以证 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D" alt="\mathbb{A}" eeimg="1"/> 的真实值一定小于 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon+" alt="\epsilon " eeimg="1"/> ，再考虑到 policy chooser 为 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy，因此可以得知 <img src="https://www.zhihu.com/equation?tex=OPT%28%5Cmathbb%7BA%7D%29%3C2%5Cepsilon" alt="OPT(\mathbb{A})&lt;2\epsilon" eeimg="1"/> 。我们会看到，这个条件可以被转化为 optimality 条件。</p><p><b><i>3.4 Optimality</i></b></p><p>如果从某个 restart distribution 出发，某个策略对应的 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BA%7D_%7B%5Cpi%2C%5Cmu%7D" alt="\mathbb{A}_{\pi,\mu}" eeimg="1"/> 再找不到可以使它大幅改进策略，那么该策略的性能比较接近最优策略的性能。不过前提是 restart distribution 需要和最优策略下的稳态分布比较接近。以下定理说明了这一点，注意把它和专栏前一篇里面的 gradient domination 作比较，gradient domination 说明了 gradient 小的时候接近最优。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c545a2b7148d2180952a587d57d22fa9_b.jpg" data-caption="" data-size="normal" data-rawwidth="1344" data-rawheight="346" class="origin_image zh-lightbox-thumb" width="1344" data-original="https://pic2.zhimg.com/v2-c545a2b7148d2180952a587d57d22fa9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1344&#39; height=&#39;346&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1344" data-rawheight="346" class="origin_image zh-lightbox-thumb lazy" width="1344" data-original="https://pic2.zhimg.com/v2-c545a2b7148d2180952a587d57d22fa9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c545a2b7148d2180952a587d57d22fa9_b.jpg"/></figure><p>其证明过程和 gradient domination 类似。</p><h3>4. 讨论</h3><p>原则上来说存在一个最优策略使得关于 <img src="https://www.zhihu.com/equation?tex=%5Ceta_%5Cmu" alt="\eta_\mu" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Ceta_D" alt="\eta_D" eeimg="1"/> 能够被同时最大化，但是 CPI 只保证每回合 <img src="https://www.zhihu.com/equation?tex=%5Ceta_%5Cmu" alt="\eta_\mu" eeimg="1"/> 有 policy improvement。那要是把算法里面的 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 都换成 <img src="https://www.zhihu.com/equation?tex=D" alt="D" eeimg="1"/> 呢？首先，这样不能保证得到的策略较优。当 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 分布比较均匀的时候，能保证不过最优策略下的稳态分布如何，Corolloary 4.5 中的上界都比较小，而一个分布不太均匀的 <img src="https://www.zhihu.com/equation?tex=D" alt="D" eeimg="1"/> 不能保证这一点。其次，有时候能产生 large advantage 的状态不是从 <img src="https://www.zhihu.com/equation?tex=D" alt="D" eeimg="1"/> 出发能经常访问到的状态（考虑前面 sparse reward 的例子），因此相比于 <img src="https://www.zhihu.com/equation?tex=OPT%28%5Cmathbb%7BA%7D_%7B%5Cpi%2C%5Cmu%7D%29" alt="OPT(\mathbb{A}_{\pi,\mu})" eeimg="1"/> ，不容易得到一个较大的 <img src="https://www.zhihu.com/equation?tex=OPT%28%5Cmathbb%7BA%7D_%7B%5Cpi%2CD%7D%29" alt="OPT(\mathbb{A}_{\pi,D})" eeimg="1"/> 。</p><p>如何选择 restart distribution 呢？可以根据先验选择最优策略容易访问到的状态。</p></div></div><div class="ContentItem-time">发布于 2019-08-23</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 20 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 20</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>1 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="9ede9223-a285-48da-8062-1bc5b5235b19" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="9ede9223-a285-48da-8062-1bc5b5235b19">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"79478298":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":79478298,"title":"【强化学习 91】Kakade&Langford 02'","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F79478298","imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2f9fe9a8661630e18826bc8c9290571_b.jpg","titleImage":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e2f9fe9a8661630e18826bc8c9290571_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5a81c2e3bac44e33e111ca829c3b409f_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"276\" data-watermark=\"watermark\" data-original-src=\"v2-5a81c2e3bac44e33e111ca829c3b409f\" data-watermark-src=\"v2-24956bcaf4f6e90093ade3c304ecdb32\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5a81c2e3bac44e33e111ca829c3b409f_r.png\"\u002F\u003E这么有名的一篇工作竟然没有笔记，今天重新看了一下，补一下笔记。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fhomes.cs.washington.edu\u002F~sham\u002Fpapers\u002Frl\u002Faoarl.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EKakade, Sham, and John Langford. &#34;Approximately optimal approximate reinforcement learning.&#34; ICML. Vol. 2. 2002.\u003C\u002Fa\u003E特色提出了 conservative policy iteration（CPI），在理…","created":1566564839,"updated":1566564839,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":2040,"imageHeight":640,"content":"\u003Cp\u003E这么有名的一篇工作竟然没有笔记，今天重新看了一下，补一下笔记。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fhomes.cs.washington.edu\u002F~sham\u002Fpapers\u002Frl\u002Faoarl.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EKakade, Sham, and John Langford. &#34;Approximately optimal approximate reinforcement learning.&#34; ICML. Vol. 2. 2002.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E提出了 conservative policy iteration（CPI），在理论分析上使用 restart distribution 来处理探索难的问题，并且把策略评估步（policy evaluation）和策略改进步（policy improvement ）打包形成一个 greedy policy chooser。规定这个 greedy policy chooser 的误差为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E ，代表这两步综合的 approximation error。文章证明了，当给定 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 时，CPI 方法：1）能在某种 metric 下保证 policy improvement；2）能在有限次调用 greedy policy chooser 之后结束；3）最终的策略 near-optimal。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1. 主要想法\u003C\u002Fh3\u003E\u003Cp\u003E这篇文章提出 restart distribution 和 greedy policy chooser。具体细节后面都会再提到。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003ERestart distribution：策略梯度方法把 exploration 和 exploitation 交杂在一起，即使用当前的（随机）策略进行 rollout 产生样本，然后计算在样本上的策略梯度，并用于更新当前样本。因此对于状态空间的探索全靠当前策略产生，如果当前策略使得某一些重要的状态很难被探索到，则很难学习到最优策略。Restart distribution 的主要想法是，既然全凭当前策略无法充分覆盖状态空间，那么可以让 rollout 的初始状态分布尽可能覆盖到整个状态空间，这样就从另一个角度解决了探索的难题（至少在理论分析上）。\u003C\u002Fli\u003E\u003Cli\u003EGreedy policy chooser：如果策略估计步估计准确（exact case），那么每次就选择相对于所估计的价值函数的 greedy policy 即可（one step improvement），这样的操作可以保证 policy improvement 和 optimality，这其实就是 dynamic programming。但是如果策略估计步不准确，那么每次还选择相对于这个不准确估计的 greedy policy，就不能再保证 policy improvement 了。如果策略估计步使用函数近似（approximate case），那么就不能保证最后选择出来的 greedy policy 对应最大的 one step improvement。文章假设在每一步上，该 greedy policy chooser 选择到的策略距离最大的 one step improvement 相差小于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003E考虑以上两个部分之后，文章设计了 CPI 使得：1）能在某种 metric 下保证 policy improvement；2）能在有限次调用 greedy policy chooser 之后结束；3）最终的策略 near-optimal。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E2. 之前的方法\u003C\u002Fh3\u003E\u003Cp\u003E文章说，希望能够设计一个算法使得理论上能够对于以下三个问题有保障：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-24956bcaf4f6e90093ade3c304ecdb32_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"1230\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-24956bcaf4f6e90093ade3c304ecdb32_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1230&#39; height=&#39;276&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1230\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-24956bcaf4f6e90093ade3c304ecdb32_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-24956bcaf4f6e90093ade3c304ecdb32_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E第一个问题是讲，能不能保障 policy improvement，该 policy improvement 可以不是最后关心的那个 policy performance，而是自己定义的某种 policy performance measure。第二个问题是讲，如果能够保障 policy improvement，那么有没有什么判定依据使得我们知道这一轮还能不能产生相应的 policy improvement。它是在第一个问题基础上提出的，即只有第一个问题有肯定的回答，才会有第二个问题。第三个问题是说，能不能在有限次更新之后得到一个 optimality 的保障，即希望有一个 finite iteration analysis 而不是一个笼统的 convergence and asymptotic analysis。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2.1 Exact value function methods (e.g. policy iteration)\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E对于一个策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E ，每次计算得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2Ca%29\" alt=\"Q_\\pi(s,a)\" eeimg=\"1\"\u002F\u003E ，然后把策略更新为相对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2Ca%29\" alt=\"Q_\\pi(s,a)\" eeimg=\"1\"\u002F\u003E 的 greedy policy\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-07eb69dff89dd082fcc561dbbaa37ef0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1422\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"1422\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-07eb69dff89dd082fcc561dbbaa37ef0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1422&#39; height=&#39;48&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1422\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1422\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-07eb69dff89dd082fcc561dbbaa37ef0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-07eb69dff89dd082fcc561dbbaa37ef0_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E(Puterman, 1994) 书中说明这种方法能够保证收敛到 optimal。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2.2 Approximate value function methods\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E不再能够得到一个准确的价值函数估计，而是得到一个近似估计 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BV%7D%28s%29\" alt=\"\\tilde{V}(s)\" eeimg=\"1\"\u002F\u003E ，满足\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-41de3066b9c131c35a04250991a7c06f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"1216\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-41de3066b9c131c35a04250991a7c06f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1216&#39; height=&#39;82&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1216\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-41de3066b9c131c35a04250991a7c06f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-41de3066b9c131c35a04250991a7c06f_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E 是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BV%7D%28s%29\" alt=\"\\tilde{V}(s)\" eeimg=\"1\"\u002F\u003E 对应的 greedy policy （这里不太明白，V 函数如何对应相应的 greedy policy，大概认为这里是用的类似 Sutton 书 Chap 6.8 里面讲的 afterstate value function 吧）。这种情况下，不能保证 policy improvement，只能保证性能减小的不太多，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-84b3594bbcd885099db7eae0aa6a545b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-84b3594bbcd885099db7eae0aa6a545b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1280&#39; height=&#39;102&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1280\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-84b3594bbcd885099db7eae0aa6a545b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-84b3594bbcd885099db7eae0aa6a545b_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E从上式可以看出，exact case 时 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon%3D0\" alt=\"\\epsilon=0\" eeimg=\"1\"\u002F\u003E ，能保证 policy improvement。太\u003C\u002Fp\u003E\u003Cp\u003E这说明，该方法不能回答前述第 1、2 个问题。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2.3 Policy gradient methods\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E文章中说明要准确估计 policy gradient 的方向所需要的样本会非常地多。\u003C\u002Fp\u003E\u003Cp\u003E文章举了一个 sparse reward 的例子，即，除了一个目标状态之外， 其他状态上的奖励都为零。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EOn-policy：在该例子中，在策略 far from optimal 的时候，如果让策略去随机采样产生 rollout，需要指数级多的样本才能够采样到目标状态。如果采样不到目标状态，那么估计的策略梯度为零，这样策略也不会做任何更新。\u003C\u002Fli\u003E\u003Cli\u003EOff-policy：另外一种自然的想法就是使用更容易到达目标状态的 off-policy 的轨迹来计算策略梯度，同时为了弥补相应的分布的不同，再乘上 importance weight，这其中对应的 importance weight 就会指数级地小，这样在有用的策略梯度方向上只会产生一个很小的值。（文章中说这种情况下 importance weight 会指数级地大，我觉得是弄反了）\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E有文章提到策略梯度能够较为准确地被估计，不过注意到在文章中的例子上，策略梯度为零其实是一个比较准确的估计，但是它可能使得收敛指数级地慢。\u003C\u002Fp\u003E\u003Ch3\u003E3. Conservative policy iteration\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E3.1 Conservative policy update and policy improvement lower bound\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这篇文章最重要的观察是：对于一个任意的策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E ，只要它能产生一个正的 one step improvement，那么就能够找到一个相应的 conservative update \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_%5Ctext%7Bnew%7D\" alt=\"\\pi_\\text{new}\" eeimg=\"1\"\u002F\u003E ，使得新策略相比于旧策略有 policy improvement。具体的细节如下。\u003C\u002Fp\u003E\u003Cp\u003EPolicy improvement 可以写作（performance difference lemma）：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e07eea4600e4b0b4fed7ecff38f95697_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"1380\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e07eea4600e4b0b4fed7ecff38f95697_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1380&#39; height=&#39;200&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1380\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e07eea4600e4b0b4fed7ecff38f95697_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e07eea4600e4b0b4fed7ecff38f95697_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E而 one step improvement（policy advantage）可以写作：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1b77b3fe10bcb2fa60bb3ae33fda087b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1312\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb\" width=\"1312\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1b77b3fe10bcb2fa60bb3ae33fda087b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1312&#39; height=&#39;68&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1312\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1312\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1b77b3fe10bcb2fa60bb3ae33fda087b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1b77b3fe10bcb2fa60bb3ae33fda087b_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到两者的区别在于 state-action 的分布。在文中，最后关心的性能是在初始状态分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=D\" alt=\"D\" eeimg=\"1\"\u002F\u003E 下得到的性能，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta_D%28%5Cpi%29\" alt=\"\\eta_D(\\pi)\" eeimg=\"1\"\u002F\u003E ；这里考虑的是一个在状态空间中分布更为均匀的 restart distribution \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 下得到的性能，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta_%5Cmu%28%5Cpi%29\" alt=\"\\eta_\\mu(\\pi)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E考虑一个 conservative update rule\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a3a7fc72861695e26a1b419e509f9508_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1336\" data-rawheight=\"72\" class=\"origin_image zh-lightbox-thumb\" width=\"1336\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a3a7fc72861695e26a1b419e509f9508_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1336&#39; height=&#39;72&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1336\" data-rawheight=\"72\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1336\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a3a7fc72861695e26a1b419e509f9508_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a3a7fc72861695e26a1b419e509f9508_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E利用 policy gradient theorem （Sutton书）可以得到\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e2ab77b9485c60efa067b0b435779d46_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"984\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e2ab77b9485c60efa067b0b435779d46_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;984&#39; height=&#39;66&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"984\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e2ab77b9485c60efa067b0b435779d46_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e2ab77b9485c60efa067b0b435779d46_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-41585f1c9356566b85af162261a02dc9_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1220\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb\" width=\"1220\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-41585f1c9356566b85af162261a02dc9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1220&#39; height=&#39;98&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1220\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1220\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-41585f1c9356566b85af162261a02dc9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-41585f1c9356566b85af162261a02dc9_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这个性质告诉我们，只要这个任意的策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E 使得 one step improvement 大于零，那么我们至少能够用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=0%3C%5Calpha+%5Cll+1\" alt=\"0&lt;\\alpha \\ll 1\" eeimg=\"1\"\u002F\u003E 构造一个 conservative policy 使得该策略有大于零的 policy improvement。\u003C\u002Fp\u003E\u003Cp\u003E那么选择一个怎样的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u002F\u003E 能够获取一个最大的 policy improvement lower bound 呢？ 顺着这个思路往下，考虑给定 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u002F\u003E ，计算 policy improvement 的下界。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-881806da094deab85008068160fb8692_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1338\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb\" width=\"1338\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-881806da094deab85008068160fb8692_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1338&#39; height=&#39;338&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1338\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1338\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-881806da094deab85008068160fb8692_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-881806da094deab85008068160fb8692_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E由于这个定理比较重要，因此加了一个红框框。定理的推导也比较直观，policy improvement 和 one step policy improvement 差距产生的原因就在于策略不同导致相应的状态分布不一样。而根据 conservative policy update rule，这两个比较的策略每一步至少有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1-%5Calpha\" alt=\"1-\\alpha\" eeimg=\"1\"\u002F\u003E 的概率选择相同的行动，从而产生相同的状态分布；而不同的那一部分遵循 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E ，而我们已知它所带来的 one step improvement；剔除这两部分之后，bound 剩余的项即可得到上述定理。\u003C\u002Fp\u003E\u003Cp\u003E比较有意思的是当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha%3D1\" alt=\"\\alpha=1\" eeimg=\"1\"\u002F\u003E 时，上述定理变为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-54ebb89b65c4e60056938e033831c73d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb\" width=\"1190\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-54ebb89b65c4e60056938e033831c73d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1190&#39; height=&#39;98&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1190\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-54ebb89b65c4e60056938e033831c73d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-54ebb89b65c4e60056938e033831c73d_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其形式上和 (3.1) 类似，不过两处地方 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 的含义不太一样，这里是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+%3D+%5Cmax_s+%7C%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%27%7D+%5BA_%5Cpi%28s%2Ca%29%5D%7C+%3D+%5Cmax_s+%7C%5Cmathbb%7BE%7D_%7Ba%5Csim+%5Cpi%27%7D+%5BQ_%5Cpi%28s%2Ca%29%5D+-+V_%5Cpi%28s%29%7C\" alt=\"\\epsilon = \\max_s |\\mathbb{E}_{a\\sim \\pi&#39;} [A_\\pi(s,a)]| = \\max_s |\\mathbb{E}_{a\\sim \\pi&#39;} [Q_\\pi(s,a)] - V_\\pi(s)|\" eeimg=\"1\"\u002F\u003E ，而 (3.1) 中为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+%3D+%5Cmax_s+%7C%5Ctilde%7BV%7D%28s%29+-+V_%5Cpi%28s%29%7C%2C+%5Ctext%7Bwhere+%7D+%5Cpi%27%3Dgreedy%28%5Ctilde%7BV%7D%29\" alt=\"\\epsilon = \\max_s |\\tilde{V}(s) - V_\\pi(s)|, \\text{where } \\pi&#39;=greedy(\\tilde{V})\" eeimg=\"1\"\u002F\u003E 。究竟是否讲的是一回事，有点分不清，原因是不太清楚 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27%3Dgreedy%28%5Ctilde%7BV%7D%29\" alt=\"\\pi&#39;=greedy(\\tilde{V})\" eeimg=\"1\"\u002F\u003E  是怎么来的。\u003C\u002Fp\u003E\u003Cp\u003E通过上述定理可以找到一个步长 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"\u002F\u003E 使得 policy improvement lower bound 最大。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2800bcd8babca83f91ac894fd7f8d24_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2800bcd8babca83f91ac894fd7f8d24_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1240&#39; height=&#39;330&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2800bcd8babca83f91ac894fd7f8d24_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d2800bcd8babca83f91ac894fd7f8d24_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E3.2 Greedy policy chooser\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E假设我们能够获得一个比较好的找到策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E 的方法，即能找到一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E 比最好的能找到的情况差注意到不了多少，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fac754d24501ecfa1ee33e8ff76c8adb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"1160\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fac754d24501ecfa1ee33e8ff76c8adb_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1160&#39; height=&#39;234&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1160\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fac754d24501ecfa1ee33e8ff76c8adb_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fac754d24501ecfa1ee33e8ff76c8adb_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意，从这里开始一直到最后面， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 都应该是代表 greedy policy chooser 的误差，和 (3.1) 中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 以及 Theorem 4.1 中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 都不一样。Theorem 4.1 中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 用于得到 Corollary 4.2 之后就没再用到了。\u003C\u002Fp\u003E\u003Cp\u003E该 greedy policy chooser 隐含了 policy evaluation 和 policy improvement 两项。它获取的方式可以先得到一个 value function approximator\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1ee9f0ed4f253f98d128dfbf749bd11b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1086\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"1086\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1ee9f0ed4f253f98d128dfbf749bd11b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1086&#39; height=&#39;92&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1086\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1086\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1ee9f0ed4f253f98d128dfbf749bd11b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1ee9f0ed4f253f98d128dfbf749bd11b_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E如果它的误差能够控制到小于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon%2F2\" alt=\"\\epsilon\u002F2\" eeimg=\"1\"\u002F\u003E ，那么选取关于它的 greedy policy 就可以组成一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy policy chooser。显然 greedy policy chooser 也是需要一定的 sample complexity 来完成的，文章把它作为一个黑盒子，没有具体分析。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E3.3 Sketch of CPI\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003ECPI 的大致步骤如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3487094c4bd4e1d29f08df1197bd40f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"1236\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3487094c4bd4e1d29f08df1197bd40f1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1236&#39; height=&#39;250&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1236\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3487094c4bd4e1d29f08df1197bd40f1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3487094c4bd4e1d29f08df1197bd40f1_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E由于对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BA%7D\" alt=\"\\mathbb{A}\" eeimg=\"1\"\u002F\u003E 的估计可能会有一些误差，具体细节如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-35b6da90f1f6a976a54aa0b0ef20771c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1452\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb\" width=\"1452\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-35b6da90f1f6a976a54aa0b0ef20771c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1452&#39; height=&#39;410&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1452\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1452\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-35b6da90f1f6a976a54aa0b0ef20771c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-35b6da90f1f6a976a54aa0b0ef20771c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E关于该算法有如下定理：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34cbe297e1e1b320d71a81dc9fe3e56f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"1362\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34cbe297e1e1b320d71a81dc9fe3e56f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1362&#39; height=&#39;224&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1362\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34cbe297e1e1b320d71a81dc9fe3e56f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-34cbe297e1e1b320d71a81dc9fe3e56f_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E该定理回答了之前的几个问题。\u003C\u002Fp\u003E\u003Cp\u003E首先，虽然我们关心的是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta_D\" alt=\"\\eta_D\" eeimg=\"1\"\u002F\u003E 但是在使用restart distribution \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E  之后，可以保证 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta_%5Cmu\" alt=\"\\eta_\\mu\" eeimg=\"1\"\u002F\u003E 有 policy improvement。\u003C\u002Fp\u003E\u003Cp\u003E其次，第 2 步中的结论只需利用 Hoeffding 不等式即可。当第 4 步中的条件满足时，可以保证 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BA%7D\" alt=\"\\mathbb{A}\" eeimg=\"1\"\u002F\u003E 的真实值一定大于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+%2F+3\" alt=\"\\epsilon \u002F 3\" eeimg=\"1\"\u002F\u003E ，这样能够保证每次 improve 的量都不小于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdfrac%7B%5Cepsilon%5E2%7D%7B72+R%7D\" alt=\"\\dfrac{\\epsilon^2}{72 R}\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R\" alt=\"R\" eeimg=\"1\"\u002F\u003E 为最大的 cumulative reward。另外注意到总体 performance 有上界，因此可以算出迭代次数的上界。\u003C\u002Fp\u003E\u003Cp\u003E最后，当第 3 步中的条件满足时，可以证 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BA%7D\" alt=\"\\mathbb{A}\" eeimg=\"1\"\u002F\u003E 的真实值一定小于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+\" alt=\"\\epsilon \" eeimg=\"1\"\u002F\u003E ，再考虑到 policy chooser 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy，因此可以得知 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=OPT%28%5Cmathbb%7BA%7D%29%3C2%5Cepsilon\" alt=\"OPT(\\mathbb{A})&lt;2\\epsilon\" eeimg=\"1\"\u002F\u003E 。我们会看到，这个条件可以被转化为 optimality 条件。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E3.4 Optimality\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E如果从某个 restart distribution 出发，某个策略对应的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BA%7D_%7B%5Cpi%2C%5Cmu%7D\" alt=\"\\mathbb{A}_{\\pi,\\mu}\" eeimg=\"1\"\u002F\u003E 再找不到可以使它大幅改进策略，那么该策略的性能比较接近最优策略的性能。不过前提是 restart distribution 需要和最优策略下的稳态分布比较接近。以下定理说明了这一点，注意把它和专栏前一篇里面的 gradient domination 作比较，gradient domination 说明了 gradient 小的时候接近最优。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c545a2b7148d2180952a587d57d22fa9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb\" width=\"1344\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c545a2b7148d2180952a587d57d22fa9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1344&#39; height=&#39;346&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1344\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c545a2b7148d2180952a587d57d22fa9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-c545a2b7148d2180952a587d57d22fa9_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其证明过程和 gradient domination 类似。\u003C\u002Fp\u003E\u003Ch3\u003E4. 讨论\u003C\u002Fh3\u003E\u003Cp\u003E原则上来说存在一个最优策略使得关于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta_%5Cmu\" alt=\"\\eta_\\mu\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta_D\" alt=\"\\eta_D\" eeimg=\"1\"\u002F\u003E 能够被同时最大化，但是 CPI 只保证每回合 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta_%5Cmu\" alt=\"\\eta_\\mu\" eeimg=\"1\"\u002F\u003E 有 policy improvement。那要是把算法里面的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 都换成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=D\" alt=\"D\" eeimg=\"1\"\u002F\u003E 呢？首先，这样不能保证得到的策略较优。当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 分布比较均匀的时候，能保证不过最优策略下的稳态分布如何，Corolloary 4.5 中的上界都比较小，而一个分布不太均匀的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=D\" alt=\"D\" eeimg=\"1\"\u002F\u003E 不能保证这一点。其次，有时候能产生 large advantage 的状态不是从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=D\" alt=\"D\" eeimg=\"1\"\u002F\u003E 出发能经常访问到的状态（考虑前面 sparse reward 的例子），因此相比于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=OPT%28%5Cmathbb%7BA%7D_%7B%5Cpi%2C%5Cmu%7D%29\" alt=\"OPT(\\mathbb{A}_{\\pi,\\mu})\" eeimg=\"1\"\u002F\u003E ，不容易得到一个较大的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=OPT%28%5Cmathbb%7BA%7D_%7B%5Cpi%2CD%7D%29\" alt=\"OPT(\\mathbb{A}_{\\pi,D})\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E如何选择 restart distribution 呢？可以根据先验选择最优策略容易访问到的状态。\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":20,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":1,"contributions":[{"id":21586041,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 91】Kakade&amp;Langford 02&#39; - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F79478298 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F79478298","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F79478298","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>