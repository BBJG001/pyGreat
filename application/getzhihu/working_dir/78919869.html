<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 89】PG Theory 1 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="是非常新的一篇理论工作，从理论上分析 policy gradient 算法相关的各种性质。这篇文章比较长（有 71 页），我们分两次讲，这是第一部分。原文传送门Agarwal, Alekh, et al. &amp;#34;Optimality and Approximation wi…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 89】PG Theory 1"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/78919869"/><meta data-react-helmet="true" property="og:description" content="是非常新的一篇理论工作，从理论上分析 policy gradient 算法相关的各种性质。这篇文章比较长（有 71 页），我们分两次讲，这是第一部分。原文传送门Agarwal, Alekh, et al. &amp;#34;Optimality and Approximation wi…"/><meta data-react-helmet="true" property="og:image" content="https://pic3.zhimg.com/v2-ca6da0474ead68fa57f921a2e5468129_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:78919869,&quot;title&quot;:&quot;【强化学习 89】PG Theory 1&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic3.zhimg.com/v2-ca6da0474ead68fa57f921a2e5468129_1200x500.jpg" alt="【强化学习 89】PG Theory 1"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 89】PG Theory 1</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">40 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>是非常新的一篇理论工作，从理论上分析 policy gradient 算法相关的各种性质。这篇文章比较长（有 71 页），我们分两次讲，这是第一部分。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1908.00261" class=" wrap external" target="_blank" rel="nofollow noreferrer">Agarwal, Alekh, et al. &#34;Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes.&#34; arXiv preprint arXiv:1908.00261 (2019).</a></p><h2>特色</h2><p>看到 Kakade 的强化学习文章，别的不用说，只需要读读读就好了！个人感觉，基于策略的算法 （包括策略梯度类算法）含有 global optimality 和 finite sample analysis 的理论比较少，这里就出现了一篇比较系统讲这件事情的文章，非常值得一读。</p><p>基于策略的算法算法每一轮都使用当前的策略进行采样，这样并不能保证所有的状态都被访问到，更不能保证均匀地访问到所有的状态，在这种情况下，一般很难有 global optimality。（考虑有些可能很『好』的区域根本没有被策略访问到）对比基于值函数的方法（比如 Q-learning、<a href="https://zhuanlan.zhihu.com/p/65759756" class="internal">certainty-equivalence</a>），它们只需要零散的 transition，这样就能要求在足够多的样本上把任意状态访问足够多次，从而有相应的 contraction。</p><p>本文分为两大部分：第一个部分为对 tabular policy parameterization 的分析，即 policy class 一定包含 optimal policy，在这一部分文章中给出了 global optimality 的分析；第二部分为对 restricted policy class 的分析，即 policy class 不一定包含 optimal policy，在这一部分中文字给出了 agnostic result，即相比于该 policy class 中最优 policy 的 loss。这里先讲第一部分。</p><p>这一部分主要贡献为：1）引入 distribution mismatch coefficient，来描述状态空间上访问不均匀的情形；2）引入 gradient domination，说明只要策略梯度较小，则策略接近全局最优；3）给出了不同 parameterization 和不同设定下的分析。</p><h2>过程</h2><h3>1. 综述</h3><p>Policy gradient 算法每次使用当前的策略进行采样，因此每次对于策略参数的更新都更加侧重于当前策略访问频率较高的状态。而要想得到一个最优策略，需要对所有状态（更准确地说是从初始状态分布出发能够 reachable 的所有状态）都有足够的访问频率。该问题就是强化学习里面非常重要的『探索』（exploration）问题。对于该问题，本文引入 distribution mismatch coefficient 来对其进行描述。</p><p>这一部分主要分析四种情况：1）direct parameterized policy classes；2）direct softmax parameterization；3）softmax parameterization with relative entropy regularization；4）softmax parameterization with natural policy gradient ascent。分别对应下表中的含有『Thm』的项目。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-3f216bd28a5f0cf9aa86fff1c860c2d8_b.jpg" data-caption="" data-size="normal" data-rawwidth="1436" data-rawheight="1320" class="origin_image zh-lightbox-thumb" width="1436" data-original="https://pic1.zhimg.com/v2-3f216bd28a5f0cf9aa86fff1c860c2d8_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1436&#39; height=&#39;1320&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1436" data-rawheight="1320" class="origin_image zh-lightbox-thumb lazy" width="1436" data-original="https://pic1.zhimg.com/v2-3f216bd28a5f0cf9aa86fff1c860c2d8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-3f216bd28a5f0cf9aa86fff1c860c2d8_b.jpg"/></figure><p>该表中的 iteration complexity 指需要多少次迭代能够收敛到距离不动点 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 的位置。由于每一次迭代中所用到的价值函数都认为是准确的，而不需要样本去做估计，因此 iteration complexity 可以看做是 sample complexity 的 lower bound。</p><h3>2. 设定</h3><p>和通常的强化学习设定一样，这里只讲一下稍微特殊一点的部分。</p><p><b><i>State visitation distribution</i></b></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-dc8c8c8fee9da91f0d5974e9d7395ba5_b.png" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="102" class="origin_image zh-lightbox-thumb" width="1380" data-original="https://pic2.zhimg.com/v2-dc8c8c8fee9da91f0d5974e9d7395ba5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1380&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="102" class="origin_image zh-lightbox-thumb lazy" width="1380" data-original="https://pic2.zhimg.com/v2-dc8c8c8fee9da91f0d5974e9d7395ba5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-dc8c8c8fee9da91f0d5974e9d7395ba5_b.png"/></figure><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-bf883d8c76b3f96901d1bea48948b99a_b.png" data-caption="" data-size="normal" data-rawwidth="1356" data-rawheight="68" class="origin_image zh-lightbox-thumb" width="1356" data-original="https://pic3.zhimg.com/v2-bf883d8c76b3f96901d1bea48948b99a_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1356&#39; height=&#39;68&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1356" data-rawheight="68" class="origin_image zh-lightbox-thumb lazy" width="1356" data-original="https://pic3.zhimg.com/v2-bf883d8c76b3f96901d1bea48948b99a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-bf883d8c76b3f96901d1bea48948b99a_b.png"/></figure><p>能够归一化为 1 的一般叫做 distribution，否则（没有前面的常数项系数）一般叫 frequency。</p><p><b><i>Policy parameterization</i></b></p><p>对于离散状态空间和离散动作空间，这篇文章研究三种策略参数化的方法：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-0a137984e98311c423813ccacba19f50_b.jpg" data-caption="" data-size="normal" data-rawwidth="1478" data-rawheight="614" class="origin_image zh-lightbox-thumb" width="1478" data-original="https://pic1.zhimg.com/v2-0a137984e98311c423813ccacba19f50_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1478&#39; height=&#39;614&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1478" data-rawheight="614" class="origin_image zh-lightbox-thumb lazy" width="1478" data-original="https://pic1.zhimg.com/v2-0a137984e98311c423813ccacba19f50_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0a137984e98311c423813ccacba19f50_b.jpg"/></figure><p>Direct parameterization 的好处在于对于离散的状态和行动空间，它有绝对完整的表示能力，坏处在于每次梯度更新完之后，不能保证 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5Cin%5CDelta%28A%29%5E%7B%7CS%7C%7D" alt="\theta\in\Delta(A)^{|S|}" eeimg="1"/> ，因此需要 projection step。Softmax parameterization 的好处在于任意 <img src="https://www.zhihu.com/equation?tex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5E%7B%7CS%7C%7CA%7C%7D" alt="\theta \in \mathbb{R}^{|S||A|}" eeimg="1"/> 的参数都代表合法的策略，坏处在于不能表示确定性的策略。</p><p><b><i>Lemmas and definitions</i></b></p><p>强化学习的目标是最大化 <img src="https://www.zhihu.com/equation?tex=f%28%5Ctheta%29+%3D+V%5E%7B%5Cpi_%5Ctheta%7D%28s%29" alt="f(\theta) = V^{\pi_\theta}(s)" eeimg="1"/> ，下面引理说明该目标相对于 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 不是 concave optimization。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-3d602950da99ad8f30b9cfaf519ca2a6_b.png" data-caption="" data-size="normal" data-rawwidth="1362" data-rawheight="96" class="origin_image zh-lightbox-thumb" width="1362" data-original="https://pic3.zhimg.com/v2-3d602950da99ad8f30b9cfaf519ca2a6_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1362&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1362" data-rawheight="96" class="origin_image zh-lightbox-thumb lazy" width="1362" data-original="https://pic3.zhimg.com/v2-3d602950da99ad8f30b9cfaf519ca2a6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-3d602950da99ad8f30b9cfaf519ca2a6_b.png"/></figure><p>这里 Figure 1 就不放了，其实很简单，如果整个 MDP 是奖励稀疏的，很有可能在大多是从参数空间上， <img src="https://www.zhihu.com/equation?tex=f%28%5Ctheta%29" alt="f(\theta)" eeimg="1"/> 都是平的，因此不是 concave。</p><p>接下来是非常熟悉、反复出现的引理：performance difference lemma！</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-2afdb32abdbc389cb462bd0fe9ce980e_b.png" data-caption="" data-size="normal" data-rawwidth="1388" data-rawheight="210" class="origin_image zh-lightbox-thumb" width="1388" data-original="https://pic3.zhimg.com/v2-2afdb32abdbc389cb462bd0fe9ce980e_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1388&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1388" data-rawheight="210" class="origin_image zh-lightbox-thumb lazy" width="1388" data-original="https://pic3.zhimg.com/v2-2afdb32abdbc389cb462bd0fe9ce980e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-2afdb32abdbc389cb462bd0fe9ce980e_b.png"/></figure><p>最后是 distribution mismatch coefficient 的定义</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d98e16c205310a97d12c5bd7729e91c8_b.png" data-caption="" data-size="normal" data-rawwidth="1366" data-rawheight="128" class="origin_image zh-lightbox-thumb" width="1366" data-original="https://pic1.zhimg.com/v2-d98e16c205310a97d12c5bd7729e91c8_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1366&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1366" data-rawheight="128" class="origin_image zh-lightbox-thumb lazy" width="1366" data-original="https://pic1.zhimg.com/v2-d98e16c205310a97d12c5bd7729e91c8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-d98e16c205310a97d12c5bd7729e91c8_b.png"/></figure><p>其中， <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 表示算法进行 rollout 时使用的初始状态分布， <img src="https://www.zhihu.com/equation?tex=%5Crho" alt="\rho" eeimg="1"/> 表示用来进行评价算法性能时使用的初始状态分布。</p><h3>3. Gradient Domination</h3><p>本文的目标是证明 global optimality，而策略梯度算法中我们只知道一些局部的信息，如何使用局部的信息来判断全局的最优性呢？这就需要 gradient domination property 了，简单说来，即：<br/></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-ba1edea839ada485e0c846ae20654869_b.png" data-caption="" data-size="normal" data-rawwidth="1336" data-rawheight="178" class="origin_image zh-lightbox-thumb" width="1336" data-original="https://pic2.zhimg.com/v2-ba1edea839ada485e0c846ae20654869_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1336&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1336" data-rawheight="178" class="origin_image zh-lightbox-thumb lazy" width="1336" data-original="https://pic2.zhimg.com/v2-ba1edea839ada485e0c846ae20654869_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ba1edea839ada485e0c846ae20654869_b.png"/></figure><p>假设有了上属性值，那么假设某个位置上的梯度较小，那么就可以推出这个地方距离全局最优差距不大。下面引理说明了强化学习中策略梯度满足该性质。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0ed50ddf64eec080b38645adf4f7cee9_b.jpg" data-caption="" data-size="normal" data-rawwidth="1382" data-rawheight="480" class="origin_image zh-lightbox-thumb" width="1382" data-original="https://pic2.zhimg.com/v2-0ed50ddf64eec080b38645adf4f7cee9_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1382&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1382" data-rawheight="480" class="origin_image zh-lightbox-thumb lazy" width="1382" data-original="https://pic2.zhimg.com/v2-0ed50ddf64eec080b38645adf4f7cee9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-0ed50ddf64eec080b38645adf4f7cee9_b.jpg"/></figure><p>证明的过程从 performance difference lemma 开始，定理中的第一行右边第一项与策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 有关，考虑到稳态分布和初始分布的关系，能够进一步放缩得到与 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 无关的不等式（只有梯度项与 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 有关）。注意到该引理和策略的参数化方式无关，因为它只管 <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Cpi+V%5E%5Cpi%28%5Cmu%29" alt="\nabla_\pi V^\pi(\mu)" eeimg="1"/> ，不管策略的参数化形式。文章把该引理放到 direct policy parameterization 中，而我单独把这个引理列出来。</p><h3>4. Direct policy parameterization</h3><p>Direct policy parameterization 的策略梯度为：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-201ba9243f457481eda5f07034a448bc_b.png" data-caption="" data-size="normal" data-rawwidth="1420" data-rawheight="94" class="origin_image zh-lightbox-thumb" width="1420" data-original="https://pic1.zhimg.com/v2-201ba9243f457481eda5f07034a448bc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1420&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1420" data-rawheight="94" class="origin_image zh-lightbox-thumb lazy" width="1420" data-original="https://pic1.zhimg.com/v2-201ba9243f457481eda5f07034a448bc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-201ba9243f457481eda5f07034a448bc_b.png"/></figure><p>考虑一个 projected gradient ascent update：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-c9160c2485c99e853ae4e1e6c9cfff9b_b.png" data-caption="" data-size="normal" data-rawwidth="1350" data-rawheight="108" class="origin_image zh-lightbox-thumb" width="1350" data-original="https://pic4.zhimg.com/v2-c9160c2485c99e853ae4e1e6c9cfff9b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1350" data-rawheight="108" class="origin_image zh-lightbox-thumb lazy" width="1350" data-original="https://pic4.zhimg.com/v2-c9160c2485c99e853ae4e1e6c9cfff9b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-c9160c2485c99e853ae4e1e6c9cfff9b_b.png"/></figure><p>有如下的 global optimality + finite iteration result：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-4f33cd8bede995de9ae91496b0c4b699_b.jpg" data-caption="" data-size="normal" data-rawwidth="1398" data-rawheight="260" class="origin_image zh-lightbox-thumb" width="1398" data-original="https://pic2.zhimg.com/v2-4f33cd8bede995de9ae91496b0c4b699_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1398&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1398" data-rawheight="260" class="origin_image zh-lightbox-thumb lazy" width="1398" data-original="https://pic2.zhimg.com/v2-4f33cd8bede995de9ae91496b0c4b699_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-4f33cd8bede995de9ae91496b0c4b699_b.jpg"/></figure><p>这里会用到 Lemma4.1，梯度大小的衡量为 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -stationary，即</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-0543c95992954681a3248e90893f82dc_b.png" data-caption="" data-size="normal" data-rawwidth="612" data-rawheight="31" class="origin_image zh-lightbox-thumb" width="612" data-original="https://pic1.zhimg.com/v2-0543c95992954681a3248e90893f82dc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;612&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="612" data-rawheight="31" class="origin_image zh-lightbox-thumb lazy" width="612" data-original="https://pic1.zhimg.com/v2-0543c95992954681a3248e90893f82dc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0543c95992954681a3248e90893f82dc_b.png"/></figure><p>换句话说，即 stationarity implies optimality。这其中包含的 distribution mismatch coefficient 是必须的，同样考虑一个 sparse reward 的环境，如果策略们都 exponentially hard 地得到非零奖励，那么这些策略附近也是 stationary 的，但是并没有 optimality。利用该思路，可以证明相应的 lower bound，即至少存在某种 MDP 使得 iteration 的数量大致上得有这么多。</p><p><i><u>证明的过程需要看一下，特别是 projection operator 是如何分析和处理的。</u></i></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-94ebca8596386f98ba04c7e39f950e15_b.jpg" data-caption="" data-size="normal" data-rawwidth="1392" data-rawheight="268" class="origin_image zh-lightbox-thumb" width="1392" data-original="https://pic2.zhimg.com/v2-94ebca8596386f98ba04c7e39f950e15_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1392&#39; height=&#39;268&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1392" data-rawheight="268" class="origin_image zh-lightbox-thumb lazy" width="1392" data-original="https://pic2.zhimg.com/v2-94ebca8596386f98ba04c7e39f950e15_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-94ebca8596386f98ba04c7e39f950e15_b.jpg"/></figure><p>该引理给出了一个探索难的例子，说明如果无视探索的问题（即，distribution mismatch coefficient 很大），在梯度较小的情况下，仍然离最优策略较远。</p><h3>5. Softmax parameterization without regularization</h3><p>在 softmax parameterization 下，策略梯度可以写为：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-e1c2c98645b90ea6392a274bd50c4a74_b.png" data-caption="" data-size="normal" data-rawwidth="1392" data-rawheight="102" class="origin_image zh-lightbox-thumb" width="1392" data-original="https://pic1.zhimg.com/v2-e1c2c98645b90ea6392a274bd50c4a74_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1392&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1392" data-rawheight="102" class="origin_image zh-lightbox-thumb lazy" width="1392" data-original="https://pic1.zhimg.com/v2-e1c2c98645b90ea6392a274bd50c4a74_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e1c2c98645b90ea6392a274bd50c4a74_b.png"/></figure><p>考虑一个正常的 gradient ascent 更新：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-50eb66758839f1768e2e2a744ee16103_b.png" data-caption="" data-size="normal" data-rawwidth="1370" data-rawheight="80" class="origin_image zh-lightbox-thumb" width="1370" data-original="https://pic4.zhimg.com/v2-50eb66758839f1768e2e2a744ee16103_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1370&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1370" data-rawheight="80" class="origin_image zh-lightbox-thumb lazy" width="1370" data-original="https://pic4.zhimg.com/v2-50eb66758839f1768e2e2a744ee16103_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-50eb66758839f1768e2e2a744ee16103_b.png"/></figure><p>文章给出了 optimality 的结果，没有 finite iteration 的结果，实际上该方法可能收敛地指数级地慢。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-527dbf1930c4ec6e671a3b31f5488698_b.png" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="188" class="origin_image zh-lightbox-thumb" width="1380" data-original="https://pic1.zhimg.com/v2-527dbf1930c4ec6e671a3b31f5488698_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1380&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="188" class="origin_image zh-lightbox-thumb lazy" width="1380" data-original="https://pic1.zhimg.com/v2-527dbf1930c4ec6e671a3b31f5488698_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-527dbf1930c4ec6e671a3b31f5488698_b.png"/></figure><p>该结果要求算法 rollout 的初始状态分布就要布满整个状态空间，但是原则上来讲，transition dynamics 应该也能把采样的状态带到各个需要的状态上，这个条件不是很必须。不过文章的证明还是用到了这个条件。</p><p>注意到</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-908648f3dd2416be253669f5a94a2de5_b.png" data-caption="" data-size="normal" data-rawwidth="1352" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="1352" data-original="https://pic2.zhimg.com/v2-908648f3dd2416be253669f5a94a2de5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1352&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1352" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="1352" data-original="https://pic2.zhimg.com/v2-908648f3dd2416be253669f5a94a2de5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-908648f3dd2416be253669f5a94a2de5_b.png"/></figure><p>即，和 direct policy parameterization 不一样的地方在于， <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+V" alt="\nabla_\theta V" eeimg="1"/> 较小不代表 <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Cpi+V" alt="\nabla_\pi V" eeimg="1"/> 很小。因此， <img src="https://www.zhihu.com/equation?tex=%5Cnabla_%5Ctheta+V" alt="\nabla_\theta V" eeimg="1"/> 较小的时候，更新就比较缓慢了，但实际上可能离最优策略还比较远。这就是该算法可能到最后收敛缓慢的原因。后面会讲到，这个问题可以通过增加 entropic regularization （代价是，根据正则项强度的不同，会产生大小不同的 bias）和使用 natural policy gradient 来解决。</p><h3>6. Softmax parameterization with relative entropy regularization</h3><p>首先，区分一下常用的两种熵正则：entropy / relative entropy。</p><p>Relative entropy 指相对于一个均匀分布的相对熵，即，目标函数变为：</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-13ff7cf9a7e11d07b03bec8cada5c4cc_b.png" data-caption="" data-size="normal" data-rawwidth="1378" data-rawheight="204" class="origin_image zh-lightbox-thumb" width="1378" data-original="https://pic1.zhimg.com/v2-13ff7cf9a7e11d07b03bec8cada5c4cc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1378&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1378" data-rawheight="204" class="origin_image zh-lightbox-thumb lazy" width="1378" data-original="https://pic1.zhimg.com/v2-13ff7cf9a7e11d07b03bec8cada5c4cc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-13ff7cf9a7e11d07b03bec8cada5c4cc_b.png"/></figure><p>由于后面要对它求导，因此最后一个常数项可以被扔掉。</p><p>Entropy 指的是策略自身的熵：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-071506c80bb3374099419b3dbebea8f5_b.png" data-caption="" data-size="normal" data-rawwidth="1360" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="1360" data-original="https://pic2.zhimg.com/v2-071506c80bb3374099419b3dbebea8f5_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1360&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1360" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="1360" data-original="https://pic2.zhimg.com/v2-071506c80bb3374099419b3dbebea8f5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-071506c80bb3374099419b3dbebea8f5_b.png"/></figure><p>区别在于 relative entropy 中蓝色框的部分为 <img src="https://www.zhihu.com/equation?tex=1%2F%7C%5Cmathcal%7BA%7D%7C" alt="1/|\mathcal{A}|" eeimg="1"/> 。可以看出，relative entropy 对于确定性策略（在其他行动上产生非常小的 probability）的惩罚更大。文章这里分析 relative entropy 正则项的情形。</p><p>考虑如下更新策略：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-919b5c752c62540c6993bc7c683c58fd_b.png" data-caption="" data-size="normal" data-rawwidth="1370" data-rawheight="62" class="origin_image zh-lightbox-thumb" width="1370" data-original="https://pic2.zhimg.com/v2-919b5c752c62540c6993bc7c683c58fd_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1370&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1370" data-rawheight="62" class="origin_image zh-lightbox-thumb lazy" width="1370" data-original="https://pic2.zhimg.com/v2-919b5c752c62540c6993bc7c683c58fd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-919b5c752c62540c6993bc7c683c58fd_b.png"/></figure><p>有如下 global optimality + finite iteration result：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-a06c5318097a576e7b1d275a044c9dc2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1384" data-rawheight="326" class="origin_image zh-lightbox-thumb" width="1384" data-original="https://pic3.zhimg.com/v2-a06c5318097a576e7b1d275a044c9dc2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1384&#39; height=&#39;326&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1384" data-rawheight="326" class="origin_image zh-lightbox-thumb lazy" width="1384" data-original="https://pic3.zhimg.com/v2-a06c5318097a576e7b1d275a044c9dc2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-a06c5318097a576e7b1d275a044c9dc2_b.jpg"/></figure><p>注意到一点，需要先知道所需要的精度 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 再确定正则项的强度 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"/> ，并且如果要求的精度高，则需要少加一些正则，这样才能够保证 bias 足够小，相应的代价是需要更多的 iteration。</p><h3>7. Softmax parameterization with natural policy gradient</h3><p>考虑如下更新 natural policy gradient 的更新公式</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-7abc8472bf72f1637ebe8e286afd9632_b.png" data-caption="" data-size="normal" data-rawwidth="1398" data-rawheight="200" class="origin_image zh-lightbox-thumb" width="1398" data-original="https://pic3.zhimg.com/v2-7abc8472bf72f1637ebe8e286afd9632_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1398&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1398" data-rawheight="200" class="origin_image zh-lightbox-thumb lazy" width="1398" data-original="https://pic3.zhimg.com/v2-7abc8472bf72f1637ebe8e286afd9632_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-7abc8472bf72f1637ebe8e286afd9632_b.png"/></figure><p>softmax parameterization + NPG updates 能够得到 closed-form 的 update rule，文章直接引用了一下结论：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-075a3e8f7723297745f7fe92b02fc9f2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1386" data-rawheight="246" class="origin_image zh-lightbox-thumb" width="1386" data-original="https://pic3.zhimg.com/v2-075a3e8f7723297745f7fe92b02fc9f2_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1386&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1386" data-rawheight="246" class="origin_image zh-lightbox-thumb lazy" width="1386" data-original="https://pic3.zhimg.com/v2-075a3e8f7723297745f7fe92b02fc9f2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-075a3e8f7723297745f7fe92b02fc9f2_b.jpg"/></figure><p><i><u>这个是怎么来的日后还需要看一下，特别是 NPG update rule 应该怎么分析。</u></i></p><p>以下引理证明了 policy improvement：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0fe33290b3ee0fbfd820005c6efeb395_b.png" data-caption="" data-size="normal" data-rawwidth="1336" data-rawheight="199" class="origin_image zh-lightbox-thumb" width="1336" data-original="https://pic2.zhimg.com/v2-0fe33290b3ee0fbfd820005c6efeb395_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1336&#39; height=&#39;199&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1336" data-rawheight="199" class="origin_image zh-lightbox-thumb lazy" width="1336" data-original="https://pic2.zhimg.com/v2-0fe33290b3ee0fbfd820005c6efeb395_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-0fe33290b3ee0fbfd820005c6efeb395_b.png"/></figure><p>其证明过程感觉还是很巧妙的。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-ef87466382bf2b792f9b9f1cb2b26c51_b.jpg" data-caption="" data-size="normal" data-rawwidth="1373" data-rawheight="886" class="origin_image zh-lightbox-thumb" width="1373" data-original="https://pic2.zhimg.com/v2-ef87466382bf2b792f9b9f1cb2b26c51_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1373&#39; height=&#39;886&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1373" data-rawheight="886" class="origin_image zh-lightbox-thumb lazy" width="1373" data-original="https://pic2.zhimg.com/v2-ef87466382bf2b792f9b9f1cb2b26c51_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ef87466382bf2b792f9b9f1cb2b26c51_b.jpg"/></figure><p>以下定理说明了 softmax parameterization + NPG updates 的性质（global optimality + finite iteration）</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-74de7481104d7fe20275745217eabff7_b.jpg" data-caption="" data-size="normal" data-rawwidth="1388" data-rawheight="434" class="origin_image zh-lightbox-thumb" width="1388" data-original="https://pic4.zhimg.com/v2-74de7481104d7fe20275745217eabff7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1388&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1388" data-rawheight="434" class="origin_image zh-lightbox-thumb lazy" width="1388" data-original="https://pic4.zhimg.com/v2-74de7481104d7fe20275745217eabff7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-74de7481104d7fe20275745217eabff7_b.jpg"/></figure><p>证明过程用到前面的 policy improvement lemma，考虑然后创造能够前后抵消的项。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-1e7c428e906b709391480eaec85aa974_b.jpg" data-caption="" data-size="normal" data-rawwidth="745" data-rawheight="752" class="origin_image zh-lightbox-thumb" width="745" data-original="https://pic1.zhimg.com/v2-1e7c428e906b709391480eaec85aa974_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;745&#39; height=&#39;752&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="745" data-rawheight="752" class="origin_image zh-lightbox-thumb lazy" width="745" data-original="https://pic1.zhimg.com/v2-1e7c428e906b709391480eaec85aa974_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-1e7c428e906b709391480eaec85aa974_b.jpg"/></figure><p>其中最后一个不等式应该略去了 <img src="https://www.zhihu.com/equation?tex=R_%5Cmax" alt="R_\max" eeimg="1"/> 。</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p></div></div><div class="ContentItem-time">编辑于 2019-08-22</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 40 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 40</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>4 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="3db9e262-9ee6-446e-89af-7ffd7c1b8c49" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="3db9e262-9ee6-446e-89af-7ffd7c1b8c49">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"78919869":{"id":78919869,"title":"【强化学习 89】PG Theory 1","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F78919869","imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ca6da0474ead68fa57f921a2e5468129_b.jpg","titleImage":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ca6da0474ead68fa57f921a2e5468129_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ce09268a626adbe2a9abef2da7500313_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"1320\" data-watermark=\"watermark\" data-original-src=\"v2-ce09268a626adbe2a9abef2da7500313\" data-watermark-src=\"v2-3f216bd28a5f0cf9aa86fff1c860c2d8\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ce09268a626adbe2a9abef2da7500313_r.png\"\u002F\u003E是非常新的一篇理论工作，从理论上分析 policy gradient 算法相关的各种性质。这篇文章比较长（有 71 页），我们分两次讲，这是第一部分。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1908.00261\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EAgarwal, Alekh, et al. &#34;Optimality and Approximation with Policy Gradient Methods in Markov Decisio…\u003C\u002Fa\u003E","created":1566297170,"updated":1566404223,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1998,"imageHeight":662,"content":"\u003Cp\u003E是非常新的一篇理论工作，从理论上分析 policy gradient 算法相关的各种性质。这篇文章比较长（有 71 页），我们分两次讲，这是第一部分。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1908.00261\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EAgarwal, Alekh, et al. &#34;Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes.&#34; arXiv preprint arXiv:1908.00261 (2019).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003E看到 Kakade 的强化学习文章，别的不用说，只需要读读读就好了！个人感觉，基于策略的算法 （包括策略梯度类算法）含有 global optimality 和 finite sample analysis 的理论比较少，这里就出现了一篇比较系统讲这件事情的文章，非常值得一读。\u003C\u002Fp\u003E\u003Cp\u003E基于策略的算法算法每一轮都使用当前的策略进行采样，这样并不能保证所有的状态都被访问到，更不能保证均匀地访问到所有的状态，在这种情况下，一般很难有 global optimality。（考虑有些可能很『好』的区域根本没有被策略访问到）对比基于值函数的方法（比如 Q-learning、\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F65759756\" class=\"internal\"\u003Ecertainty-equivalence\u003C\u002Fa\u003E），它们只需要零散的 transition，这样就能要求在足够多的样本上把任意状态访问足够多次，从而有相应的 contraction。\u003C\u002Fp\u003E\u003Cp\u003E本文分为两大部分：第一个部分为对 tabular policy parameterization 的分析，即 policy class 一定包含 optimal policy，在这一部分文章中给出了 global optimality 的分析；第二部分为对 restricted policy class 的分析，即 policy class 不一定包含 optimal policy，在这一部分中文字给出了 agnostic result，即相比于该 policy class 中最优 policy 的 loss。这里先讲第一部分。\u003C\u002Fp\u003E\u003Cp\u003E这一部分主要贡献为：1）引入 distribution mismatch coefficient，来描述状态空间上访问不均匀的情形；2）引入 gradient domination，说明只要策略梯度较小，则策略接近全局最优；3）给出了不同 parameterization 和不同设定下的分析。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1. 综述\u003C\u002Fh3\u003E\u003Cp\u003EPolicy gradient 算法每次使用当前的策略进行采样，因此每次对于策略参数的更新都更加侧重于当前策略访问频率较高的状态。而要想得到一个最优策略，需要对所有状态（更准确地说是从初始状态分布出发能够 reachable 的所有状态）都有足够的访问频率。该问题就是强化学习里面非常重要的『探索』（exploration）问题。对于该问题，本文引入 distribution mismatch coefficient 来对其进行描述。\u003C\u002Fp\u003E\u003Cp\u003E这一部分主要分析四种情况：1）direct parameterized policy classes；2）direct softmax parameterization；3）softmax parameterization with relative entropy regularization；4）softmax parameterization with natural policy gradient ascent。分别对应下表中的含有『Thm』的项目。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3f216bd28a5f0cf9aa86fff1c860c2d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"1320\" class=\"origin_image zh-lightbox-thumb\" width=\"1436\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3f216bd28a5f0cf9aa86fff1c860c2d8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1436&#39; height=&#39;1320&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"1320\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1436\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3f216bd28a5f0cf9aa86fff1c860c2d8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-3f216bd28a5f0cf9aa86fff1c860c2d8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E该表中的 iteration complexity 指需要多少次迭代能够收敛到距离不动点 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 的位置。由于每一次迭代中所用到的价值函数都认为是准确的，而不需要样本去做估计，因此 iteration complexity 可以看做是 sample complexity 的 lower bound。\u003C\u002Fp\u003E\u003Ch3\u003E2. 设定\u003C\u002Fh3\u003E\u003Cp\u003E和通常的强化学习设定一样，这里只讲一下稍微特殊一点的部分。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EState visitation distribution\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc8c8c8fee9da91f0d5974e9d7395ba5_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"1380\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc8c8c8fee9da91f0d5974e9d7395ba5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1380&#39; height=&#39;102&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1380\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc8c8c8fee9da91f0d5974e9d7395ba5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-dc8c8c8fee9da91f0d5974e9d7395ba5_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-bf883d8c76b3f96901d1bea48948b99a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1356\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb\" width=\"1356\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-bf883d8c76b3f96901d1bea48948b99a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1356&#39; height=&#39;68&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1356\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1356\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-bf883d8c76b3f96901d1bea48948b99a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-bf883d8c76b3f96901d1bea48948b99a_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E能够归一化为 1 的一般叫做 distribution，否则（没有前面的常数项系数）一般叫 frequency。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003EPolicy parameterization\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E对于离散状态空间和离散动作空间，这篇文章研究三种策略参数化的方法：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0a137984e98311c423813ccacba19f50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1478\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb\" width=\"1478\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0a137984e98311c423813ccacba19f50_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1478&#39; height=&#39;614&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1478\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1478\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0a137984e98311c423813ccacba19f50_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0a137984e98311c423813ccacba19f50_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EDirect parameterization 的好处在于对于离散的状态和行动空间，它有绝对完整的表示能力，坏处在于每次梯度更新完之后，不能保证 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta%5Cin%5CDelta%28A%29%5E%7B%7CS%7C%7D\" alt=\"\\theta\\in\\Delta(A)^{|S|}\" eeimg=\"1\"\u002F\u003E ，因此需要 projection step。Softmax parameterization 的好处在于任意 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5E%7B%7CS%7C%7CA%7C%7D\" alt=\"\\theta \\in \\mathbb{R}^{|S||A|}\" eeimg=\"1\"\u002F\u003E 的参数都代表合法的策略，坏处在于不能表示确定性的策略。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003ELemmas and definitions\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E强化学习的目标是最大化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28%5Ctheta%29+%3D+V%5E%7B%5Cpi_%5Ctheta%7D%28s%29\" alt=\"f(\\theta) = V^{\\pi_\\theta}(s)\" eeimg=\"1\"\u002F\u003E ，下面引理说明该目标相对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"\u002F\u003E 不是 concave optimization。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3d602950da99ad8f30b9cfaf519ca2a6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb\" width=\"1362\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3d602950da99ad8f30b9cfaf519ca2a6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1362&#39; height=&#39;96&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1362\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1362\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3d602950da99ad8f30b9cfaf519ca2a6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3d602950da99ad8f30b9cfaf519ca2a6_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这里 Figure 1 就不放了，其实很简单，如果整个 MDP 是奖励稀疏的，很有可能在大多是从参数空间上， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28%5Ctheta%29\" alt=\"f(\\theta)\" eeimg=\"1\"\u002F\u003E 都是平的，因此不是 concave。\u003C\u002Fp\u003E\u003Cp\u003E接下来是非常熟悉、反复出现的引理：performance difference lemma！\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2afdb32abdbc389cb462bd0fe9ce980e_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb\" width=\"1388\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2afdb32abdbc389cb462bd0fe9ce980e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1388&#39; height=&#39;210&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1388\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2afdb32abdbc389cb462bd0fe9ce980e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2afdb32abdbc389cb462bd0fe9ce980e_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E最后是 distribution mismatch coefficient 的定义\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d98e16c205310a97d12c5bd7729e91c8_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1366\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"1366\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d98e16c205310a97d12c5bd7729e91c8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1366&#39; height=&#39;128&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1366\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1366\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d98e16c205310a97d12c5bd7729e91c8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d98e16c205310a97d12c5bd7729e91c8_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 表示算法进行 rollout 时使用的初始状态分布， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"\u002F\u003E 表示用来进行评价算法性能时使用的初始状态分布。\u003C\u002Fp\u003E\u003Ch3\u003E3. Gradient Domination\u003C\u002Fh3\u003E\u003Cp\u003E本文的目标是证明 global optimality，而策略梯度算法中我们只知道一些局部的信息，如何使用局部的信息来判断全局的最优性呢？这就需要 gradient domination property 了，简单说来，即：\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba1edea839ada485e0c846ae20654869_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1336\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1336\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba1edea839ada485e0c846ae20654869_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1336&#39; height=&#39;178&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1336\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1336\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba1edea839ada485e0c846ae20654869_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ba1edea839ada485e0c846ae20654869_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E假设有了上属性值，那么假设某个位置上的梯度较小，那么就可以推出这个地方距离全局最优差距不大。下面引理说明了强化学习中策略梯度满足该性质。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0ed50ddf64eec080b38645adf4f7cee9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1382\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"1382\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0ed50ddf64eec080b38645adf4f7cee9_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1382&#39; height=&#39;480&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1382\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1382\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0ed50ddf64eec080b38645adf4f7cee9_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0ed50ddf64eec080b38645adf4f7cee9_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E证明的过程从 performance difference lemma 开始，定理中的第一行右边第一项与策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 有关，考虑到稳态分布和初始分布的关系，能够进一步放缩得到与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 无关的不等式（只有梯度项与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 有关）。注意到该引理和策略的参数化方式无关，因为它只管 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Cpi+V%5E%5Cpi%28%5Cmu%29\" alt=\"\\nabla_\\pi V^\\pi(\\mu)\" eeimg=\"1\"\u002F\u003E ，不管策略的参数化形式。文章把该引理放到 direct policy parameterization 中，而我单独把这个引理列出来。\u003C\u002Fp\u003E\u003Ch3\u003E4. Direct policy parameterization\u003C\u002Fh3\u003E\u003Cp\u003EDirect policy parameterization 的策略梯度为：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-201ba9243f457481eda5f07034a448bc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1420\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"1420\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-201ba9243f457481eda5f07034a448bc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1420&#39; height=&#39;94&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1420\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1420\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-201ba9243f457481eda5f07034a448bc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-201ba9243f457481eda5f07034a448bc_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E考虑一个 projected gradient ascent update：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c9160c2485c99e853ae4e1e6c9cfff9b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c9160c2485c99e853ae4e1e6c9cfff9b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1350&#39; height=&#39;108&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c9160c2485c99e853ae4e1e6c9cfff9b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c9160c2485c99e853ae4e1e6c9cfff9b_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E有如下的 global optimality + finite iteration result：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4f33cd8bede995de9ae91496b0c4b699_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1398\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb\" width=\"1398\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4f33cd8bede995de9ae91496b0c4b699_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1398&#39; height=&#39;260&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1398\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1398\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4f33cd8bede995de9ae91496b0c4b699_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4f33cd8bede995de9ae91496b0c4b699_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这里会用到 Lemma4.1，梯度大小的衡量为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -stationary，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0543c95992954681a3248e90893f82dc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb\" width=\"612\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0543c95992954681a3248e90893f82dc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;612&#39; height=&#39;31&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"612\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0543c95992954681a3248e90893f82dc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0543c95992954681a3248e90893f82dc_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E换句话说，即 stationarity implies optimality。这其中包含的 distribution mismatch coefficient 是必须的，同样考虑一个 sparse reward 的环境，如果策略们都 exponentially hard 地得到非零奖励，那么这些策略附近也是 stationary 的，但是并没有 optimality。利用该思路，可以证明相应的 lower bound，即至少存在某种 MDP 使得 iteration 的数量大致上得有这么多。\u003C\u002Fp\u003E\u003Cp\u003E\u003Ci\u003E\u003Cu\u003E证明的过程需要看一下，特别是 projection operator 是如何分析和处理的。\u003C\u002Fu\u003E\u003C\u002Fi\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-94ebca8596386f98ba04c7e39f950e15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1392\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"1392\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-94ebca8596386f98ba04c7e39f950e15_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1392&#39; height=&#39;268&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1392\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1392\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-94ebca8596386f98ba04c7e39f950e15_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-94ebca8596386f98ba04c7e39f950e15_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E该引理给出了一个探索难的例子，说明如果无视探索的问题（即，distribution mismatch coefficient 很大），在梯度较小的情况下，仍然离最优策略较远。\u003C\u002Fp\u003E\u003Ch3\u003E5. Softmax parameterization without regularization\u003C\u002Fh3\u003E\u003Cp\u003E在 softmax parameterization 下，策略梯度可以写为：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e1c2c98645b90ea6392a274bd50c4a74_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1392\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"1392\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e1c2c98645b90ea6392a274bd50c4a74_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1392&#39; height=&#39;102&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1392\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1392\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e1c2c98645b90ea6392a274bd50c4a74_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e1c2c98645b90ea6392a274bd50c4a74_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E考虑一个正常的 gradient ascent 更新：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-50eb66758839f1768e2e2a744ee16103_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb\" width=\"1370\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-50eb66758839f1768e2e2a744ee16103_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1370&#39; height=&#39;80&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1370\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-50eb66758839f1768e2e2a744ee16103_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-50eb66758839f1768e2e2a744ee16103_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E文章给出了 optimality 的结果，没有 finite iteration 的结果，实际上该方法可能收敛地指数级地慢。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-527dbf1930c4ec6e671a3b31f5488698_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb\" width=\"1380\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-527dbf1930c4ec6e671a3b31f5488698_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1380&#39; height=&#39;188&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1380\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-527dbf1930c4ec6e671a3b31f5488698_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-527dbf1930c4ec6e671a3b31f5488698_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E该结果要求算法 rollout 的初始状态分布就要布满整个状态空间，但是原则上来讲，transition dynamics 应该也能把采样的状态带到各个需要的状态上，这个条件不是很必须。不过文章的证明还是用到了这个条件。\u003C\u002Fp\u003E\u003Cp\u003E注意到\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-908648f3dd2416be253669f5a94a2de5_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"1352\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-908648f3dd2416be253669f5a94a2de5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1352&#39; height=&#39;106&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1352\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-908648f3dd2416be253669f5a94a2de5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-908648f3dd2416be253669f5a94a2de5_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E即，和 direct policy parameterization 不一样的地方在于， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+V\" alt=\"\\nabla_\\theta V\" eeimg=\"1\"\u002F\u003E 较小不代表 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Cpi+V\" alt=\"\\nabla_\\pi V\" eeimg=\"1\"\u002F\u003E 很小。因此， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cnabla_%5Ctheta+V\" alt=\"\\nabla_\\theta V\" eeimg=\"1\"\u002F\u003E 较小的时候，更新就比较缓慢了，但实际上可能离最优策略还比较远。这就是该算法可能到最后收敛缓慢的原因。后面会讲到，这个问题可以通过增加 entropic regularization （代价是，根据正则项强度的不同，会产生大小不同的 bias）和使用 natural policy gradient 来解决。\u003C\u002Fp\u003E\u003Ch3\u003E6. Softmax parameterization with relative entropy regularization\u003C\u002Fh3\u003E\u003Cp\u003E首先，区分一下常用的两种熵正则：entropy \u002F relative entropy。\u003C\u002Fp\u003E\u003Cp\u003ERelative entropy 指相对于一个均匀分布的相对熵，即，目标函数变为：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-13ff7cf9a7e11d07b03bec8cada5c4cc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1378\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1378\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-13ff7cf9a7e11d07b03bec8cada5c4cc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1378&#39; height=&#39;204&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1378\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1378\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-13ff7cf9a7e11d07b03bec8cada5c4cc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-13ff7cf9a7e11d07b03bec8cada5c4cc_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E由于后面要对它求导，因此最后一个常数项可以被扔掉。\u003C\u002Fp\u003E\u003Cp\u003EEntropy 指的是策略自身的熵：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-071506c80bb3374099419b3dbebea8f5_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1360\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"1360\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-071506c80bb3374099419b3dbebea8f5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1360&#39; height=&#39;106&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1360\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1360\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-071506c80bb3374099419b3dbebea8f5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-071506c80bb3374099419b3dbebea8f5_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E区别在于 relative entropy 中蓝色框的部分为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1%2F%7C%5Cmathcal%7BA%7D%7C\" alt=\"1\u002F|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E 。可以看出，relative entropy 对于确定性策略（在其他行动上产生非常小的 probability）的惩罚更大。文章这里分析 relative entropy 正则项的情形。\u003C\u002Fp\u003E\u003Cp\u003E考虑如下更新策略：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-919b5c752c62540c6993bc7c683c58fd_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb\" width=\"1370\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-919b5c752c62540c6993bc7c683c58fd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1370&#39; height=&#39;62&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1370\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-919b5c752c62540c6993bc7c683c58fd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-919b5c752c62540c6993bc7c683c58fd_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E有如下 global optimality + finite iteration result：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a06c5318097a576e7b1d275a044c9dc2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1384\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb\" width=\"1384\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a06c5318097a576e7b1d275a044c9dc2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1384&#39; height=&#39;326&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1384\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1384\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a06c5318097a576e7b1d275a044c9dc2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a06c5318097a576e7b1d275a044c9dc2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E注意到一点，需要先知道所需要的精度 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E 再确定正则项的强度 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"\u002F\u003E ，并且如果要求的精度高，则需要少加一些正则，这样才能够保证 bias 足够小，相应的代价是需要更多的 iteration。\u003C\u002Fp\u003E\u003Ch3\u003E7. Softmax parameterization with natural policy gradient\u003C\u002Fh3\u003E\u003Cp\u003E考虑如下更新 natural policy gradient 的更新公式\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-7abc8472bf72f1637ebe8e286afd9632_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1398\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"1398\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-7abc8472bf72f1637ebe8e286afd9632_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1398&#39; height=&#39;200&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1398\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1398\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-7abc8472bf72f1637ebe8e286afd9632_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-7abc8472bf72f1637ebe8e286afd9632_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003Esoftmax parameterization + NPG updates 能够得到 closed-form 的 update rule，文章直接引用了一下结论：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-075a3e8f7723297745f7fe92b02fc9f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1386\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb\" width=\"1386\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-075a3e8f7723297745f7fe92b02fc9f2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1386&#39; height=&#39;246&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1386\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1386\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-075a3e8f7723297745f7fe92b02fc9f2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-075a3e8f7723297745f7fe92b02fc9f2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Ci\u003E\u003Cu\u003E这个是怎么来的日后还需要看一下，特别是 NPG update rule 应该怎么分析。\u003C\u002Fu\u003E\u003C\u002Fi\u003E\u003C\u002Fp\u003E\u003Cp\u003E以下引理证明了 policy improvement：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0fe33290b3ee0fbfd820005c6efeb395_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1336\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb\" width=\"1336\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0fe33290b3ee0fbfd820005c6efeb395_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1336&#39; height=&#39;199&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1336\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1336\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0fe33290b3ee0fbfd820005c6efeb395_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-0fe33290b3ee0fbfd820005c6efeb395_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其证明过程感觉还是很巧妙的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ef87466382bf2b792f9b9f1cb2b26c51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1373\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb\" width=\"1373\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ef87466382bf2b792f9b9f1cb2b26c51_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1373&#39; height=&#39;886&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1373\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1373\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ef87466382bf2b792f9b9f1cb2b26c51_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-ef87466382bf2b792f9b9f1cb2b26c51_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E以下定理说明了 softmax parameterization + NPG updates 的性质（global optimality + finite iteration）\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-74de7481104d7fe20275745217eabff7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb\" width=\"1388\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-74de7481104d7fe20275745217eabff7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1388&#39; height=&#39;434&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1388\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-74de7481104d7fe20275745217eabff7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-74de7481104d7fe20275745217eabff7_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E证明过程用到前面的 policy improvement lemma，考虑然后创造能够前后抵消的项。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1e7c428e906b709391480eaec85aa974_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb\" width=\"745\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1e7c428e906b709391480eaec85aa974_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;745&#39; height=&#39;752&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"745\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1e7c428e906b709391480eaec85aa974_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-1e7c428e906b709391480eaec85aa974_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中最后一个不等式应该略去了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_%5Cmax\" alt=\"R_\\max\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":40,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":4,"contributions":[{"id":21558319,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 89】PG Theory 1 - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F78919869 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false,"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"]}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F78919869","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F78919869","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning",null]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>