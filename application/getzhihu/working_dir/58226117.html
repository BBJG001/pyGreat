<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 44】IMPALA/V-trace - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="这篇文章主要讲了两个重要的技术：IMPALA全称是IMPortance weighted Actor-Learner Architecture，V-trace和之前本专栏讲的Retrace类似，只不过Retrace估计的是Q函数，这里估计的是V函数，因此叫V-trace。原文传送…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 44】IMPALA/V-trace"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/58226117"/><meta data-react-helmet="true" property="og:description" content="这篇文章主要讲了两个重要的技术：IMPALA全称是IMPortance weighted Actor-Learner Architecture，V-trace和之前本专栏讲的Retrace类似，只不过Retrace估计的是Q函数，这里估计的是V函数，因此叫V-trace。原文传送…"/><meta data-react-helmet="true" property="og:image" content="https://pic2.zhimg.com/v2-5dcf9d2d0bad74c3550c99db3523702f_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:58226117,&quot;title&quot;:&quot;【强化学习 44】IMPALA/V-trace&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic2.zhimg.com/v2-5dcf9d2d0bad74c3550c99db3523702f_1200x500.jpg" alt="【强化学习 44】IMPALA/V-trace"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 44】IMPALA/V-trace</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">26 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>这篇文章主要讲了两个重要的技术：IMPALA全称是IMPortance weighted Actor-Learner Architecture，V-trace和之前本专栏讲的Retrace类似，只不过Retrace估计的是Q函数，这里估计的是V函数，因此叫V-trace。</p><h2><b>原文传送门</b></h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1802.01561" class=" wrap external" target="_blank" rel="nofollow noreferrer">Espeholt, Lasse, et al. &#34;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.&#34; arXiv preprint arXiv:1802.01561 (2018).</a></p><h2><b>特色</b></h2><p>IMPLALA是一个大规模强化学习训练的框架，具有较高的性能（high throughput）、较好的扩展性（scalability）和较高的效率（data-efficiency）。在大规模计算的框架下，采样和策略更新会有一些错位（不再是完全的on-policy），在这种情况下，文章通过V-trace技术来完成地使用off-policy样本进行训练。</p><h2><b>过程</b></h2><p><b>1. IMPALA</b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1c78d19f0daa300486a1440c43bd79f3_b.jpg" data-caption="" data-size="normal" data-rawwidth="779" data-rawheight="565" class="origin_image zh-lightbox-thumb" width="779" data-original="https://pic4.zhimg.com/v2-1c78d19f0daa300486a1440c43bd79f3_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;779&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="779" data-rawheight="565" class="origin_image zh-lightbox-thumb lazy" width="779" data-original="https://pic4.zhimg.com/v2-1c78d19f0daa300486a1440c43bd79f3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1c78d19f0daa300486a1440c43bd79f3_b.jpg"/></figure><p>首先，我们来看一个single learner的模式。learner的主要作用是通过获取actor得到的轨迹来做SGD来更新各个神经网络的参数，神经网络训练本身可并行的特性，learner使用的是一块GPU。actor定期从learner获取最新的神经网络参数，并且每个actor起一个模拟环境，来使用自己能获得的最新策略去采样，并且把获取到的 <img src="https://www.zhihu.com/equation?tex=%5C%7Bx_t%2C+a_t%2C+r_t%2C+%5Cmu%28a_t%7Cx_t%29+%5C%7D" alt="\{x_t, a_t, r_t, \mu(a_t|x_t) \}" eeimg="1"/> 传回供learner去更新各个神经网络参数。由于模拟环境的运行通常不方便做并行，actor一般使用CPU。由于actor上的策略 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 可能不是learner中最新的策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> ，因此这里使用了不同的符号来表示。</p><p>下一步，当训练规模扩大的时候，可以考虑使用多个learner（多块GPU）并且每块GPU配套多个actor（CPU）。每个learner只从自己的actor们中获取样本进行更新，learner之间定期交换gradient并且更新网络参数，actor也定期从任意learner上获取并更新神经网络参数。（这里有点没搞懂，为啥actor会去从别的learner那里拿神经网络参数？参考了[1]还是不明白）</p><p>IMPALA中actor和learner相互异步工作，极大提高了时间利用率。文章还与与batched A2C做了对比，如下图所示。a图中，正向传播和反向传播都想凑一批来做（可能是给到GPU来算），因此每一步都需要同步，而模拟环境各步所需时间方差很大，这样浪费了大量的等待时间；b图中，只把耗时较长的反向传播凑一批来做，正向传播就给各个actor自己做；而c图中的IMPALA则完全把actor和learner分开异步进行，这样actor不用去等待别的actor，可以尽可能多的做采样，相应地，所作出的牺牲就是每次更新得到的样本变为了off-policy样本。接下来本文提出了V-trace对于off-policy样本做修正。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-6c9c6422600816b8e705ce04b1dff815_b.jpg" data-caption="" data-size="normal" data-rawwidth="1078" data-rawheight="672" class="origin_image zh-lightbox-thumb" width="1078" data-original="https://pic2.zhimg.com/v2-6c9c6422600816b8e705ce04b1dff815_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1078&#39; height=&#39;672&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1078" data-rawheight="672" class="origin_image zh-lightbox-thumb lazy" width="1078" data-original="https://pic2.zhimg.com/v2-6c9c6422600816b8e705ce04b1dff815_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-6c9c6422600816b8e705ce04b1dff815_b.jpg"/></figure><p><b>2. V-trace</b></p><p>算法中需要根据采样到的样本来维护一个状态价值函数 <img src="https://www.zhihu.com/equation?tex=V_%5Ctheta%28x%29" alt="V_\theta(x)" eeimg="1"/> 。V-trace的目的是根据采样到的 <img src="https://www.zhihu.com/equation?tex=%5C%7Bx_t%2C+a_t%2C+r_t%2C+%5Cmu%28a_t%7Cx_t%29+%5C%7D" alt="\{x_t, a_t, r_t, \mu(a_t|x_t) \}" eeimg="1"/> 和当前状态价值函数网络来给出当前状态价值函数的一个更好的估计 <img src="https://www.zhihu.com/equation?tex=v_s" alt="v_s" eeimg="1"/>（ <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 下标表示它是其中的一个样本），这样价值神经网络就可以把它作为一个更新的目标来更新权重。</p><p>我们直接写出 <img src="https://www.zhihu.com/equation?tex=v_s" alt="v_s" eeimg="1"/> 的表达形式。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-e9672e295f91103a83bbb81736dd052e_b.jpg" data-caption="" data-size="normal" data-rawwidth="498" data-rawheight="218" class="origin_image zh-lightbox-thumb" width="498" data-original="https://pic3.zhimg.com/v2-e9672e295f91103a83bbb81736dd052e_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;498&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="498" data-rawheight="218" class="origin_image zh-lightbox-thumb lazy" width="498" data-original="https://pic3.zhimg.com/v2-e9672e295f91103a83bbb81736dd052e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-e9672e295f91103a83bbb81736dd052e_b.jpg"/></figure><p>它具有如下性质：</p><ul><li>状态价值函数 <img src="https://www.zhihu.com/equation?tex=V_%5Ctheta%28x%29" alt="V_\theta(x)" eeimg="1"/> 每次往 <img src="https://www.zhihu.com/equation?tex=v_s" alt="v_s" eeimg="1"/> 上更新，最后能够收敛；最后面我们会证明如果有V-trace算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BR%7D%3A+V%28x_s%29+%5Cto+v_s" alt="\mathcal{R}: V(x_s) \to v_s" eeimg="1"/> ，那么该算子是contraction。</li><li>状态价值函数 <img src="https://www.zhihu.com/equation?tex=V_%5Ctheta%28x%29" alt="V_\theta(x)" eeimg="1"/> 每次往 <img src="https://www.zhihu.com/equation?tex=v_s" alt="v_s" eeimg="1"/> 上更新，收敛到的状态价值函数是介于 <img src="https://www.zhihu.com/equation?tex=V%5E%5Cpi" alt="V^\pi" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=V%5E%5Cmu" alt="V^\mu" eeimg="1"/> 之间的某个价值函数，我们记该价值函数为 <img src="https://www.zhihu.com/equation?tex=V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D" alt="V^{\pi_{\bar{\rho}}}" eeimg="1"/> ，该价值函数对应的策略如下所示；最后面我们通过计算V-trace算子的不动点可以得到这个结论。</li></ul><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-2c810f5e8453613430bf9365b9de96dc_b.png" data-caption="" data-size="normal" data-rawwidth="980" data-rawheight="138" class="origin_image zh-lightbox-thumb" width="980" data-original="https://pic1.zhimg.com/v2-2c810f5e8453613430bf9365b9de96dc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;980&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="980" data-rawheight="138" class="origin_image zh-lightbox-thumb lazy" width="980" data-original="https://pic1.zhimg.com/v2-2c810f5e8453613430bf9365b9de96dc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-2c810f5e8453613430bf9365b9de96dc_b.png"/></figure><ul><li>为了避免importance weight发散，我们需要加上相应的上界来避免；参数 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Crho%7D" alt="\bar{\rho}" eeimg="1"/> 决定了收敛到的不动点位置； <img src="https://www.zhihu.com/equation?tex=%5Cbar%7Bc%7D" alt="\bar{c}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Crho%7D" alt="\bar{\rho}" eeimg="1"/> 决定了收敛的速率。</li><li>在on-policy的情况下，如果 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Crho%7D+%5Cge+%5Cbar%7Bc%7D+%5Cge+1" alt="\bar{\rho} \ge \bar{c} \ge 1" eeimg="1"/> ，那么 <img src="https://www.zhihu.com/equation?tex=v_s" alt="v_s" eeimg="1"/> 就退化为on-policy n-steps Bellman target。</li></ul><p><b>3. Actor-critic </b></p><p>IMPALA中需要维护两个神经网络，一个是策略神经网络（用作actor），一个是状态价值函数网络（用作critic）。前面提到，V-trace技术就是根据采样到的 <img src="https://www.zhihu.com/equation?tex=%5C%7Bx_t%2C+a_t%2C+r_t%2C+%5Cmu%28a_t%7Cx_t%29+%5C%7D" alt="\{x_t, a_t, r_t, \mu(a_t|x_t) \}" eeimg="1"/> 和当前状态价值函数网络来给出当前状态价值函数的一个更好的估计 <img src="https://www.zhihu.com/equation?tex=v_s" alt="v_s" eeimg="1"/> 。</p><p><b>3.1 Critic的更新</b></p><p>Critic的更新方式为最小化拟合的价值函数 <img src="https://www.zhihu.com/equation?tex=V_%5Ctheta+%28x_s%29" alt="V_\theta (x_s)" eeimg="1"/>  相对于目标价值函数 <img src="https://www.zhihu.com/equation?tex=v_s" alt="v_s" eeimg="1"/> 的均方误差，即朝如下方向更新</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-3bc8e4b51be7ee5788779e06d8f8e58f_b.png" data-caption="" data-size="normal" data-rawwidth="988" data-rawheight="84" class="origin_image zh-lightbox-thumb" width="988" data-original="https://pic4.zhimg.com/v2-3bc8e4b51be7ee5788779e06d8f8e58f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;988&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="988" data-rawheight="84" class="origin_image zh-lightbox-thumb lazy" width="988" data-original="https://pic4.zhimg.com/v2-3bc8e4b51be7ee5788779e06d8f8e58f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-3bc8e4b51be7ee5788779e06d8f8e58f_b.png"/></figure><p><b>3.2 Actor的更新</b></p><p>Actor朝着off-policy policy gradient给出的梯度方向更新，即 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%5Cmu%5B%5Cnabla+%5Clog+%5Cmu%28a_s%7Cx_s%29+Q%5E%5Cmu%28x_s%2C+a_s%29%5D" alt="\mathbb{E}_\mu[\nabla \log \mu(a_s|x_s) Q^\mu(x_s, a_s)]" eeimg="1"/> 。我们更新的目标是策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> ，而不是策略 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> ，因此要做代换 <img src="https://www.zhihu.com/equation?tex=%5Cmu%5Cto+%28%5Cfrac%7B%5Cmu%7D%7B%5Cpi%7D%29+%5Cpi" alt="\mu\to (\frac{\mu}{\pi}) \pi" eeimg="1"/> ，把括号中的当做系数，后面的 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 才是变量，即</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%5Cmu%5B%5Cnabla+%5Clog+%5Cmu%28a_s%7Cx_s%29+Q%5E%5Cmu%28x_s%2C+a_s%29%5D+%3D+%5Cmathbb%7BE%7D_%5Cmu%5B%5Cdfrac%7B1%7D%7B%5Cdfrac%7B%5Cmu%7D%7B%5Cpi%7D+%5Cpi%7D+%5Cnabla+%5Cpi+%5C%2C+Q%5E%5Cpi%28x_s%2C+a_s%29%5D+%3D+%5Cmathbb%7BE%7D_%5Cmu%5B%5Cdfrac%7B%5Cpi%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D+%5Cnabla+%5Clog+%5Cpi+Q%5E%5Cpi%28x_s%2C+a_s%29%5D" alt="\mathbb{E}_\mu[\nabla \log \mu(a_s|x_s) Q^\mu(x_s, a_s)] = \mathbb{E}_\mu[\dfrac{1}{\dfrac{\mu}{\pi} \pi} \nabla \pi \, Q^\pi(x_s, a_s)] = \mathbb{E}_\mu[\dfrac{\pi(a_s|x_s)}{\mu(a_s|x_s)} \nabla \log \pi Q^\pi(x_s, a_s)]" eeimg="1"/> </p><p>接下来，</p><ul><li><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpi%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D" alt="\dfrac{\pi(a_s|x_s)}{\mu(a_s|x_s)}" eeimg="1"/> 容易算发散，我们把它换成 <img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D+%5Cpropto+%5Cmin%28%5Cbar%7B%5Crho%7D%2C+%5Cdfrac%7B%5Cpi%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D%29+%3D+%5Crho_s" alt="\dfrac{\pi_{\bar{\rho}}(a_s|x_s)}{\mu(a_s|x_s)} \propto \min(\bar{\rho}, \dfrac{\pi(a_s|x_s)}{\mu(a_s|x_s)}) = \rho_s" eeimg="1"/> 。</li><li><img src="https://www.zhihu.com/equation?tex=Q%5E%5Cpi" alt="Q^\pi" eeimg="1"/> 没法估计到，我们只能估计到 <img src="https://www.zhihu.com/equation?tex=Q%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D" alt="Q^{\pi_{\bar{\rho}}}" eeimg="1"/> ，即使用 <img src="https://www.zhihu.com/equation?tex=r_s+%2B+%5Cgamma+v_%7Bs%2B1%7D+" alt="r_s + \gamma v_{s+1} " eeimg="1"/> 作为估计。</li><li>使用baseline来减小误差，即减去 <img src="https://www.zhihu.com/equation?tex=V_%5Ctheta%28x_s%29" alt="V_\theta(x_s)" eeimg="1"/> 。</li></ul><p>最后，actor的更新方向可以写为</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-6727d548447f3b1e9057702998e34fbe_b.png" data-caption="" data-size="normal" data-rawwidth="978" data-rawheight="94" class="origin_image zh-lightbox-thumb" width="978" data-original="https://pic3.zhimg.com/v2-6727d548447f3b1e9057702998e34fbe_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;978&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="978" data-rawheight="94" class="origin_image zh-lightbox-thumb lazy" width="978" data-original="https://pic3.zhimg.com/v2-6727d548447f3b1e9057702998e34fbe_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-6727d548447f3b1e9057702998e34fbe_b.png"/></figure><p><b>3.3 熵</b></p><p>前两项再加上对于熵的激励，就可以得到最后的算法更新公式。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-a97dd86fc030030d1d675974342e4b26_b.png" data-caption="" data-size="normal" data-rawwidth="984" data-rawheight="114" class="origin_image zh-lightbox-thumb" width="984" data-original="https://pic3.zhimg.com/v2-a97dd86fc030030d1d675974342e4b26_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;984&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="984" data-rawheight="114" class="origin_image zh-lightbox-thumb lazy" width="984" data-original="https://pic3.zhimg.com/v2-a97dd86fc030030d1d675974342e4b26_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-a97dd86fc030030d1d675974342e4b26_b.png"/></figure><h2><b>实验结果</b></h2><p>实验主要说明了以下几点：</p><ul><li>相比于A3C和batched A2C，具有更好的高性能计算性能；</li><li>单任务训练上相比于分布式A3C、单机A3C和batched A2C有更好的性能，并且对于超参数更稳定；</li><li>本文中使用的V-trace相比于no correction、 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -correction、1-step importance sampling都有更好的效果（ablation study）。其中no correction指的是认为样本都是on-policy样本； <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -correction指的是仅仅在计算 <img src="https://www.zhihu.com/equation?tex=%5Clog+%5Cpi" alt="\log \pi" eeimg="1"/> 的时候加上一个很小的数值防止不稳定（不太懂）；1-step importance sampling，V-trace其实是做了多步，这里只做一步。</li><li>训练单一智能体去完成多个任务。</li></ul><h2><b>其他技术</b></h2><p>在这种大规模训练中，训练一次耗资巨大，为了避免训练的这一波陷入局部极小值点，采用了 <b><a href="https://link.zhihu.com/?target=https%3A//deepmind.com/blog/population-based-training-neural-networks/" class=" wrap external" target="_blank" rel="nofollow noreferrer">population based training（PBT）</a> </b>方法。每次训练若干个智能体，每隔一段时间剔除表现不好的，并且对于表现较好的智能体进行mutation（通常是扰动一下超参数组合）。通过这种方法，保证长达几天的训练结束后能得到好的结果。</p><p>有意思是，通过这种方法，学习率会随着学习进度自然慢慢减小，这和很多算法里面linear scheduled learning rate的trick不谋而合。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-b4b947e972aedb456a710248031b7884_b.jpg" data-caption="" data-size="normal" data-rawwidth="2084" data-rawheight="870" class="origin_image zh-lightbox-thumb" width="2084" data-original="https://pic1.zhimg.com/v2-b4b947e972aedb456a710248031b7884_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2084&#39; height=&#39;870&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2084" data-rawheight="870" class="origin_image zh-lightbox-thumb lazy" width="2084" data-original="https://pic1.zhimg.com/v2-b4b947e972aedb456a710248031b7884_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-b4b947e972aedb456a710248031b7884_b.jpg"/></figure><hr/><h2><b>V-trace定理证明</b></h2><p>首先定义V-trace的算子</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-2b8ba3a8d4c2e9aeda638a1d190cc15d_b.png" data-caption="" data-size="normal" data-rawwidth="2018" data-rawheight="126" class="origin_image zh-lightbox-thumb" width="2018" data-original="https://pic2.zhimg.com/v2-2b8ba3a8d4c2e9aeda638a1d190cc15d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2018&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2018" data-rawheight="126" class="origin_image zh-lightbox-thumb lazy" width="2018" data-original="https://pic2.zhimg.com/v2-2b8ba3a8d4c2e9aeda638a1d190cc15d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-2b8ba3a8d4c2e9aeda638a1d190cc15d_b.png"/></figure><p>不考虑神经网络 <img src="https://www.zhihu.com/equation?tex=V_%5Ctheta%28x_s%29" alt="V_\theta(x_s)" eeimg="1"/> 的拟合误差，可以认为每轮更新中价值函数都在做 <img src="https://www.zhihu.com/equation?tex=V%28x%29+%5Cto+%5Cmathcal%7BR%7D+V%28x%29%2C+%5C+%5Cforall+x" alt="V(x) \to \mathcal{R} V(x), \ \forall x" eeimg="1"/> 。</p><p><b>定理1</b></p><p>关于V-trace算子不动点和收敛速度的定理可以表述为</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-ce4068b6368befdf894571d17720d6b0_b.jpg" data-caption="" data-size="normal" data-rawwidth="2078" data-rawheight="532" class="origin_image zh-lightbox-thumb" width="2078" data-original="https://pic1.zhimg.com/v2-ce4068b6368befdf894571d17720d6b0_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2078&#39; height=&#39;532&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2078" data-rawheight="532" class="origin_image zh-lightbox-thumb lazy" width="2078" data-original="https://pic1.zhimg.com/v2-ce4068b6368befdf894571d17720d6b0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-ce4068b6368befdf894571d17720d6b0_b.jpg"/></figure><p><b>证明</b></p><p>首先把算子重写，前面一个求和里面有前后两个状态的 <img src="https://www.zhihu.com/equation?tex=V%28x_t%29%2C+V%28x_%7Bt%2B1%7D%29" alt="V(x_t), V(x_{t+1})" eeimg="1"/> ，这里把它们重排一下，</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-158c0f96291d872b85ce502d9cc9c114_b.png" data-caption="" data-size="normal" data-rawwidth="1990" data-rawheight="168" class="origin_image zh-lightbox-thumb" width="1990" data-original="https://pic1.zhimg.com/v2-158c0f96291d872b85ce502d9cc9c114_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1990&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1990" data-rawheight="168" class="origin_image zh-lightbox-thumb lazy" width="1990" data-original="https://pic1.zhimg.com/v2-158c0f96291d872b85ce502d9cc9c114_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-158c0f96291d872b85ce502d9cc9c114_b.png"/></figure><p>证明不动点和收敛速率最关键的是证明contraction，我们复习一下contraction的形式</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-24c0204745b25b2818f26830159de249_b.png" data-caption="" data-size="normal" data-rawwidth="1460" data-rawheight="100" class="origin_image zh-lightbox-thumb" width="1460" data-original="https://pic2.zhimg.com/v2-24c0204745b25b2818f26830159de249_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1460&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1460" data-rawheight="100" class="origin_image zh-lightbox-thumb lazy" width="1460" data-original="https://pic2.zhimg.com/v2-24c0204745b25b2818f26830159de249_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-24c0204745b25b2818f26830159de249_b.png"/></figure><p>即每作用一次该算子，任意两个状态价值函数 <img src="https://www.zhihu.com/equation?tex=V_1" alt="V_1" eeimg="1"/> 离 <img src="https://www.zhihu.com/equation?tex=V_2" alt="V_2" eeimg="1"/> 的距离都会减小，这样作用无穷多次之后，不管最开始状态价值函数是什么，都会收敛到同一个状态价值函数上。因此，我们自然需要把上式左手边写出来看看。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_b.jpg" data-caption="" data-size="normal" data-rawwidth="2010" data-rawheight="376" class="origin_image zh-lightbox-thumb" width="2010" data-original="https://pic1.zhimg.com/v2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2010&#39; height=&#39;376&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2010" data-rawheight="376" class="origin_image zh-lightbox-thumb lazy" width="2010" data-original="https://pic1.zhimg.com/v2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_b.jpg"/></figure><p>化简的要点就是凑出右手边 <img src="https://www.zhihu.com/equation?tex=V_1+-+V_2" alt="V_1 - V_2" eeimg="1"/> 这种形式。定义系数 <img src="https://www.zhihu.com/equation?tex=%5Calpha_t" alt="\alpha_t" eeimg="1"/> ，并且考虑到 <img src="https://www.zhihu.com/equation?tex=%5Crho_t" alt="\rho_t" eeimg="1"/> 的定义，有</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-bf8794be2477ee55b7a6cb6c63fd0f4b_b.png" data-caption="" data-size="normal" data-rawwidth="1988" data-rawheight="102" class="origin_image zh-lightbox-thumb" width="1988" data-original="https://pic4.zhimg.com/v2-bf8794be2477ee55b7a6cb6c63fd0f4b_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1988&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1988" data-rawheight="102" class="origin_image zh-lightbox-thumb lazy" width="1988" data-original="https://pic4.zhimg.com/v2-bf8794be2477ee55b7a6cb6c63fd0f4b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-bf8794be2477ee55b7a6cb6c63fd0f4b_b.png"/></figure><p>可以看出 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BR%7D+V_1%28x%29+-+%5Cmathcal%7BR%7D+V_2%28x%29" alt="\mathcal{R} V_1(x) - \mathcal{R} V_2(x)" eeimg="1"/> 被凑成了其他 <img src="https://www.zhihu.com/equation?tex=V_1%28x_t%29+-+V_2%28x_t%29" alt="V_1(x_t) - V_2(x_t)" eeimg="1"/> 的线性组合，如果我们能够证明组合中的各项系数和小于1，那么就容易证明 <img src="https://www.zhihu.com/equation?tex=%7C%7C+%5Cmathcal%7BR%7D%5En+V_1%28x%29+-+%5Cmathcal%7BR%7D%5En+V_2%28x%29+%7C%7C_%5Cinfty+%5Cto+0" alt="|| \mathcal{R}^n V_1(x) - \mathcal{R}^n V_2(x) ||_\infty \to 0" eeimg="1"/> 。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-b6f79abceb6f4309e4ce247bddbab687_b.jpg" data-caption="" data-size="normal" data-rawwidth="2008" data-rawheight="864" class="origin_image zh-lightbox-thumb" width="2008" data-original="https://pic4.zhimg.com/v2-b6f79abceb6f4309e4ce247bddbab687_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2008&#39; height=&#39;864&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2008" data-rawheight="864" class="origin_image zh-lightbox-thumb lazy" width="2008" data-original="https://pic4.zhimg.com/v2-b6f79abceb6f4309e4ce247bddbab687_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b6f79abceb6f4309e4ce247bddbab687_b.jpg"/></figure><p>其中倒数第三个不等式是利用了求和符号内各项的非负性，只保留了 <img src="https://www.zhihu.com/equation?tex=t%3D0" alt="t=0" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=t%3D1" alt="t=1" eeimg="1"/> 项，扔掉了其他项。其中倒数第三项就是该contraction的收敛速率 <img src="https://www.zhihu.com/equation?tex=%5Ceta" alt="\eta" eeimg="1"/> ，可以看出，该速率与 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7B%5Crho%7D" alt="\bar{\rho}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cbar%7Bc%7D" alt="\bar{c}" eeimg="1"/> 有关。contraction的存在，也说明了唯一不动点的存在。</p><p>下面验证 <img src="https://www.zhihu.com/equation?tex=V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D" alt="V^{\pi_{\bar{\rho}}}" eeimg="1"/> 为不动点，这只需要说明 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BR%7D+V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D+-+V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D+%3D+0" alt="\mathcal{R} V^{\pi_{\bar{\rho}}} - V^{\pi_{\bar{\rho}}} = 0" eeimg="1"/> 即可。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8f19ff183357c5e8683e97e6e75541b4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1998" data-rawheight="464" class="origin_image zh-lightbox-thumb" width="1998" data-original="https://pic1.zhimg.com/v2-8f19ff183357c5e8683e97e6e75541b4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1998&#39; height=&#39;464&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1998" data-rawheight="464" class="origin_image zh-lightbox-thumb lazy" width="1998" data-original="https://pic1.zhimg.com/v2-8f19ff183357c5e8683e97e6e75541b4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8f19ff183357c5e8683e97e6e75541b4_b.jpg"/></figure><p>其中第一等式是按照 <img src="https://www.zhihu.com/equation?tex=%5Crho_t" alt="\rho_t" eeimg="1"/> 的定义写出，第二个等式可以由前面 <img src="https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D" alt="\pi_{\bar{\rho}}" eeimg="1"/> 的定义得到，最后一个等式是因为 <img src="https://www.zhihu.com/equation?tex=V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D%28x_t%29++%3D+%5Csum_a+%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%28a%7Cx_t%29+%5Br%28x_t%2C+a%29+%2B+%5Cgamma+%5Csum_%7Bx_%7Bt%2B1%7D%7D+p%28x_%7Bt%2B1%7D%7Cx_t%2C+a%29V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D%28x_%7Bt%2B1%7D%29+%5D" alt="V^{\pi_{\bar{\rho}}}(x_t)  = \sum_a \pi_{\bar{\rho}}(a|x_t) [r(x_t, a) + \gamma \sum_{x_{t+1}} p(x_{t+1}|x_t, a)V^{\pi_{\bar{\rho}}}(x_{t+1}) ]" eeimg="1"/> 。</p><p class="ztext-empty-paragraph"><br/></p><p><b>定理2</b></p><p>定理1说明了如果在每轮，每个状态都访问并且更新一遍，那么能收敛到一个确定的不动点。但实际中，每个状态并不能在每一轮中都是均匀和遍历地访问的，而是走一个轨迹，走到哪个状态就更新哪个状态。这种情况下（online learning），是否还能收敛呢？下面这个定理说明，它也能收敛。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-41832ef8799b820f1530fb1a3c544c4c_b.jpg" data-caption="" data-size="normal" data-rawwidth="2042" data-rawheight="626" class="origin_image zh-lightbox-thumb" width="2042" data-original="https://pic1.zhimg.com/v2-41832ef8799b820f1530fb1a3c544c4c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2042&#39; height=&#39;626&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2042" data-rawheight="626" class="origin_image zh-lightbox-thumb lazy" width="2042" data-original="https://pic1.zhimg.com/v2-41832ef8799b820f1530fb1a3c544c4c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-41832ef8799b820f1530fb1a3c544c4c_b.jpg"/></figure><p class="ztext-empty-paragraph"><br/></p><hr/><h2><b>参考文献</b></h2><p>[1] Chen, Jianmin, et al. &#34;Revisiting distributed synchronous SGD.&#34;<i>arXiv preprint arXiv:1604.00981</i>(2016).</p></div></div><div class="ContentItem-time">编辑于 2019-03-06</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 26 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 26</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="3d8b5cf8-72d9-4c98-a112-0b2b1624e906" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="3d8b5cf8-72d9-4c98-a112-0b2b1624e906">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"58226117":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":58226117,"title":"【强化学习 44】IMPALA\u002FV-trace","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F58226117","imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5dcf9d2d0bad74c3550c99db3523702f_b.jpg","titleImage":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5dcf9d2d0bad74c3550c99db3523702f_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-e0ce3e96553049ae921ba6649fde9f39_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"779\" data-rawheight=\"565\" data-watermark=\"watermark\" data-original-src=\"v2-e0ce3e96553049ae921ba6649fde9f39\" data-watermark-src=\"v2-1c78d19f0daa300486a1440c43bd79f3\" data-private-watermark-src=\"\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-e0ce3e96553049ae921ba6649fde9f39_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"\u002F\u003E这篇文章主要讲了两个重要的技术：IMPALA全称是IMPortance weighted Actor-Learner Architecture，V-trace和之前本专栏讲的Retrace类似，只不过Retrace估计的是Q函数，这里估计的是V函数，因此叫V-trace。\u003Cb\u003E原文传送门\u003C\u002Fb\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1802.01561\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EEspeholt, Lasse, et al. &#34;Impala: Scala…\u003C\u002Fa\u003E","created":1551695890,"updated":1551834091,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1604,"imageHeight":518,"content":"\u003Cp\u003E这篇文章主要讲了两个重要的技术：IMPALA全称是IMPortance weighted Actor-Learner Architecture，V-trace和之前本专栏讲的Retrace类似，只不过Retrace估计的是Q函数，这里估计的是V函数，因此叫V-trace。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E原文传送门\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1802.01561\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EEspeholt, Lasse, et al. &#34;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.&#34; arXiv preprint arXiv:1802.01561 (2018).\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E特色\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003EIMPLALA是一个大规模强化学习训练的框架，具有较高的性能（high throughput）、较好的扩展性（scalability）和较高的效率（data-efficiency）。在大规模计算的框架下，采样和策略更新会有一些错位（不再是完全的on-policy），在这种情况下，文章通过V-trace技术来完成地使用off-policy样本进行训练。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E过程\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Cb\u003E1. IMPALA\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1c78d19f0daa300486a1440c43bd79f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"779\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"779\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1c78d19f0daa300486a1440c43bd79f3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;779&#39; height=&#39;565&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"779\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"779\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1c78d19f0daa300486a1440c43bd79f3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1c78d19f0daa300486a1440c43bd79f3_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E首先，我们来看一个single learner的模式。learner的主要作用是通过获取actor得到的轨迹来做SGD来更新各个神经网络的参数，神经网络训练本身可并行的特性，learner使用的是一块GPU。actor定期从learner获取最新的神经网络参数，并且每个actor起一个模拟环境，来使用自己能获得的最新策略去采样，并且把获取到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bx_t%2C+a_t%2C+r_t%2C+%5Cmu%28a_t%7Cx_t%29+%5C%7D\" alt=\"\\{x_t, a_t, r_t, \\mu(a_t|x_t) \\}\" eeimg=\"1\"\u002F\u003E 传回供learner去更新各个神经网络参数。由于模拟环境的运行通常不方便做并行，actor一般使用CPU。由于actor上的策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 可能不是learner中最新的策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E ，因此这里使用了不同的符号来表示。\u003C\u002Fp\u003E\u003Cp\u003E下一步，当训练规模扩大的时候，可以考虑使用多个learner（多块GPU）并且每块GPU配套多个actor（CPU）。每个learner只从自己的actor们中获取样本进行更新，learner之间定期交换gradient并且更新网络参数，actor也定期从任意learner上获取并更新神经网络参数。（这里有点没搞懂，为啥actor会去从别的learner那里拿神经网络参数？参考了[1]还是不明白）\u003C\u002Fp\u003E\u003Cp\u003EIMPALA中actor和learner相互异步工作，极大提高了时间利用率。文章还与与batched A2C做了对比，如下图所示。a图中，正向传播和反向传播都想凑一批来做（可能是给到GPU来算），因此每一步都需要同步，而模拟环境各步所需时间方差很大，这样浪费了大量的等待时间；b图中，只把耗时较长的反向传播凑一批来做，正向传播就给各个actor自己做；而c图中的IMPALA则完全把actor和learner分开异步进行，这样actor不用去等待别的actor，可以尽可能多的做采样，相应地，所作出的牺牲就是每次更新得到的样本变为了off-policy样本。接下来本文提出了V-trace对于off-policy样本做修正。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6c9c6422600816b8e705ce04b1dff815_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1078\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb\" width=\"1078\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6c9c6422600816b8e705ce04b1dff815_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1078&#39; height=&#39;672&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1078\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1078\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6c9c6422600816b8e705ce04b1dff815_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6c9c6422600816b8e705ce04b1dff815_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E2. V-trace\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E算法中需要根据采样到的样本来维护一个状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Ctheta%28x%29\" alt=\"V_\\theta(x)\" eeimg=\"1\"\u002F\u003E 。V-trace的目的是根据采样到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bx_t%2C+a_t%2C+r_t%2C+%5Cmu%28a_t%7Cx_t%29+%5C%7D\" alt=\"\\{x_t, a_t, r_t, \\mu(a_t|x_t) \\}\" eeimg=\"1\"\u002F\u003E 和当前状态价值函数网络来给出当前状态价值函数的一个更好的估计 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v_s\" alt=\"v_s\" eeimg=\"1\"\u002F\u003E（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下标表示它是其中的一个样本），这样价值神经网络就可以把它作为一个更新的目标来更新权重。\u003C\u002Fp\u003E\u003Cp\u003E我们直接写出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v_s\" alt=\"v_s\" eeimg=\"1\"\u002F\u003E 的表达形式。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9672e295f91103a83bbb81736dd052e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb\" width=\"498\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9672e295f91103a83bbb81736dd052e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;498&#39; height=&#39;218&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"498\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9672e295f91103a83bbb81736dd052e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-e9672e295f91103a83bbb81736dd052e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E它具有如下性质：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Ctheta%28x%29\" alt=\"V_\\theta(x)\" eeimg=\"1\"\u002F\u003E 每次往 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v_s\" alt=\"v_s\" eeimg=\"1\"\u002F\u003E 上更新，最后能够收敛；最后面我们会证明如果有V-trace算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BR%7D%3A+V%28x_s%29+%5Cto+v_s\" alt=\"\\mathcal{R}: V(x_s) \\to v_s\" eeimg=\"1\"\u002F\u003E ，那么该算子是contraction。\u003C\u002Fli\u003E\u003Cli\u003E状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Ctheta%28x%29\" alt=\"V_\\theta(x)\" eeimg=\"1\"\u002F\u003E 每次往 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v_s\" alt=\"v_s\" eeimg=\"1\"\u002F\u003E 上更新，收敛到的状态价值函数是介于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cpi\" alt=\"V^\\pi\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%5Cmu\" alt=\"V^\\mu\" eeimg=\"1\"\u002F\u003E 之间的某个价值函数，我们记该价值函数为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D\" alt=\"V^{\\pi_{\\bar{\\rho}}}\" eeimg=\"1\"\u002F\u003E ，该价值函数对应的策略如下所示；最后面我们通过计算V-trace算子的不动点可以得到这个结论。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c810f5e8453613430bf9365b9de96dc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"980\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb\" width=\"980\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c810f5e8453613430bf9365b9de96dc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;980&#39; height=&#39;138&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"980\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"980\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c810f5e8453613430bf9365b9de96dc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c810f5e8453613430bf9365b9de96dc_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli\u003E为了避免importance weight发散，我们需要加上相应的上界来避免；参数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Crho%7D\" alt=\"\\bar{\\rho}\" eeimg=\"1\"\u002F\u003E 决定了收敛到的不动点位置； \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7Bc%7D\" alt=\"\\bar{c}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Crho%7D\" alt=\"\\bar{\\rho}\" eeimg=\"1\"\u002F\u003E 决定了收敛的速率。\u003C\u002Fli\u003E\u003Cli\u003E在on-policy的情况下，如果 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Crho%7D+%5Cge+%5Cbar%7Bc%7D+%5Cge+1\" alt=\"\\bar{\\rho} \\ge \\bar{c} \\ge 1\" eeimg=\"1\"\u002F\u003E ，那么 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v_s\" alt=\"v_s\" eeimg=\"1\"\u002F\u003E 就退化为on-policy n-steps Bellman target。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cb\u003E3. Actor-critic \u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EIMPALA中需要维护两个神经网络，一个是策略神经网络（用作actor），一个是状态价值函数网络（用作critic）。前面提到，V-trace技术就是根据采样到的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7Bx_t%2C+a_t%2C+r_t%2C+%5Cmu%28a_t%7Cx_t%29+%5C%7D\" alt=\"\\{x_t, a_t, r_t, \\mu(a_t|x_t) \\}\" eeimg=\"1\"\u002F\u003E 和当前状态价值函数网络来给出当前状态价值函数的一个更好的估计 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v_s\" alt=\"v_s\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E3.1 Critic的更新\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003ECritic的更新方式为最小化拟合的价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Ctheta+%28x_s%29\" alt=\"V_\\theta (x_s)\" eeimg=\"1\"\u002F\u003E  相对于目标价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=v_s\" alt=\"v_s\" eeimg=\"1\"\u002F\u003E 的均方误差，即朝如下方向更新\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3bc8e4b51be7ee5788779e06d8f8e58f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"988\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"988\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3bc8e4b51be7ee5788779e06d8f8e58f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;988&#39; height=&#39;84&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"988\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"988\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3bc8e4b51be7ee5788779e06d8f8e58f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3bc8e4b51be7ee5788779e06d8f8e58f_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E3.2 Actor的更新\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EActor朝着off-policy policy gradient给出的梯度方向更新，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BE%7D_%5Cmu%5B%5Cnabla+%5Clog+%5Cmu%28a_s%7Cx_s%29+Q%5E%5Cmu%28x_s%2C+a_s%29%5D\" alt=\"\\mathbb{E}_\\mu[\\nabla \\log \\mu(a_s|x_s) Q^\\mu(x_s, a_s)]\" eeimg=\"1\"\u002F\u003E 。我们更新的目标是策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E ，而不是策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E ，因此要做代换 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu%5Cto+%28%5Cfrac%7B%5Cmu%7D%7B%5Cpi%7D%29+%5Cpi\" alt=\"\\mu\\to (\\frac{\\mu}{\\pi}) \\pi\" eeimg=\"1\"\u002F\u003E ，把括号中的当做系数，后面的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 才是变量，即\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BE%7D_%5Cmu%5B%5Cnabla+%5Clog+%5Cmu%28a_s%7Cx_s%29+Q%5E%5Cmu%28x_s%2C+a_s%29%5D+%3D+%5Cmathbb%7BE%7D_%5Cmu%5B%5Cdfrac%7B1%7D%7B%5Cdfrac%7B%5Cmu%7D%7B%5Cpi%7D+%5Cpi%7D+%5Cnabla+%5Cpi+%5C%2C+Q%5E%5Cpi%28x_s%2C+a_s%29%5D+%3D+%5Cmathbb%7BE%7D_%5Cmu%5B%5Cdfrac%7B%5Cpi%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D+%5Cnabla+%5Clog+%5Cpi+Q%5E%5Cpi%28x_s%2C+a_s%29%5D\" alt=\"\\mathbb{E}_\\mu[\\nabla \\log \\mu(a_s|x_s) Q^\\mu(x_s, a_s)] = \\mathbb{E}_\\mu[\\dfrac{1}{\\dfrac{\\mu}{\\pi} \\pi} \\nabla \\pi \\, Q^\\pi(x_s, a_s)] = \\mathbb{E}_\\mu[\\dfrac{\\pi(a_s|x_s)}{\\mu(a_s|x_s)} \\nabla \\log \\pi Q^\\pi(x_s, a_s)]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E接下来，\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdfrac%7B%5Cpi%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D\" alt=\"\\dfrac{\\pi(a_s|x_s)}{\\mu(a_s|x_s)}\" eeimg=\"1\"\u002F\u003E 容易算发散，我们把它换成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cdfrac%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D+%5Cpropto+%5Cmin%28%5Cbar%7B%5Crho%7D%2C+%5Cdfrac%7B%5Cpi%28a_s%7Cx_s%29%7D%7B%5Cmu%28a_s%7Cx_s%29%7D%29+%3D+%5Crho_s\" alt=\"\\dfrac{\\pi_{\\bar{\\rho}}(a_s|x_s)}{\\mu(a_s|x_s)} \\propto \\min(\\bar{\\rho}, \\dfrac{\\pi(a_s|x_s)}{\\mu(a_s|x_s)}) = \\rho_s\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%5Cpi\" alt=\"Q^\\pi\" eeimg=\"1\"\u002F\u003E 没法估计到，我们只能估计到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D\" alt=\"Q^{\\pi_{\\bar{\\rho}}}\" eeimg=\"1\"\u002F\u003E ，即使用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_s+%2B+%5Cgamma+v_%7Bs%2B1%7D+\" alt=\"r_s + \\gamma v_{s+1} \" eeimg=\"1\"\u002F\u003E 作为估计。\u003C\u002Fli\u003E\u003Cli\u003E使用baseline来减小误差，即减去 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Ctheta%28x_s%29\" alt=\"V_\\theta(x_s)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E最后，actor的更新方向可以写为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6727d548447f3b1e9057702998e34fbe_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"978\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"978\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6727d548447f3b1e9057702998e34fbe_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;978&#39; height=&#39;94&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"978\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"978\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6727d548447f3b1e9057702998e34fbe_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6727d548447f3b1e9057702998e34fbe_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E3.3 熵\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E前两项再加上对于熵的激励，就可以得到最后的算法更新公式。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a97dd86fc030030d1d675974342e4b26_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"984\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a97dd86fc030030d1d675974342e4b26_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;984&#39; height=&#39;114&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"984\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a97dd86fc030030d1d675974342e4b26_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-a97dd86fc030030d1d675974342e4b26_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E实验结果\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E实验主要说明了以下几点：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E相比于A3C和batched A2C，具有更好的高性能计算性能；\u003C\u002Fli\u003E\u003Cli\u003E单任务训练上相比于分布式A3C、单机A3C和batched A2C有更好的性能，并且对于超参数更稳定；\u003C\u002Fli\u003E\u003Cli\u003E本文中使用的V-trace相比于no correction、 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -correction、1-step importance sampling都有更好的效果（ablation study）。其中no correction指的是认为样本都是on-policy样本； \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -correction指的是仅仅在计算 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clog+%5Cpi\" alt=\"\\log \\pi\" eeimg=\"1\"\u002F\u003E 的时候加上一个很小的数值防止不稳定（不太懂）；1-step importance sampling，V-trace其实是做了多步，这里只做一步。\u003C\u002Fli\u003E\u003Cli\u003E训练单一智能体去完成多个任务。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E其他技术\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E在这种大规模训练中，训练一次耗资巨大，为了避免训练的这一波陷入局部极小值点，采用了 \u003Cb\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fdeepmind.com\u002Fblog\u002Fpopulation-based-training-neural-networks\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003Epopulation based training（PBT）\u003C\u002Fa\u003E \u003C\u002Fb\u003E方法。每次训练若干个智能体，每隔一段时间剔除表现不好的，并且对于表现较好的智能体进行mutation（通常是扰动一下超参数组合）。通过这种方法，保证长达几天的训练结束后能得到好的结果。\u003C\u002Fp\u003E\u003Cp\u003E有意思是，通过这种方法，学习率会随着学习进度自然慢慢减小，这和很多算法里面linear scheduled learning rate的trick不谋而合。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b4b947e972aedb456a710248031b7884_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2084\" data-rawheight=\"870\" class=\"origin_image zh-lightbox-thumb\" width=\"2084\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b4b947e972aedb456a710248031b7884_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2084&#39; height=&#39;870&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2084\" data-rawheight=\"870\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2084\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b4b947e972aedb456a710248031b7884_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b4b947e972aedb456a710248031b7884_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Cb\u003EV-trace定理证明\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E首先定义V-trace的算子\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2b8ba3a8d4c2e9aeda638a1d190cc15d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2018\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"2018\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2b8ba3a8d4c2e9aeda638a1d190cc15d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2018&#39; height=&#39;126&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2018\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2018\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2b8ba3a8d4c2e9aeda638a1d190cc15d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2b8ba3a8d4c2e9aeda638a1d190cc15d_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E不考虑神经网络 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Ctheta%28x_s%29\" alt=\"V_\\theta(x_s)\" eeimg=\"1\"\u002F\u003E 的拟合误差，可以认为每轮更新中价值函数都在做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28x%29+%5Cto+%5Cmathcal%7BR%7D+V%28x%29%2C+%5C+%5Cforall+x\" alt=\"V(x) \\to \\mathcal{R} V(x), \\ \\forall x\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E定理1\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E关于V-trace算子不动点和收敛速度的定理可以表述为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce4068b6368befdf894571d17720d6b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2078\" data-rawheight=\"532\" class=\"origin_image zh-lightbox-thumb\" width=\"2078\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce4068b6368befdf894571d17720d6b0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2078&#39; height=&#39;532&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2078\" data-rawheight=\"532\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2078\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce4068b6368befdf894571d17720d6b0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce4068b6368befdf894571d17720d6b0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E证明\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E首先把算子重写，前面一个求和里面有前后两个状态的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28x_t%29%2C+V%28x_%7Bt%2B1%7D%29\" alt=\"V(x_t), V(x_{t+1})\" eeimg=\"1\"\u002F\u003E ，这里把它们重排一下，\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-158c0f96291d872b85ce502d9cc9c114_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1990\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"1990\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-158c0f96291d872b85ce502d9cc9c114_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1990&#39; height=&#39;168&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1990\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1990\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-158c0f96291d872b85ce502d9cc9c114_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-158c0f96291d872b85ce502d9cc9c114_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E证明不动点和收敛速率最关键的是证明contraction，我们复习一下contraction的形式\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-24c0204745b25b2818f26830159de249_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1460\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"1460\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-24c0204745b25b2818f26830159de249_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1460&#39; height=&#39;100&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1460\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1460\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-24c0204745b25b2818f26830159de249_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-24c0204745b25b2818f26830159de249_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E即每作用一次该算子，任意两个状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_1\" alt=\"V_1\" eeimg=\"1\"\u002F\u003E 离 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_2\" alt=\"V_2\" eeimg=\"1\"\u002F\u003E 的距离都会减小，这样作用无穷多次之后，不管最开始状态价值函数是什么，都会收敛到同一个状态价值函数上。因此，我们自然需要把上式左手边写出来看看。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2010\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb\" width=\"2010\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2010&#39; height=&#39;376&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2010\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2010\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9f271e8cba3fa6b98cb2a4ec3bbb70dc_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E化简的要点就是凑出右手边 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_1+-+V_2\" alt=\"V_1 - V_2\" eeimg=\"1\"\u002F\u003E 这种形式。定义系数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t\" alt=\"\\alpha_t\" eeimg=\"1\"\u002F\u003E ，并且考虑到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_t\" alt=\"\\rho_t\" eeimg=\"1\"\u002F\u003E 的定义，有\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bf8794be2477ee55b7a6cb6c63fd0f4b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1988\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"1988\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bf8794be2477ee55b7a6cb6c63fd0f4b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1988&#39; height=&#39;102&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1988\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1988\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bf8794be2477ee55b7a6cb6c63fd0f4b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bf8794be2477ee55b7a6cb6c63fd0f4b_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以看出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BR%7D+V_1%28x%29+-+%5Cmathcal%7BR%7D+V_2%28x%29\" alt=\"\\mathcal{R} V_1(x) - \\mathcal{R} V_2(x)\" eeimg=\"1\"\u002F\u003E 被凑成了其他 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_1%28x_t%29+-+V_2%28x_t%29\" alt=\"V_1(x_t) - V_2(x_t)\" eeimg=\"1\"\u002F\u003E 的线性组合，如果我们能够证明组合中的各项系数和小于1，那么就容易证明 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7C%7C+%5Cmathcal%7BR%7D%5En+V_1%28x%29+-+%5Cmathcal%7BR%7D%5En+V_2%28x%29+%7C%7C_%5Cinfty+%5Cto+0\" alt=\"|| \\mathcal{R}^n V_1(x) - \\mathcal{R}^n V_2(x) ||_\\infty \\to 0\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b6f79abceb6f4309e4ce247bddbab687_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2008\" data-rawheight=\"864\" class=\"origin_image zh-lightbox-thumb\" width=\"2008\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b6f79abceb6f4309e4ce247bddbab687_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2008&#39; height=&#39;864&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2008\" data-rawheight=\"864\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2008\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b6f79abceb6f4309e4ce247bddbab687_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b6f79abceb6f4309e4ce247bddbab687_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中倒数第三个不等式是利用了求和符号内各项的非负性，只保留了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t%3D0\" alt=\"t=0\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t%3D1\" alt=\"t=1\" eeimg=\"1\"\u002F\u003E 项，扔掉了其他项。其中倒数第三项就是该contraction的收敛速率 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"\u002F\u003E ，可以看出，该速率与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7B%5Crho%7D\" alt=\"\\bar{\\rho}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbar%7Bc%7D\" alt=\"\\bar{c}\" eeimg=\"1\"\u002F\u003E 有关。contraction的存在，也说明了唯一不动点的存在。\u003C\u002Fp\u003E\u003Cp\u003E下面验证 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D\" alt=\"V^{\\pi_{\\bar{\\rho}}}\" eeimg=\"1\"\u002F\u003E 为不动点，这只需要说明 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BR%7D+V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D+-+V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D+%3D+0\" alt=\"\\mathcal{R} V^{\\pi_{\\bar{\\rho}}} - V^{\\pi_{\\bar{\\rho}}} = 0\" eeimg=\"1\"\u002F\u003E 即可。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8f19ff183357c5e8683e97e6e75541b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1998\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb\" width=\"1998\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8f19ff183357c5e8683e97e6e75541b4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1998&#39; height=&#39;464&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1998\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1998\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8f19ff183357c5e8683e97e6e75541b4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8f19ff183357c5e8683e97e6e75541b4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中第一等式是按照 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_t\" alt=\"\\rho_t\" eeimg=\"1\"\u002F\u003E 的定义写出，第二个等式可以由前面 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D\" alt=\"\\pi_{\\bar{\\rho}}\" eeimg=\"1\"\u002F\u003E 的定义得到，最后一个等式是因为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D%28x_t%29++%3D+%5Csum_a+%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%28a%7Cx_t%29+%5Br%28x_t%2C+a%29+%2B+%5Cgamma+%5Csum_%7Bx_%7Bt%2B1%7D%7D+p%28x_%7Bt%2B1%7D%7Cx_t%2C+a%29V%5E%7B%5Cpi_%7B%5Cbar%7B%5Crho%7D%7D%7D%28x_%7Bt%2B1%7D%29+%5D\" alt=\"V^{\\pi_{\\bar{\\rho}}}(x_t)  = \\sum_a \\pi_{\\bar{\\rho}}(a|x_t) [r(x_t, a) + \\gamma \\sum_{x_{t+1}} p(x_{t+1}|x_t, a)V^{\\pi_{\\bar{\\rho}}}(x_{t+1}) ]\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E定理2\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E定理1说明了如果在每轮，每个状态都访问并且更新一遍，那么能收敛到一个确定的不动点。但实际中，每个状态并不能在每一轮中都是均匀和遍历地访问的，而是走一个轨迹，走到哪个状态就更新哪个状态。这种情况下（online learning），是否还能收敛呢？下面这个定理说明，它也能收敛。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-41832ef8799b820f1530fb1a3c544c4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2042\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb\" width=\"2042\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-41832ef8799b820f1530fb1a3c544c4c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2042&#39; height=&#39;626&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2042\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2042\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-41832ef8799b820f1530fb1a3c544c4c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-41832ef8799b820f1530fb1a3c544c4c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Cb\u003E参考文献\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E[1] Chen, Jianmin, et al. &#34;Revisiting distributed synchronous SGD.&#34;\u003Ci\u003EarXiv preprint arXiv:1604.00981\u003C\u002Fi\u003E(2016).\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":26,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":2,"contributions":[{"id":20347215,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 44】IMPALA\u002FV-trace - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F58226117 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-1","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"web_sec672","type":"String","value":"0"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F58226117","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F58226117","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>