<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习算法 18】FuN - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="算法,机器学习,强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="很老的一篇文章提出了feudal RL，FuN指的是把它用于分层强化学习。原文传送门：Vezhnevets, Alexander Sasha, et al. &amp;#34;Feudal networks for hierarchical reinforcement learning.&amp;#34; arXiv preprint arXiv:…"/><meta data-react-helmet="true" property="og:title" content="【强化学习算法 18】FuN"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/46928498"/><meta data-react-helmet="true" property="og:description" content="很老的一篇文章提出了feudal RL，FuN指的是把它用于分层强化学习。原文传送门：Vezhnevets, Alexander Sasha, et al. &amp;#34;Feudal networks for hierarchical reinforcement learning.&amp;#34; arXiv preprint arXiv:…"/><meta data-react-helmet="true" property="og:image" content="https://pic3.zhimg.com/v2-2173d86a3c20337ce68a5d2ef2baa822_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:46928498,&quot;title&quot;:&quot;【强化学习算法 18】FuN&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic3.zhimg.com/v2-2173d86a3c20337ce68a5d2ef2baa822_1200x500.jpg" alt="【强化学习算法 18】FuN"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习算法 18】FuN</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">14 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>很老的一篇文章提出了feudal RL，FuN指的是把它用于分层强化学习。</p><h2><b>原文传送门：</b></h2><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.01161" class=" wrap external" target="_blank" rel="nofollow noreferrer">Vezhnevets, Alexander Sasha, et al. &#34;Feudal networks for hierarchical reinforcement learning.&#34; arXiv preprint arXiv:1703.01161 (2017).</a> </p><h2><b>背景：</b></h2><p>分层强化学习被认为是解决复杂强化学习问题的重要方法，一般来讲大家就分为两层，使用这篇文章里面的表示方法就是Manager和Worker。如果不分层，有很多off-the-shelf的强化学习算法已经能够解这样一个 <img src="https://www.zhihu.com/equation?tex=MDP%3D%5C%7BS%2C+A%2C+R%2C+T%2C+%5Cgamma%5C%7D" alt="MDP=\{S, A, R, T, \gamma\}" eeimg="1"/> 了，其中各个元素分别代表状态空间、动作空间、奖励、状态转移概率、discount rate。当分层之后，自然就变成了两个MDP了，一个Manager的、一个Worker的，那么这两个MDP的这个五个元素该怎么定义就需要一一考虑一下了。</p><ul><li>Manager的状态空间一般也就可以直接用原来MDP的状态空间。当然对于高维状态空间（比如图像），会用CNN转化到embedded space里面；对于不完全可观测的、非马可夫的，可能还可以用RNN来增强可观测性（这篇文章里面也用到了）。</li><li>Manager的动作空间就是一个很大的研究热点了。一种方案就是其动作空间就用来选择不同的worker去做操作，比如Manager这会让第一个worker去执行，过会再让第二个worker去执行，这对应的大概就是option-critic framework（可以参考本专栏里面讲的NJUStarCraft那篇）；第二种方案就是Manager制定一个目标，然后让Worker去执行，那么目标怎么定义又是一个问题了，这也是本文试图解决的一个问题。</li><li>Manager的奖励也可以直接用原来MDP的奖励。需要注意到HRL是用来解决困难强化学习问题的工具，其中一个很大的困难体现在sparse reward，换句话说就是long-term credit assignment问题。Manager的目标就是在时间尺度上看的更粗略一些，这样原本稀疏的奖励在Manager看来就不那么稀疏了。</li><li>Manager对应的Transition也会相应变化，以前的Transition Model是 <img src="https://www.zhihu.com/equation?tex=p%28s_%7Bt%2B1%7D%7Cs_t%2C+a_t%29" alt="p(s_{t+1}|s_t, a_t)" eeimg="1"/> ，现在的差不多变成了 <img src="https://www.zhihu.com/equation?tex=p%28s_%7Bt%2Bc%7D%7Cs_t%2C+g_t%29" alt="p(s_{t+c}|s_t, g_t)" eeimg="1"/> ，一般Manager的动作用<img src="https://www.zhihu.com/equation?tex=g_t" alt="g_t" eeimg="1"/>表示（goal）。这就产生了这篇文章里面讲的transition policy gradient（可以等会回过头来看）。</li><li>Manager希望看的更远，更加注重长远的奖励，因此它相比于Worker来讲可以有更大的 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"/> 。</li><li>Worker的状态空间呢？一般来讲，把原MDP的状态和Manager产生的goal输进来作为Worker的状态。</li><li>Worker的动作空间一般就是原问题的动作空间，也就是原问题的实际操作（脏活累活）都交给Worker来实际解决。</li><li>Worker的奖励也是一个比较棘手的问题。如果直接用原问题的奖励，原问题就是因为奖励稀疏所以难学，现在还用原来的奖励的话，等于是来了个领导不干活，所有的任务都靠Worker来做，这样显然不科学。因此，目前大家要么1）使用Manager产生的goal演化出来的比较密集的奖励（这篇文章用的方法），要么2）使用只依赖环境动力学模型的intrinsic reward（也是一个活跃的研究方向）。</li><li>Worker的transition model和discount rate和原问题类似，可以使用off-the-shelf的各种RL算法来做。</li><li>控制权的转移是分层带来的问题中的一个。</li><ul><li>一般大家每c步让Manager做一下决策，然后接下来c步都让Worker行动，这样做的好处是在时间上显式地划分了两者的层级。但c怎么选择？是不是什么sub-policy都是固定c步比较好呢？</li><li>另外有方法是让Worker行动结束了告诉Manager，但是很容易收敛到极差的worker（所有的worker都只做原来行动空间里面的一步，主要的任务靠Manager来完成）或者极好的worker（一个worker把所有的活都干了）。这都不是我们想要的。一个粗暴的解决方法就是加regularizer使得worker不要太强或者太弱。</li><li>这篇文章里面就Manager每步都出信号，但是也做了限制避免产生一个“朝令夕改”的领导。</li></ul></ul><p>有了这些框架性概念我们就可以进入这篇文章给我们提供的解决方案了。</p><h2><b>过程：</b></h2><p><b>1. 总体流程</b></p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-5737e2b796918be47f0185d8e415fab4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1018" data-rawheight="622" class="origin_image zh-lightbox-thumb" width="1018" data-original="https://pic1.zhimg.com/v2-5737e2b796918be47f0185d8e415fab4_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1018&#39; height=&#39;622&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1018" data-rawheight="622" class="origin_image zh-lightbox-thumb lazy" width="1018" data-original="https://pic1.zhimg.com/v2-5737e2b796918be47f0185d8e415fab4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-5737e2b796918be47f0185d8e415fab4_b.jpg"/></figure><p>由于高维输入，因此做了embedding（ <img src="https://www.zhihu.com/equation?tex=f%5E%7Bpercept%7D" alt="f^{percept}" eeimg="1"/> ）；Manager和Worker可能需要的对状态的embedding不一样，因此Manager又做了一个transform（ <img src="https://www.zhihu.com/equation?tex=f%5E%7BMspace%7D" alt="f^{Mspace}" eeimg="1"/> ）；任务可能non-Markovian，因此做了RNN的embedding（ <img src="https://www.zhihu.com/equation?tex=f%5E%7BMrnn%7D" alt="f^{Mrnn}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=f%5E%7BWrnn%7D" alt="f^{Wrnn}" eeimg="1"/> ）；Manager产生的goal扔给Worker，为了防止这个每步生产的goal“朝令夕改”，因此对近c步的goal做pooling和linear projection之后再扔给Worker（ <img src="https://www.zhihu.com/equation?tex=%5Cvarphi" alt="\varphi" eeimg="1"/> ）；两者拼起来通过神经网络（ <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"/> ）形成最后的动作。</p><p>这整个流程都是从头到尾都可求导的，直接用现有的RL算法就可以end-to-end训练了。那我们分层还有什么意义？？（惊！）</p><p>原来如果这样求解，Manager产生的goal就等于是一个hidden variable，没有对应的可以说的明白的含义了。因此，训练必须把两层分来来做训练。</p><p><b>2. 如何定义Manager的action（goal）？</b></p><p>要想把Manager和Worker分开训练，首先要做的就是给Manager输出的action（goal）一个确定的含义，而不是让它仅仅只是一个隐变量。这里规定goal是<b>学习到的低维状态表示空间中的方向</b>。学习到的低维状态表示即图上的 <img src="https://www.zhihu.com/equation?tex=z_t%5Cin%5Cmathbb%7BR%7D%5Ed" alt="z_t\in\mathbb{R}^d" eeimg="1"/> ，而输出的 <img src="https://www.zhihu.com/equation?tex=g_t+%5Cin+%5Cmathbb%7BR%7D%5Ed" alt="g_t \in \mathbb{R}^d" eeimg="1"/> 表示希望Worker能通过采取各种行动使得状态的表示往 <img src="https://www.zhihu.com/equation?tex=z_t+%2B+k+g_t" alt="z_t + k g_t" eeimg="1"/> 方向变化。</p><p><b>3. 如何定义Worker的reward？</b></p><p>Manager制定的目标就需要Worker来完成，那么Worker就需要不仅受到外部奖励的激励，还需要受到内部奖励的激励，即 <img src="https://www.zhihu.com/equation?tex=R_t+%2B%5Calpha+R_t%5EI" alt="R_t +\alpha R_t^I" eeimg="1"/> 。内部奖励的定义需要和Manager的目标关联，这里的定义是</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-7dfa576791e8e527f433363b471b2948_b.jpg" data-caption="" data-size="normal" data-rawwidth="568" data-rawheight="85" class="origin_image zh-lightbox-thumb" width="568" data-original="https://pic1.zhimg.com/v2-7dfa576791e8e527f433363b471b2948_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;568&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="568" data-rawheight="85" class="origin_image zh-lightbox-thumb lazy" width="568" data-original="https://pic1.zhimg.com/v2-7dfa576791e8e527f433363b471b2948_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-7dfa576791e8e527f433363b471b2948_b.jpg"/></figure><p>可以理解为当前状态相比于前 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"/> 步中每一步目标完成情况的平均值。被称作<b>directional cosine similarity reward</b>。</p><p><b>4. 如何训练Manager？</b></p><p>刚刚提到要分成两部分来训练。对应训练Manager的方法就是这里所谓的<b>transition policy gradient</b>，它和普通Policy Gradient的区别就是普通的PG的transition model是 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28s_%7Bt%2B1%7D%7Cs_t%2C+a_t%29" alt="\pi(s_{t+1}|s_t, a_t)" eeimg="1"/> ，而这里把每个连续的 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"/> 步看做Manager的一步，把Worker连续 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"/> 步的产生的状态变化当做一步transition，即 <img src="https://www.zhihu.com/equation?tex=%5Cpi%5E%7BTP%7D%28s_%7Bt%2Bc%7D%7Cs_t%29+%3D+p%28s_%7Bt%2Bc%7D%7Cs_t%2C+g_t%29+%3D+p%28s_%7Bt%2Bc%7D%7Cs_t%2C+%5Cmu%28s_t%2C+%5Ctheta%29%29" alt="\pi^{TP}(s_{t+c}|s_t) = p(s_{t+c}|s_t, g_t) = p(s_{t+c}|s_t, \mu(s_t, \theta))" eeimg="1"/> 。</p><p>对应产生的策略梯度公式就是</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-8d72b1e3026b14767146d2785e013863_b.jpg" data-caption="" data-size="normal" data-rawwidth="624" data-rawheight="79" class="origin_image zh-lightbox-thumb" width="624" data-original="https://pic4.zhimg.com/v2-8d72b1e3026b14767146d2785e013863_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;624&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="624" data-rawheight="79" class="origin_image zh-lightbox-thumb lazy" width="624" data-original="https://pic4.zhimg.com/v2-8d72b1e3026b14767146d2785e013863_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-8d72b1e3026b14767146d2785e013863_b.jpg"/></figure><p>由于内部奖励的形式是上面给出的形式，这就激励了Manager的transition分布更接近von Mises-Fisher分布，即 <img src="https://www.zhihu.com/equation?tex=p%28s_%7Bt%2Bc%7D%7Cs_t%2C+g_t%29+%5Cpropto+%5Cexp+d_%7B%5Ccos%7D%28s_%7Bt%2Bc%7D-s_t%2C+g_t%29+" alt="p(s_{t+c}|s_t, g_t) \propto \exp d_{\cos}(s_{t+c}-s_t, g_t) " eeimg="1"/> ，因此可以进一步把上式化为</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-11f1fa6808dc331c27e079965b2f1a5f_b.jpg" data-caption="" data-size="normal" data-rawwidth="558" data-rawheight="77" class="origin_image zh-lightbox-thumb" width="558" data-original="https://pic4.zhimg.com/v2-11f1fa6808dc331c27e079965b2f1a5f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;558&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="558" data-rawheight="77" class="origin_image zh-lightbox-thumb lazy" width="558" data-original="https://pic4.zhimg.com/v2-11f1fa6808dc331c27e079965b2f1a5f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-11f1fa6808dc331c27e079965b2f1a5f_b.jpg"/></figure><p>接下来就可以用策略梯度类方法来训练Manager了，文章中使用A3C方法。</p><p><b>5. 如何训练Worker？</b></p><p>定义好了Worker的奖励之后，Worker的训练就变成了普通的RL问题了，使用A3C解决。</p><h2><b>结果：</b></h2><p>论文的positive result肯定都包含学习曲线，说明该算法的渐进性能和收敛速度更好，这里不贴出来了。</p><p>比较有意思的是一些说明Manager确实能学习到有意义的goal的实验。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-fee41aa02e5b62ecbaf3900091b7589c_b.jpg" data-caption="" data-size="normal" data-rawwidth="1354" data-rawheight="647" class="origin_image zh-lightbox-thumb" width="1354" data-original="https://pic1.zhimg.com/v2-fee41aa02e5b62ecbaf3900091b7589c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1354&#39; height=&#39;647&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1354" data-rawheight="647" class="origin_image zh-lightbox-thumb lazy" width="1354" data-original="https://pic1.zhimg.com/v2-fee41aa02e5b62ecbaf3900091b7589c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-fee41aa02e5b62ecbaf3900091b7589c_b.jpg"/></figure><p>这是在Montezuma’s Revenge这个游戏上的实验，底下的count表明这一帧图像在前面的多少个时刻被当做goal，即可以把这样的bar比较高的位置对应的游戏局面理解为是Manager预想希望达到的局面。不难看出，这些帧基本上都是拿到钥匙的关键路径，比如先从楼梯边上跳下来，然后爬梯子、过河。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-7e767a069f4335bdd9506dd3683e7f49_b.jpg" data-caption="" data-size="normal" data-rawwidth="1316" data-rawheight="442" class="origin_image zh-lightbox-thumb" width="1316" data-original="https://pic2.zhimg.com/v2-7e767a069f4335bdd9506dd3683e7f49_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1316&#39; height=&#39;442&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1316" data-rawheight="442" class="origin_image zh-lightbox-thumb lazy" width="1316" data-original="https://pic2.zhimg.com/v2-7e767a069f4335bdd9506dd3683e7f49_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-7e767a069f4335bdd9506dd3683e7f49_b.jpg"/></figure><p>这个是在SeaQuest这个游戏上面的实验。后面的热力图表示不同策略下小船出现在不同地方的概率，可以看到选择不同的goal产生的不同sub-policy确实让小船局限在不同的区域移动，甚至比如sub-policy3的动作就是浮到水面上换气。</p><hr/><h2><b>Manger训练的特殊技巧</b></h2><p>回看算法的整体框图，Manager和Worker的策略产生都是使用一个RNN网络来做的，文章里面使用的LSTM。但是Manager希望在更粗粒度的时间尺度上观察问题，具体说来就是相对于Worker看问题的尺度有一个 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"/> 倍的scale。为了让Manager确实能比Worker记忆的时间更长，自然想法就是每 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"/> 步才喂一个数据到Manager的LSTM里面。但是这样可能漏掉中间的一些信息。然后自然会想到，弄 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"/> 个这样的LSTM，每步数据进来的时候只训练其中的第 <img src="https://www.zhihu.com/equation?tex=t+%5C%25+c" alt="t \% c" eeimg="1"/> 个模型（%表示取余）。这就是文章里面提到的<b>dilated LSTM</b>。</p></div></div><div class="ContentItem-time">发布于 2018-10-17</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19553510" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">算法</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">机器学习</div></div></a></span></div><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 14 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 14</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="763e4d71-b8b2-4225-9e5f-adcde3aa4749" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="763e4d71-b8b2-4225-9e5f-adcde3aa4749">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"46928498":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":46928498,"title":"【强化学习算法 18】FuN","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F46928498","imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2173d86a3c20337ce68a5d2ef2baa822_b.jpg","titleImage":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2173d86a3c20337ce68a5d2ef2baa822_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bcadf96f475546d80c207b0473300790_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"622\" data-watermark=\"watermark\" data-original-src=\"v2-bcadf96f475546d80c207b0473300790\" data-watermark-src=\"v2-5737e2b796918be47f0185d8e415fab4\" data-private-watermark-src=\"\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bcadf96f475546d80c207b0473300790_r.jpg\" class=\"origin_image inline-img zh-lightbox-thumb\"\u002F\u003E很老的一篇文章提出了feudal RL，FuN指的是把它用于分层强化学习。\u003Cb\u003E原文传送门：\u003C\u002Fb\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1703.01161\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EVezhnevets, Alexander Sasha, et al. &#34;Feudal networks for hierarchical reinforcement learning.&#34; arXiv preprint arXiv:1703.01161 (2017).\u003C\u002Fa\u003E \u003Cb\u003E背景：\u003C\u002Fb\u003E分层强化学习被认为是解…","created":1539735828,"updated":1539735828,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1188,"imageHeight":407,"content":"\u003Cp\u003E很老的一篇文章提出了feudal RL，FuN指的是把它用于分层强化学习。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E原文传送门：\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Farxiv.org\u002Fpdf\u002F1703.01161\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EVezhnevets, Alexander Sasha, et al. &#34;Feudal networks for hierarchical reinforcement learning.&#34; arXiv preprint arXiv:1703.01161 (2017).\u003C\u002Fa\u003E \u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E背景：\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E分层强化学习被认为是解决复杂强化学习问题的重要方法，一般来讲大家就分为两层，使用这篇文章里面的表示方法就是Manager和Worker。如果不分层，有很多off-the-shelf的强化学习算法已经能够解这样一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=MDP%3D%5C%7BS%2C+A%2C+R%2C+T%2C+%5Cgamma%5C%7D\" alt=\"MDP=\\{S, A, R, T, \\gamma\\}\" eeimg=\"1\"\u002F\u003E 了，其中各个元素分别代表状态空间、动作空间、奖励、状态转移概率、discount rate。当分层之后，自然就变成了两个MDP了，一个Manager的、一个Worker的，那么这两个MDP的这个五个元素该怎么定义就需要一一考虑一下了。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EManager的状态空间一般也就可以直接用原来MDP的状态空间。当然对于高维状态空间（比如图像），会用CNN转化到embedded space里面；对于不完全可观测的、非马可夫的，可能还可以用RNN来增强可观测性（这篇文章里面也用到了）。\u003C\u002Fli\u003E\u003Cli\u003EManager的动作空间就是一个很大的研究热点了。一种方案就是其动作空间就用来选择不同的worker去做操作，比如Manager这会让第一个worker去执行，过会再让第二个worker去执行，这对应的大概就是option-critic framework（可以参考本专栏里面讲的NJUStarCraft那篇）；第二种方案就是Manager制定一个目标，然后让Worker去执行，那么目标怎么定义又是一个问题了，这也是本文试图解决的一个问题。\u003C\u002Fli\u003E\u003Cli\u003EManager的奖励也可以直接用原来MDP的奖励。需要注意到HRL是用来解决困难强化学习问题的工具，其中一个很大的困难体现在sparse reward，换句话说就是long-term credit assignment问题。Manager的目标就是在时间尺度上看的更粗略一些，这样原本稀疏的奖励在Manager看来就不那么稀疏了。\u003C\u002Fli\u003E\u003Cli\u003EManager对应的Transition也会相应变化，以前的Transition Model是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s_%7Bt%2B1%7D%7Cs_t%2C+a_t%29\" alt=\"p(s_{t+1}|s_t, a_t)\" eeimg=\"1\"\u002F\u003E ，现在的差不多变成了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s_%7Bt%2Bc%7D%7Cs_t%2C+g_t%29\" alt=\"p(s_{t+c}|s_t, g_t)\" eeimg=\"1\"\u002F\u003E ，一般Manager的动作用\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g_t\" alt=\"g_t\" eeimg=\"1\"\u002F\u003E表示（goal）。这就产生了这篇文章里面讲的transition policy gradient（可以等会回过头来看）。\u003C\u002Fli\u003E\u003Cli\u003EManager希望看的更远，更加注重长远的奖励，因此它相比于Worker来讲可以有更大的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003EWorker的状态空间呢？一般来讲，把原MDP的状态和Manager产生的goal输进来作为Worker的状态。\u003C\u002Fli\u003E\u003Cli\u003EWorker的动作空间一般就是原问题的动作空间，也就是原问题的实际操作（脏活累活）都交给Worker来实际解决。\u003C\u002Fli\u003E\u003Cli\u003EWorker的奖励也是一个比较棘手的问题。如果直接用原问题的奖励，原问题就是因为奖励稀疏所以难学，现在还用原来的奖励的话，等于是来了个领导不干活，所有的任务都靠Worker来做，这样显然不科学。因此，目前大家要么1）使用Manager产生的goal演化出来的比较密集的奖励（这篇文章用的方法），要么2）使用只依赖环境动力学模型的intrinsic reward（也是一个活跃的研究方向）。\u003C\u002Fli\u003E\u003Cli\u003EWorker的transition model和discount rate和原问题类似，可以使用off-the-shelf的各种RL算法来做。\u003C\u002Fli\u003E\u003Cli\u003E控制权的转移是分层带来的问题中的一个。\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli\u003E一般大家每c步让Manager做一下决策，然后接下来c步都让Worker行动，这样做的好处是在时间上显式地划分了两者的层级。但c怎么选择？是不是什么sub-policy都是固定c步比较好呢？\u003C\u002Fli\u003E\u003Cli\u003E另外有方法是让Worker行动结束了告诉Manager，但是很容易收敛到极差的worker（所有的worker都只做原来行动空间里面的一步，主要的任务靠Manager来完成）或者极好的worker（一个worker把所有的活都干了）。这都不是我们想要的。一个粗暴的解决方法就是加regularizer使得worker不要太强或者太弱。\u003C\u002Fli\u003E\u003Cli\u003E这篇文章里面就Manager每步都出信号，但是也做了限制避免产生一个“朝令夕改”的领导。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp\u003E有了这些框架性概念我们就可以进入这篇文章给我们提供的解决方案了。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E过程：\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Cb\u003E1. 总体流程\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5737e2b796918be47f0185d8e415fab4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"622\" class=\"origin_image zh-lightbox-thumb\" width=\"1018\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5737e2b796918be47f0185d8e415fab4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1018&#39; height=&#39;622&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"622\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1018\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5737e2b796918be47f0185d8e415fab4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5737e2b796918be47f0185d8e415fab4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E由于高维输入，因此做了embedding（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%5E%7Bpercept%7D\" alt=\"f^{percept}\" eeimg=\"1\"\u002F\u003E ）；Manager和Worker可能需要的对状态的embedding不一样，因此Manager又做了一个transform（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%5E%7BMspace%7D\" alt=\"f^{Mspace}\" eeimg=\"1\"\u002F\u003E ）；任务可能non-Markovian，因此做了RNN的embedding（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%5E%7BMrnn%7D\" alt=\"f^{Mrnn}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%5E%7BWrnn%7D\" alt=\"f^{Wrnn}\" eeimg=\"1\"\u002F\u003E ）；Manager产生的goal扔给Worker，为了防止这个每步生产的goal“朝令夕改”，因此对近c步的goal做pooling和linear projection之后再扔给Worker（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cvarphi\" alt=\"\\varphi\" eeimg=\"1\"\u002F\u003E ）；两者拼起来通过神经网络（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X\" alt=\"X\" eeimg=\"1\"\u002F\u003E ）形成最后的动作。\u003C\u002Fp\u003E\u003Cp\u003E这整个流程都是从头到尾都可求导的，直接用现有的RL算法就可以end-to-end训练了。那我们分层还有什么意义？？（惊！）\u003C\u002Fp\u003E\u003Cp\u003E原来如果这样求解，Manager产生的goal就等于是一个hidden variable，没有对应的可以说的明白的含义了。因此，训练必须把两层分来来做训练。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E2. 如何定义Manager的action（goal）？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E要想把Manager和Worker分开训练，首先要做的就是给Manager输出的action（goal）一个确定的含义，而不是让它仅仅只是一个隐变量。这里规定goal是\u003Cb\u003E学习到的低维状态表示空间中的方向\u003C\u002Fb\u003E。学习到的低维状态表示即图上的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=z_t%5Cin%5Cmathbb%7BR%7D%5Ed\" alt=\"z_t\\in\\mathbb{R}^d\" eeimg=\"1\"\u002F\u003E ，而输出的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g_t+%5Cin+%5Cmathbb%7BR%7D%5Ed\" alt=\"g_t \\in \\mathbb{R}^d\" eeimg=\"1\"\u002F\u003E 表示希望Worker能通过采取各种行动使得状态的表示往 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=z_t+%2B+k+g_t\" alt=\"z_t + k g_t\" eeimg=\"1\"\u002F\u003E 方向变化。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E3. 如何定义Worker的reward？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003EManager制定的目标就需要Worker来完成，那么Worker就需要不仅受到外部奖励的激励，还需要受到内部奖励的激励，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_t+%2B%5Calpha+R_t%5EI\" alt=\"R_t +\\alpha R_t^I\" eeimg=\"1\"\u002F\u003E 。内部奖励的定义需要和Manager的目标关联，这里的定义是\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7dfa576791e8e527f433363b471b2948_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"568\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"568\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7dfa576791e8e527f433363b471b2948_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;568&#39; height=&#39;85&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"568\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"568\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7dfa576791e8e527f433363b471b2948_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7dfa576791e8e527f433363b471b2948_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以理解为当前状态相比于前 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c\" alt=\"c\" eeimg=\"1\"\u002F\u003E 步中每一步目标完成情况的平均值。被称作\u003Cb\u003Edirectional cosine similarity reward\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E4. 如何训练Manager？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E刚刚提到要分成两部分来训练。对应训练Manager的方法就是这里所谓的\u003Cb\u003Etransition policy gradient\u003C\u002Fb\u003E，它和普通Policy Gradient的区别就是普通的PG的transition model是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28s_%7Bt%2B1%7D%7Cs_t%2C+a_t%29\" alt=\"\\pi(s_{t+1}|s_t, a_t)\" eeimg=\"1\"\u002F\u003E ，而这里把每个连续的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c\" alt=\"c\" eeimg=\"1\"\u002F\u003E 步看做Manager的一步，把Worker连续 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c\" alt=\"c\" eeimg=\"1\"\u002F\u003E 步的产生的状态变化当做一步transition，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%5E%7BTP%7D%28s_%7Bt%2Bc%7D%7Cs_t%29+%3D+p%28s_%7Bt%2Bc%7D%7Cs_t%2C+g_t%29+%3D+p%28s_%7Bt%2Bc%7D%7Cs_t%2C+%5Cmu%28s_t%2C+%5Ctheta%29%29\" alt=\"\\pi^{TP}(s_{t+c}|s_t) = p(s_{t+c}|s_t, g_t) = p(s_{t+c}|s_t, \\mu(s_t, \\theta))\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E对应产生的策略梯度公式就是\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d72b1e3026b14767146d2785e013863_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"79\" class=\"origin_image zh-lightbox-thumb\" width=\"624\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d72b1e3026b14767146d2785e013863_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;624&#39; height=&#39;79&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"79\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"624\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d72b1e3026b14767146d2785e013863_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d72b1e3026b14767146d2785e013863_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E由于内部奖励的形式是上面给出的形式，这就激励了Manager的transition分布更接近von Mises-Fisher分布，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s_%7Bt%2Bc%7D%7Cs_t%2C+g_t%29+%5Cpropto+%5Cexp+d_%7B%5Ccos%7D%28s_%7Bt%2Bc%7D-s_t%2C+g_t%29+\" alt=\"p(s_{t+c}|s_t, g_t) \\propto \\exp d_{\\cos}(s_{t+c}-s_t, g_t) \" eeimg=\"1\"\u002F\u003E ，因此可以进一步把上式化为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-11f1fa6808dc331c27e079965b2f1a5f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"558\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb\" width=\"558\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-11f1fa6808dc331c27e079965b2f1a5f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;558&#39; height=&#39;77&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"558\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"558\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-11f1fa6808dc331c27e079965b2f1a5f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-11f1fa6808dc331c27e079965b2f1a5f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E接下来就可以用策略梯度类方法来训练Manager了，文章中使用A3C方法。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E5. 如何训练Worker？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E定义好了Worker的奖励之后，Worker的训练就变成了普通的RL问题了，使用A3C解决。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E结果：\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E论文的positive result肯定都包含学习曲线，说明该算法的渐进性能和收敛速度更好，这里不贴出来了。\u003C\u002Fp\u003E\u003Cp\u003E比较有意思的是一些说明Manager确实能学习到有意义的goal的实验。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fee41aa02e5b62ecbaf3900091b7589c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"647\" class=\"origin_image zh-lightbox-thumb\" width=\"1354\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fee41aa02e5b62ecbaf3900091b7589c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1354&#39; height=&#39;647&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"647\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1354\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fee41aa02e5b62ecbaf3900091b7589c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fee41aa02e5b62ecbaf3900091b7589c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这是在Montezuma’s Revenge这个游戏上的实验，底下的count表明这一帧图像在前面的多少个时刻被当做goal，即可以把这样的bar比较高的位置对应的游戏局面理解为是Manager预想希望达到的局面。不难看出，这些帧基本上都是拿到钥匙的关键路径，比如先从楼梯边上跳下来，然后爬梯子、过河。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7e767a069f4335bdd9506dd3683e7f49_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1316\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb\" width=\"1316\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7e767a069f4335bdd9506dd3683e7f49_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1316&#39; height=&#39;442&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1316\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1316\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7e767a069f4335bdd9506dd3683e7f49_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7e767a069f4335bdd9506dd3683e7f49_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这个是在SeaQuest这个游戏上面的实验。后面的热力图表示不同策略下小船出现在不同地方的概率，可以看到选择不同的goal产生的不同sub-policy确实让小船局限在不同的区域移动，甚至比如sub-policy3的动作就是浮到水面上换气。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Cb\u003EManger训练的特殊技巧\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E回看算法的整体框图，Manager和Worker的策略产生都是使用一个RNN网络来做的，文章里面使用的LSTM。但是Manager希望在更粗粒度的时间尺度上观察问题，具体说来就是相对于Worker看问题的尺度有一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c\" alt=\"c\" eeimg=\"1\"\u002F\u003E 倍的scale。为了让Manager确实能比Worker记忆的时间更长，自然想法就是每 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c\" alt=\"c\" eeimg=\"1\"\u002F\u003E 步才喂一个数据到Manager的LSTM里面。但是这样可能漏掉中间的一些信息。然后自然会想到，弄 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c\" alt=\"c\" eeimg=\"1\"\u002F\u003E 个这样的LSTM，每步数据进来的时候只训练其中的第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t+%5C%25+c\" alt=\"t \\% c\" eeimg=\"1\"\u002F\u003E 个模型（%表示取余）。这就是文章里面提到的\u003Cb\u003Edilated LSTM\u003C\u002Fb\u003E。\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19553510","type":"topic","id":"19553510","name":"算法"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","type":"topic","id":"19559450","name":"机器学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":14,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":2,"contributions":[{"id":2301782,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习算法 18】FuN - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F46928498 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-1","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F46928498","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F46928498","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>