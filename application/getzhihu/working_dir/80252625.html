<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习 92】Non-stationary MAB - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="MAB 问题（multi-armed-bandit problem）在 non-stationary reward 设定下的一种算法分析。关于 MAB 问题的具体定义，可以看 Sutton 的书（大概是第二章）。原文传送门Besbes, Omar, Yonatan Gur, and Assaf Zeevi…"/><meta data-react-helmet="true" property="og:title" content="【强化学习 92】Non-stationary MAB"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/80252625"/><meta data-react-helmet="true" property="og:description" content="MAB 问题（multi-armed-bandit problem）在 non-stationary reward 设定下的一种算法分析。关于 MAB 问题的具体定义，可以看 Sutton 的书（大概是第二章）。原文传送门Besbes, Omar, Yonatan Gur, and Assaf Zeevi…"/><meta data-react-helmet="true" property="og:image" content="https://pic3.zhimg.com/v2-8a21aab080f18c994db59df50b4cd5e7_b.jpg"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:80252625,&quot;title&quot;:&quot;【强化学习 92】Non-stationary MAB&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><img class="TitleImage" src="https://pic3.zhimg.com/v2-8a21aab080f18c994db59df50b4cd5e7_1200x500.jpg" alt="【强化学习 92】Non-stationary MAB"/><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习 92】Non-stationary MAB</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">12 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>MAB 问题（multi-armed-bandit problem）在 non-stationary reward 设定下的一种算法分析。关于 MAB 问题的具体定义，可以看 Sutton 的书（大概是第二章）。</p><h2>原文传送门</h2><p><a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Besbes, Omar, Yonatan Gur, and Assaf Zeevi. &#34;Stochastic multi-armed-bandit problem with non-stationary rewards.&#34; Advances in neural information processing systems. 2014.</a></p><h2>特色</h2><p>MAB 问题一般有两种设定：一种是 stochastic reward，即每个臂上奖励是从一个稳定的分布中 i.i.d. 采样得到的，该问题的 regret 和一直拉任意的一个臂（或者可以认为拉最优的臂）；另一种是 adversary reward，即考虑每个臂上的奖励可以被任意选择（即可能是有一个对手跟你『作对』），该问题的 regret 也是和一直拉任意一个臂相比。</p><p>本文研究的设定叫做 non-stationary reward，它可以被看做介于 stochastic 和 adversary 中间的一种状态，即各个臂上 reward 的分布可以变化，但是变化不太大。基于这种设定，其 regret 的基准可以选择的比 adversary setting 下更强一些，即可以和每一轮中最优的那个臂相比较。之前 regret 中基准为一直拉同一个臂，这种情形文章把它叫做 static oracle，现在期望比较的是每一轮中最优的那个臂，这种情形叫做 dynamic oracle。和 dynamic oracle 的比较更为困难。</p><h2>过程</h2><h3>1. 背景</h3><p>MAB 问题最原始的设定就是 stochastic setting，之后大家把该问题推广到各种不同的设定下。这篇文章也讲了一下之前的一些关于 reward 变化的 setting。</p><ul><li>被拉过的臂上的奖励分布会产生变化；</li><li>被拉过的臂上的奖励分布产生变化，并且该变化的过程已知；</li><li>给定总的奖励分布变化的次数；</li><li>给定最优臂的轮换次数；</li><li>Adversary 设定。</li></ul><h3>2. 问题设定</h3><p><b><i>2.1 基本的 MAB 问题设定</i></b></p><ul><li>臂： <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BK%7D%3D%5C%7B1%2C+%5Ccdots%2C+K%5C%7D" alt="\mathcal{K}=\{1, \cdots, K\}" eeimg="1"/> </li><li>玩的轮数： <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D+%3D+%5C%7B1%2C+%5Ccdots%2C+T%5C%7D" alt="\mathcal{T} = \{1, \cdots, T\}" eeimg="1"/> </li><li>在 <img src="https://www.zhihu.com/equation?tex=t%5Cin%5Cmathcal%7BT%7D" alt="t\in\mathcal{T}" eeimg="1"/> 的时候选取 <img src="https://www.zhihu.com/equation?tex=k%5Cin%5Cmathcal%7BK%7D" alt="k\in\mathcal{K}" eeimg="1"/> ，得到一个奖励 <img src="https://www.zhihu.com/equation?tex=X_t%5Ek%5Cin%5B0%2C1%5D" alt="X_t^k\in[0,1]" eeimg="1"/> </li><ul><li>注意到这里把奖励归一化了</li><li>按理来说，只有被选中的臂才会产生一个奖励（随机变量），这里便于分析，假设选不选中都有这么一个量</li></ul><li>奖励分布的均值： <img src="https://www.zhihu.com/equation?tex=%5Cmu_t%5Ek+%3D+%5Cmathbb%7BE%7D%5BX_t%5Ek%5D" alt="\mu_t^k = \mathbb{E}[X_t^k]" eeimg="1"/> </li><li>每一轮最优臂上的奖励分布均值： <img src="https://www.zhihu.com/equation?tex=%5Cmu_t%5E%2A+%3D+%5Cmax_%7Bk%5Cin%5Cmathcal%7BK%7D%7D%5C%7B%5Cmu_t%5Ek%5C%7D" alt="\mu_t^* = \max_{k\in\mathcal{K}}\{\mu_t^k\}" eeimg="1"/> </li><li>一个臂上整个游戏过程中的均值： <img src="https://www.zhihu.com/equation?tex=%5Cmu%5Ek+%3D+%5C%7B%5Cmu_t%5Ek%5C%7D_%7Bt%3D1%7D%5ET" alt="\mu^k = \{\mu_t^k\}_{t=1}^T" eeimg="1"/> </li><li>所有臂上整个游戏过程中的均值： <img src="https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5C%7B%5Cmu%5Ek%5C%7D_%7Bk%3D1%7D%5EK" alt="\mu = \{\mu^k\}_{k=1}^K" eeimg="1"/></li></ul><p><b><i>2.2 变换缓慢的奖励分布</i></b></p><p>下面规定奖励分布变化的不要太快（否则就退化为 adversary 设定了）。变化不要太快的标准是每一轮均值变化最大的那个臂的的变化量加起来不要超过一个规定好的 variation budget <img src="https://www.zhihu.com/equation?tex=V_T" alt="V_T" eeimg="1"/> ，即奖励均值应该属于以下集合</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-77670132796b3cbc91a45a700d464e22_b.png" data-caption="" data-size="normal" data-rawwidth="1578" data-rawheight="136" class="origin_image zh-lightbox-thumb" width="1578" data-original="https://pic3.zhimg.com/v2-77670132796b3cbc91a45a700d464e22_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1578&#39; height=&#39;136&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1578" data-rawheight="136" class="origin_image zh-lightbox-thumb lazy" width="1578" data-original="https://pic3.zhimg.com/v2-77670132796b3cbc91a45a700d464e22_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-77670132796b3cbc91a45a700d464e22_b.png"/></figure><p>同时，文章要求 variation budget 不能超过 <img src="https://www.zhihu.com/equation?tex=K%5E%7B-1%7D+T" alt="K^{-1} T" eeimg="1"/> ，即臂是数目越多，允许的变化量越小。</p><p>同时可以注意到，这样的 variation budget 只要求整个过程中变化不超过 <img src="https://www.zhihu.com/equation?tex=V_T" alt="V_T" eeimg="1"/> ，但是对于这个变化发生在整个过程中的什么地方没有限制，即以下两种变化形式，都对应同样的 <img src="https://www.zhihu.com/equation?tex=V_T+" alt="V_T " eeimg="1"/> 。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-16b6a314c110460540ad46a612ca75fb_b.jpg" data-caption="" data-size="normal" data-rawwidth="1714" data-rawheight="544" class="origin_image zh-lightbox-thumb" width="1714" data-original="https://pic4.zhimg.com/v2-16b6a314c110460540ad46a612ca75fb_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1714&#39; height=&#39;544&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1714" data-rawheight="544" class="origin_image zh-lightbox-thumb lazy" width="1714" data-original="https://pic4.zhimg.com/v2-16b6a314c110460540ad46a612ca75fb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-16b6a314c110460540ad46a612ca75fb_b.jpg"/></figure><p><b><i>2.3 策略空间</i></b></p><p>这里考虑的策略空间是一个 non-anticipating 的策略，即非预测的。大致可以认为策略只与过去的经历有关，但是可以是随机性的策略。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-bb784b37634b5059a22ebe7e8eac3b33_b.png" data-caption="" data-size="normal" data-rawwidth="1586" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="1586" data-original="https://pic4.zhimg.com/v2-bb784b37634b5059a22ebe7e8eac3b33_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1586" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="1586" data-original="https://pic4.zhimg.com/v2-bb784b37634b5059a22ebe7e8eac3b33_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-bb784b37634b5059a22ebe7e8eac3b33_b.png"/></figure><p><b><i>2.4 Regret</i></b></p><p>这里考虑的 regret 的基准是一个 dynamic oracle，即</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-27113834fcd152910f42936faf9d8082_b.png" data-caption="" data-size="normal" data-rawwidth="1582" data-rawheight="152" class="origin_image zh-lightbox-thumb" width="1582" data-original="https://pic3.zhimg.com/v2-27113834fcd152910f42936faf9d8082_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1582&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1582" data-rawheight="152" class="origin_image zh-lightbox-thumb lazy" width="1582" data-original="https://pic3.zhimg.com/v2-27113834fcd152910f42936faf9d8082_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-27113834fcd152910f42936faf9d8082_b.png"/></figure><p>需要注意</p><ul><li>这里的 regret 是均值和均值的比较。</li><li><img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5E%5Cpi" alt="\mathbb{E}^\pi" eeimg="1"/> 包含策略的随机性（即，策略每次可能按照一定概率分布随机选择一个臂），也包含环境的随机性。后面一点不太好理解，因为这里写的是均值，因此在单步拉臂上的随机性是没有了，但是 <img src="https://www.zhihu.com/equation?tex=X_t%5Ek" alt="X_t^k" eeimg="1"/> 的随机性能够影响策略，从而最后影响期望。</li><li>如果 regret 相对于 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 是 sub-linear 的，称相应的策略是 long-run average optimal 的。</li></ul><p>把所有策略能产生的最优 regret 记为</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-e2bfbf419050a56e482e0c61e27fb8cf_b.png" data-caption="" data-size="normal" data-rawwidth="1720" data-rawheight="56" class="origin_image zh-lightbox-thumb" width="1720" data-original="https://pic4.zhimg.com/v2-e2bfbf419050a56e482e0c61e27fb8cf_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1720&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1720" data-rawheight="56" class="origin_image zh-lightbox-thumb lazy" width="1720" data-original="https://pic4.zhimg.com/v2-e2bfbf419050a56e482e0c61e27fb8cf_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e2bfbf419050a56e482e0c61e27fb8cf_b.png"/></figure><h3>3. 算法性能下界</h3><p>在上述设定中，最好算法的性能也会有一个下界，这里构造了一个这样的下界。构造下界的方法是找出一个环境『对抗』算法的方式，证明对于任意的算法都能遭受到这种『对抗』方式多大的损失。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-44c12e2c951e4a81dcca90381e6001f1_b.png" data-caption="" data-size="normal" data-rawwidth="1638" data-rawheight="206" class="origin_image zh-lightbox-thumb" width="1638" data-original="https://pic2.zhimg.com/v2-44c12e2c951e4a81dcca90381e6001f1_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1638&#39; height=&#39;206&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1638" data-rawheight="206" class="origin_image zh-lightbox-thumb lazy" width="1638" data-original="https://pic2.zhimg.com/v2-44c12e2c951e4a81dcca90381e6001f1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-44c12e2c951e4a81dcca90381e6001f1_b.png"/></figure><p>先看结论本身，再讲构造方法。</p><ul><li>对于 stationary 的情况（stochastic setting），已知最优算法 UCB1 能够达到 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BT%7D" alt="\sqrt{T}" eeimg="1"/> 的 regret。本文设定下，最优就只有 <img src="https://www.zhihu.com/equation?tex=T%5E%7B2%2F3%7D" alt="T^{2/3}" eeimg="1"/> ，这是『price of non-stationary』，即 variation budget 如果是和 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 无关的常数，但是只要奖励分布会变，就会产生一个这样的代价。</li><li>当奖励会变时，我们可以只利用最近的样本去估计每个臂上的均值，这样相应的 bias 会比较小；也可以把比较远的样本也用上，这样 variance 会比较小。文章把它称作 remembering / forgetting tradeoff，这是和 non-stationary 关联的。</li><li>与之对应的是 MAB 本身的 tradeoff，即 exploration / exploitation tradeoff。</li></ul><p>构造方法如下：</p><p>首先把整个游戏过程分为 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"/> 个 batch，每个 batch为</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1efb0a31d76628c4f400a31693944323_b.png" data-caption="" data-size="normal" data-rawwidth="1586" data-rawheight="98" class="origin_image zh-lightbox-thumb" width="1586" data-original="https://pic4.zhimg.com/v2-1efb0a31d76628c4f400a31693944323_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1586" data-rawheight="98" class="origin_image zh-lightbox-thumb lazy" width="1586" data-original="https://pic4.zhimg.com/v2-1efb0a31d76628c4f400a31693944323_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1efb0a31d76628c4f400a31693944323_b.png"/></figure><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7B%5CDelta%7D_T" alt="\tilde{\Delta}_T" eeimg="1"/> 为每个 batch 的长度（最后一个 batch 不太一样）。每个 batch 中各个臂的奖励分布均值都不变，但是每个 batch 开始的时候随机选择一个最优的臂，其均值设定为 <img src="https://www.zhihu.com/equation?tex=1%2F2+%2B+%5Cepsilon" alt="1/2 + \epsilon" eeimg="1"/> ，其余的臂都设置为 <img src="https://www.zhihu.com/equation?tex=1%2F2" alt="1/2" eeimg="1"/> 。要满足前述假设，要求 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon+T+%2F+%5Ctilde%7B%5CDelta%7D_T+%5Cle+V_T" alt="\epsilon T / \tilde{\Delta}_T \le V_T" eeimg="1"/> 。同时，当 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon%5Capprox+1%2F%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D" alt="\epsilon\approx 1/\sqrt{\tilde{\Delta}_T}" eeimg="1"/> 的时候，由于臂之间的均值太过接近，因此没法有效分辨出最优的臂（之前工作的结论），因此每个 batch 会产生大约 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon+%5Ctilde%7B%5CDelta%7D_T%5Capprox%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D" alt="\epsilon \tilde{\Delta}_T\approx\sqrt{\tilde{\Delta}_T}" eeimg="1"/> 的 regret。把各个 batch 加起来，能产生一个 <img src="https://www.zhihu.com/equation?tex=T%2F+%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D" alt="T/ \sqrt{\tilde{\Delta}_T}" eeimg="1"/> 的 regret。环境通过选择一个合适的 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7B%5CDelta%7D_T" alt="\tilde{\Delta}_T" eeimg="1"/> 使得 regret 最大，即</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmax+T+%2F+%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D%2C+%5C+%5C+%5C+s.t.+%5C+%5Cepsilon+%5Cdfrac%7BT%7D%7B%5Ctilde%7B%5CDelta%7D_T%7D+%5Cle+V_T" alt="\max T / \sqrt{\tilde{\Delta}_T}, \ \ \ s.t. \ \epsilon \dfrac{T}{\tilde{\Delta}_T} \le V_T" eeimg="1"/> </p><p>观察到</p><p><img src="https://www.zhihu.com/equation?tex=%5Cepsilon+%5Cdfrac%7BT%7D%7B%5Ctilde%7B%5CDelta%7D_T%7D+%5Capprox+%5Cdfrac%7BT%7D%7B%28%5Ctilde%7B%5CDelta%7D_T%29%5E%7B3%2F2%7D%7D+%5Cle+V_T+%5CRightarrow+%5Ctilde%7B%5CDelta%7D_T+%5Cge+%28T%2FV_T%29%5E%7B2%2F3%7D+%5CRightarrow+%5Cmathcal%7BR%7D%3D%5Cdfrac%7BT%7D%7B%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D%7D+%3D+%28V_T%29%5E%7B1%2F3%7D+%28T%29%5E%7B2%2F3%7D" alt="\epsilon \dfrac{T}{\tilde{\Delta}_T} \approx \dfrac{T}{(\tilde{\Delta}_T)^{3/2}} \le V_T \Rightarrow \tilde{\Delta}_T \ge (T/V_T)^{2/3} \Rightarrow \mathcal{R}=\dfrac{T}{\sqrt{\tilde{\Delta}_T}} = (V_T)^{1/3} (T)^{2/3}" eeimg="1"/> </p><p>可以得到最后的结论。</p><h3>4. 算法</h3><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1e53da4b14f6151dbb4a136af36fea6f_b.jpg" data-caption="" data-size="normal" data-rawwidth="1650" data-rawheight="986" class="origin_image zh-lightbox-thumb" width="1650" data-original="https://pic4.zhimg.com/v2-1e53da4b14f6151dbb4a136af36fea6f_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1650&#39; height=&#39;986&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1650" data-rawheight="986" class="origin_image zh-lightbox-thumb lazy" width="1650" data-original="https://pic4.zhimg.com/v2-1e53da4b14f6151dbb4a136af36fea6f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1e53da4b14f6151dbb4a136af36fea6f_b.jpg"/></figure><p>算法把整个游戏过程分为了若干个 batch。在每个 batch 中使用已有的 Exp3 算法，来解决 exploration-exploitation tradeoff，达到 <img src="https://www.zhihu.com/equation?tex=%5Csqrt%7BT%7D" alt="\sqrt{T}" eeimg="1"/> 的 regret；通过调整 batch 的长度来解决 remembering-forgetting tradeoff，产生最后的结果。</p><h3>5. Near-optimal bound</h3><p>该算法能够达到如下 regret bound。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-30f515390d23c6c94883250682152fd7_b.jpg" data-caption="" data-size="normal" data-rawwidth="1652" data-rawheight="318" class="origin_image zh-lightbox-thumb" width="1652" data-original="https://pic4.zhimg.com/v2-30f515390d23c6c94883250682152fd7_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1652&#39; height=&#39;318&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1652" data-rawheight="318" class="origin_image zh-lightbox-thumb lazy" width="1652" data-original="https://pic4.zhimg.com/v2-30f515390d23c6c94883250682152fd7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-30f515390d23c6c94883250682152fd7_b.jpg"/></figure><p>证明思路当然是要利用到 Exp3 的结论，注意到 Exp3 比较的是 static oracle，文章期望分析的时候 dynamic oracle，因此，需要做一个拆分。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-7ddcd577cef5c0d593102555619f8211_b.png" data-caption="" data-size="normal" data-rawwidth="1600" data-rawheight="234" class="origin_image zh-lightbox-thumb" width="1600" data-original="https://pic2.zhimg.com/v2-7ddcd577cef5c0d593102555619f8211_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1600" data-rawheight="234" class="origin_image zh-lightbox-thumb lazy" width="1600" data-original="https://pic2.zhimg.com/v2-7ddcd577cef5c0d593102555619f8211_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-7ddcd577cef5c0d593102555619f8211_b.png"/></figure><p>其中前一项可以利用均值变化缓慢的假设来 bound。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_b.jpg" data-caption="" data-size="normal" data-rawwidth="1642" data-rawheight="1090" class="origin_image zh-lightbox-thumb" width="1642" data-original="https://pic1.zhimg.com/v2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1642&#39; height=&#39;1090&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1642" data-rawheight="1090" class="origin_image zh-lightbox-thumb lazy" width="1642" data-original="https://pic1.zhimg.com/v2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_b.jpg"/></figure><p>文章讲的很详细了，大家仔细看一下吧。比较关键的是 (6) 中的 (b)，反映了『某一轮中最优的 arm』相比于『一个 batch 中的 single best arm』之间的约束关系。</p><p>第二项可以直接利用已有算法的已有结论。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-927d9912fe416f6fe4032149ea19b149_b.png" data-caption="" data-size="normal" data-rawwidth="1586" data-rawheight="168" class="origin_image zh-lightbox-thumb" width="1586" data-original="https://pic2.zhimg.com/v2-927d9912fe416f6fe4032149ea19b149_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1586" data-rawheight="168" class="origin_image zh-lightbox-thumb lazy" width="1586" data-original="https://pic2.zhimg.com/v2-927d9912fe416f6fe4032149ea19b149_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-927d9912fe416f6fe4032149ea19b149_b.png"/></figure><p>最后，把不同的 batch 全部加和起来即可得到最后的结论。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-1e47570fb61f8c125428ea2c1b0ee76d_b.jpg" data-caption="" data-size="normal" data-rawwidth="1650" data-rawheight="1058" class="origin_image zh-lightbox-thumb" width="1650" data-original="https://pic2.zhimg.com/v2-1e47570fb61f8c125428ea2c1b0ee76d_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1650&#39; height=&#39;1058&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1650" data-rawheight="1058" class="origin_image zh-lightbox-thumb lazy" width="1650" data-original="https://pic2.zhimg.com/v2-1e47570fb61f8c125428ea2c1b0ee76d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-1e47570fb61f8c125428ea2c1b0ee76d_b.jpg"/></figure><p>这一部分都比较直接。</p><h3>6. 讨论</h3><p>以上算法在选定 <img src="https://www.zhihu.com/equation?tex=%5CDelta_T" alt="\Delta_T" eeimg="1"/> 的时候（即考虑 remembering / forgetting tradeoff 的时候）需要已知 variation budget <img src="https://www.zhihu.com/equation?tex=V_T" alt="V_T" eeimg="1"/> 。在很多情况下，这个 budget 无法提前知道，只能靠估计。估计可能有一个误差，比如估计的是 <img src="https://www.zhihu.com/equation?tex=%5Chat%7BV%7D_T%3DT%5E%5Calpha" alt="\hat{V}_T=T^\alpha" eeimg="1"/> 但是真实的是 <img src="https://www.zhihu.com/equation?tex=V_T+%3D+T%5E%7B%5Calpha+%2B+%5Cdelta%7D" alt="V_T = T^{\alpha + \delta}" eeimg="1"/> ，那么最后的 regret 为 <img src="https://www.zhihu.com/equation?tex=T%5E%7B2%2F3+%2B+%5Calpha+%2F+3+%2B+%5Cdelta%7D" alt="T^{2/3 + \alpha / 3 + \delta}" eeimg="1"/> 。</p></div></div><div class="ContentItem-time">发布于 2019-08-29</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 12 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 12</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="becb0933-c03b-45ef-a86d-351568d18e06" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="becb0933-c03b-45ef-a86d-351568d18e06">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"80252625":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":80252625,"title":"【强化学习 92】Non-stationary MAB","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F80252625","imageUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8a21aab080f18c994db59df50b4cd5e7_b.jpg","titleImage":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8a21aab080f18c994db59df50b4cd5e7_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77670132796b3cbc91a45a700d464e22_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"136\" data-watermark=\"\" data-original-src=\"\" data-watermark-src=\"\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77670132796b3cbc91a45a700d464e22_r.png\"\u002F\u003EMAB 问题（multi-armed-bandit problem）在 non-stationary reward 设定下的一种算法分析。关于 MAB 问题的具体定义，可以看 Sutton 的书（大概是第二章）。原文传送门\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBesbes, Omar, Yonatan Gur, and Assaf Zeevi. &#34;Stochastic multi-armed-bandit problem…\u003C\u002Fa\u003E","created":1567047566,"updated":1567047566,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":1862,"imageHeight":964,"content":"\u003Cp\u003EMAB 问题（multi-armed-bandit problem）在 non-stationary reward 设定下的一种算法分析。关于 MAB 问题的具体定义，可以看 Sutton 的书（大概是第二章）。\u003C\u002Fp\u003E\u003Ch2\u003E原文传送门\u003C\u002Fh2\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBesbes, Omar, Yonatan Gur, and Assaf Zeevi. &#34;Stochastic multi-armed-bandit problem with non-stationary rewards.&#34; Advances in neural information processing systems. 2014.\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E特色\u003C\u002Fh2\u003E\u003Cp\u003EMAB 问题一般有两种设定：一种是 stochastic reward，即每个臂上奖励是从一个稳定的分布中 i.i.d. 采样得到的，该问题的 regret 和一直拉任意的一个臂（或者可以认为拉最优的臂）；另一种是 adversary reward，即考虑每个臂上的奖励可以被任意选择（即可能是有一个对手跟你『作对』），该问题的 regret 也是和一直拉任意一个臂相比。\u003C\u002Fp\u003E\u003Cp\u003E本文研究的设定叫做 non-stationary reward，它可以被看做介于 stochastic 和 adversary 中间的一种状态，即各个臂上 reward 的分布可以变化，但是变化不太大。基于这种设定，其 regret 的基准可以选择的比 adversary setting 下更强一些，即可以和每一轮中最优的那个臂相比较。之前 regret 中基准为一直拉同一个臂，这种情形文章把它叫做 static oracle，现在期望比较的是每一轮中最优的那个臂，这种情形叫做 dynamic oracle。和 dynamic oracle 的比较更为困难。\u003C\u002Fp\u003E\u003Ch2\u003E过程\u003C\u002Fh2\u003E\u003Ch3\u003E1. 背景\u003C\u002Fh3\u003E\u003Cp\u003EMAB 问题最原始的设定就是 stochastic setting，之后大家把该问题推广到各种不同的设定下。这篇文章也讲了一下之前的一些关于 reward 变化的 setting。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E被拉过的臂上的奖励分布会产生变化；\u003C\u002Fli\u003E\u003Cli\u003E被拉过的臂上的奖励分布产生变化，并且该变化的过程已知；\u003C\u002Fli\u003E\u003Cli\u003E给定总的奖励分布变化的次数；\u003C\u002Fli\u003E\u003Cli\u003E给定最优臂的轮换次数；\u003C\u002Fli\u003E\u003Cli\u003EAdversary 设定。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003E2. 问题设定\u003C\u002Fh3\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2.1 基本的 MAB 问题设定\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E臂： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BK%7D%3D%5C%7B1%2C+%5Ccdots%2C+K%5C%7D\" alt=\"\\mathcal{K}=\\{1, \\cdots, K\\}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E玩的轮数： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D+%3D+%5C%7B1%2C+%5Ccdots%2C+T%5C%7D\" alt=\"\\mathcal{T} = \\{1, \\cdots, T\\}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t%5Cin%5Cmathcal%7BT%7D\" alt=\"t\\in\\mathcal{T}\" eeimg=\"1\"\u002F\u003E 的时候选取 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k%5Cin%5Cmathcal%7BK%7D\" alt=\"k\\in\\mathcal{K}\" eeimg=\"1\"\u002F\u003E ，得到一个奖励 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X_t%5Ek%5Cin%5B0%2C1%5D\" alt=\"X_t^k\\in[0,1]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cul\u003E\u003Cli\u003E注意到这里把奖励归一化了\u003C\u002Fli\u003E\u003Cli\u003E按理来说，只有被选中的臂才会产生一个奖励（随机变量），这里便于分析，假设选不选中都有这么一个量\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli\u003E奖励分布的均值： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_t%5Ek+%3D+%5Cmathbb%7BE%7D%5BX_t%5Ek%5D\" alt=\"\\mu_t^k = \\mathbb{E}[X_t^k]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E每一轮最优臂上的奖励分布均值： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu_t%5E%2A+%3D+%5Cmax_%7Bk%5Cin%5Cmathcal%7BK%7D%7D%5C%7B%5Cmu_t%5Ek%5C%7D\" alt=\"\\mu_t^* = \\max_{k\\in\\mathcal{K}}\\{\\mu_t^k\\}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E一个臂上整个游戏过程中的均值： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu%5Ek+%3D+%5C%7B%5Cmu_t%5Ek%5C%7D_%7Bt%3D1%7D%5ET\" alt=\"\\mu^k = \\{\\mu_t^k\\}_{t=1}^T\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003Cli\u003E所有臂上整个游戏过程中的均值： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu+%3D+%5C%7B%5Cmu%5Ek%5C%7D_%7Bk%3D1%7D%5EK\" alt=\"\\mu = \\{\\mu^k\\}_{k=1}^K\" eeimg=\"1\"\u002F\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2.2 变换缓慢的奖励分布\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E下面规定奖励分布变化的不要太快（否则就退化为 adversary 设定了）。变化不要太快的标准是每一轮均值变化最大的那个臂的的变化量加起来不要超过一个规定好的 variation budget \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T\" alt=\"V_T\" eeimg=\"1\"\u002F\u003E ，即奖励均值应该属于以下集合\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77670132796b3cbc91a45a700d464e22_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb\" width=\"1578\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77670132796b3cbc91a45a700d464e22_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1578&#39; height=&#39;136&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1578\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77670132796b3cbc91a45a700d464e22_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77670132796b3cbc91a45a700d464e22_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E同时，文章要求 variation budget 不能超过 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K%5E%7B-1%7D+T\" alt=\"K^{-1} T\" eeimg=\"1\"\u002F\u003E ，即臂是数目越多，允许的变化量越小。\u003C\u002Fp\u003E\u003Cp\u003E同时可以注意到，这样的 variation budget 只要求整个过程中变化不超过 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T\" alt=\"V_T\" eeimg=\"1\"\u002F\u003E ，但是对于这个变化发生在整个过程中的什么地方没有限制，即以下两种变化形式，都对应同样的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T+\" alt=\"V_T \" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16b6a314c110460540ad46a612ca75fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1714\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb\" width=\"1714\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16b6a314c110460540ad46a612ca75fb_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1714&#39; height=&#39;544&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1714\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1714\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16b6a314c110460540ad46a612ca75fb_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16b6a314c110460540ad46a612ca75fb_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2.3 策略空间\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这里考虑的策略空间是一个 non-anticipating 的策略，即非预测的。大致可以认为策略只与过去的经历有关，但是可以是随机性的策略。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb784b37634b5059a22ebe7e8eac3b33_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb784b37634b5059a22ebe7e8eac3b33_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1586&#39; height=&#39;106&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb784b37634b5059a22ebe7e8eac3b33_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb784b37634b5059a22ebe7e8eac3b33_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E2.4 Regret\u003C\u002Fi\u003E\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E这里考虑的 regret 的基准是一个 dynamic oracle，即\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-27113834fcd152910f42936faf9d8082_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-27113834fcd152910f42936faf9d8082_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1582&#39; height=&#39;152&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1582\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-27113834fcd152910f42936faf9d8082_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-27113834fcd152910f42936faf9d8082_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E需要注意\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E这里的 regret 是均值和均值的比较。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BE%7D%5E%5Cpi\" alt=\"\\mathbb{E}^\\pi\" eeimg=\"1\"\u002F\u003E 包含策略的随机性（即，策略每次可能按照一定概率分布随机选择一个臂），也包含环境的随机性。后面一点不太好理解，因为这里写的是均值，因此在单步拉臂上的随机性是没有了，但是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X_t%5Ek\" alt=\"X_t^k\" eeimg=\"1\"\u002F\u003E 的随机性能够影响策略，从而最后影响期望。\u003C\u002Fli\u003E\u003Cli\u003E如果 regret 相对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 是 sub-linear 的，称相应的策略是 long-run average optimal 的。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E把所有策略能产生的最优 regret 记为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2bfbf419050a56e482e0c61e27fb8cf_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1720\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb\" width=\"1720\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2bfbf419050a56e482e0c61e27fb8cf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1720&#39; height=&#39;56&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1720\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1720\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2bfbf419050a56e482e0c61e27fb8cf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e2bfbf419050a56e482e0c61e27fb8cf_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E3. 算法性能下界\u003C\u002Fh3\u003E\u003Cp\u003E在上述设定中，最好算法的性能也会有一个下界，这里构造了一个这样的下界。构造下界的方法是找出一个环境『对抗』算法的方式，证明对于任意的算法都能遭受到这种『对抗』方式多大的损失。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44c12e2c951e4a81dcca90381e6001f1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb\" width=\"1638\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44c12e2c951e4a81dcca90381e6001f1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1638&#39; height=&#39;206&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1638\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44c12e2c951e4a81dcca90381e6001f1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-44c12e2c951e4a81dcca90381e6001f1_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E先看结论本身，再讲构造方法。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E对于 stationary 的情况（stochastic setting），已知最优算法 UCB1 能够达到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csqrt%7BT%7D\" alt=\"\\sqrt{T}\" eeimg=\"1\"\u002F\u003E 的 regret。本文设定下，最优就只有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T%5E%7B2%2F3%7D\" alt=\"T^{2\u002F3}\" eeimg=\"1\"\u002F\u003E ，这是『price of non-stationary』，即 variation budget 如果是和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 无关的常数，但是只要奖励分布会变，就会产生一个这样的代价。\u003C\u002Fli\u003E\u003Cli\u003E当奖励会变时，我们可以只利用最近的样本去估计每个臂上的均值，这样相应的 bias 会比较小；也可以把比较远的样本也用上，这样 variance 会比较小。文章把它称作 remembering \u002F forgetting tradeoff，这是和 non-stationary 关联的。\u003C\u002Fli\u003E\u003Cli\u003E与之对应的是 MAB 本身的 tradeoff，即 exploration \u002F exploitation tradeoff。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E构造方法如下：\u003C\u002Fp\u003E\u003Cp\u003E首先把整个游戏过程分为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m\" alt=\"m\" eeimg=\"1\"\u002F\u003E 个 batch，每个 batch为\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1efb0a31d76628c4f400a31693944323_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1efb0a31d76628c4f400a31693944323_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1586&#39; height=&#39;98&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1efb0a31d76628c4f400a31693944323_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1efb0a31d76628c4f400a31693944323_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7B%5CDelta%7D_T\" alt=\"\\tilde{\\Delta}_T\" eeimg=\"1\"\u002F\u003E 为每个 batch 的长度（最后一个 batch 不太一样）。每个 batch 中各个臂的奖励分布均值都不变，但是每个 batch 开始的时候随机选择一个最优的臂，其均值设定为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1%2F2+%2B+%5Cepsilon\" alt=\"1\u002F2 + \\epsilon\" eeimg=\"1\"\u002F\u003E ，其余的臂都设置为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1%2F2\" alt=\"1\u002F2\" eeimg=\"1\"\u002F\u003E 。要满足前述假设，要求 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+T+%2F+%5Ctilde%7B%5CDelta%7D_T+%5Cle+V_T\" alt=\"\\epsilon T \u002F \\tilde{\\Delta}_T \\le V_T\" eeimg=\"1\"\u002F\u003E 。同时，当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon%5Capprox+1%2F%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D\" alt=\"\\epsilon\\approx 1\u002F\\sqrt{\\tilde{\\Delta}_T}\" eeimg=\"1\"\u002F\u003E 的时候，由于臂之间的均值太过接近，因此没法有效分辨出最优的臂（之前工作的结论），因此每个 batch 会产生大约 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+%5Ctilde%7B%5CDelta%7D_T%5Capprox%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D\" alt=\"\\epsilon \\tilde{\\Delta}_T\\approx\\sqrt{\\tilde{\\Delta}_T}\" eeimg=\"1\"\u002F\u003E 的 regret。把各个 batch 加起来，能产生一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T%2F+%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D\" alt=\"T\u002F \\sqrt{\\tilde{\\Delta}_T}\" eeimg=\"1\"\u002F\u003E 的 regret。环境通过选择一个合适的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7B%5CDelta%7D_T\" alt=\"\\tilde{\\Delta}_T\" eeimg=\"1\"\u002F\u003E 使得 regret 最大，即\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax+T+%2F+%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D%2C+%5C+%5C+%5C+s.t.+%5C+%5Cepsilon+%5Cdfrac%7BT%7D%7B%5Ctilde%7B%5CDelta%7D_T%7D+%5Cle+V_T\" alt=\"\\max T \u002F \\sqrt{\\tilde{\\Delta}_T}, \\ \\ \\ s.t. \\ \\epsilon \\dfrac{T}{\\tilde{\\Delta}_T} \\le V_T\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E观察到\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+%5Cdfrac%7BT%7D%7B%5Ctilde%7B%5CDelta%7D_T%7D+%5Capprox+%5Cdfrac%7BT%7D%7B%28%5Ctilde%7B%5CDelta%7D_T%29%5E%7B3%2F2%7D%7D+%5Cle+V_T+%5CRightarrow+%5Ctilde%7B%5CDelta%7D_T+%5Cge+%28T%2FV_T%29%5E%7B2%2F3%7D+%5CRightarrow+%5Cmathcal%7BR%7D%3D%5Cdfrac%7BT%7D%7B%5Csqrt%7B%5Ctilde%7B%5CDelta%7D_T%7D%7D+%3D+%28V_T%29%5E%7B1%2F3%7D+%28T%29%5E%7B2%2F3%7D\" alt=\"\\epsilon \\dfrac{T}{\\tilde{\\Delta}_T} \\approx \\dfrac{T}{(\\tilde{\\Delta}_T)^{3\u002F2}} \\le V_T \\Rightarrow \\tilde{\\Delta}_T \\ge (T\u002FV_T)^{2\u002F3} \\Rightarrow \\mathcal{R}=\\dfrac{T}{\\sqrt{\\tilde{\\Delta}_T}} = (V_T)^{1\u002F3} (T)^{2\u002F3}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E可以得到最后的结论。\u003C\u002Fp\u003E\u003Ch3\u003E4. 算法\u003C\u002Fh3\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e53da4b14f6151dbb4a136af36fea6f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"986\" class=\"origin_image zh-lightbox-thumb\" width=\"1650\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e53da4b14f6151dbb4a136af36fea6f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1650&#39; height=&#39;986&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"986\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1650\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e53da4b14f6151dbb4a136af36fea6f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e53da4b14f6151dbb4a136af36fea6f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E算法把整个游戏过程分为了若干个 batch。在每个 batch 中使用已有的 Exp3 算法，来解决 exploration-exploitation tradeoff，达到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csqrt%7BT%7D\" alt=\"\\sqrt{T}\" eeimg=\"1\"\u002F\u003E 的 regret；通过调整 batch 的长度来解决 remembering-forgetting tradeoff，产生最后的结果。\u003C\u002Fp\u003E\u003Ch3\u003E5. Near-optimal bound\u003C\u002Fh3\u003E\u003Cp\u003E该算法能够达到如下 regret bound。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30f515390d23c6c94883250682152fd7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1652\" data-rawheight=\"318\" class=\"origin_image zh-lightbox-thumb\" width=\"1652\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30f515390d23c6c94883250682152fd7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1652&#39; height=&#39;318&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1652\" data-rawheight=\"318\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1652\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30f515390d23c6c94883250682152fd7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-30f515390d23c6c94883250682152fd7_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E证明思路当然是要利用到 Exp3 的结论，注意到 Exp3 比较的是 static oracle，文章期望分析的时候 dynamic oracle，因此，需要做一个拆分。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7ddcd577cef5c0d593102555619f8211_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7ddcd577cef5c0d593102555619f8211_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1600&#39; height=&#39;234&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7ddcd577cef5c0d593102555619f8211_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7ddcd577cef5c0d593102555619f8211_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E其中前一项可以利用均值变化缓慢的假设来 bound。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1642\" data-rawheight=\"1090\" class=\"origin_image zh-lightbox-thumb\" width=\"1642\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1642&#39; height=&#39;1090&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1642\" data-rawheight=\"1090\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1642\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f6ee6ce2fc502dc4b42b1b8b8a27ea3c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E文章讲的很详细了，大家仔细看一下吧。比较关键的是 (6) 中的 (b)，反映了『某一轮中最优的 arm』相比于『一个 batch 中的 single best arm』之间的约束关系。\u003C\u002Fp\u003E\u003Cp\u003E第二项可以直接利用已有算法的已有结论。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-927d9912fe416f6fe4032149ea19b149_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-927d9912fe416f6fe4032149ea19b149_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1586&#39; height=&#39;168&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-927d9912fe416f6fe4032149ea19b149_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-927d9912fe416f6fe4032149ea19b149_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E最后，把不同的 batch 全部加和起来即可得到最后的结论。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1e47570fb61f8c125428ea2c1b0ee76d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"1058\" class=\"origin_image zh-lightbox-thumb\" width=\"1650\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1e47570fb61f8c125428ea2c1b0ee76d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1650&#39; height=&#39;1058&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"1058\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1650\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1e47570fb61f8c125428ea2c1b0ee76d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-1e47570fb61f8c125428ea2c1b0ee76d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这一部分都比较直接。\u003C\u002Fp\u003E\u003Ch3\u003E6. 讨论\u003C\u002Fh3\u003E\u003Cp\u003E以上算法在选定 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta_T\" alt=\"\\Delta_T\" eeimg=\"1\"\u002F\u003E 的时候（即考虑 remembering \u002F forgetting tradeoff 的时候）需要已知 variation budget \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T\" alt=\"V_T\" eeimg=\"1\"\u002F\u003E 。在很多情况下，这个 budget 无法提前知道，只能靠估计。估计可能有一个误差，比如估计的是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BV%7D_T%3DT%5E%5Calpha\" alt=\"\\hat{V}_T=T^\\alpha\" eeimg=\"1\"\u002F\u003E 但是真实的是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_T+%3D+T%5E%7B%5Calpha+%2B+%5Cdelta%7D\" alt=\"V_T = T^{\\alpha + \\delta}\" eeimg=\"1\"\u002F\u003E ，那么最后的 regret 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T%5E%7B2%2F3+%2B+%5Calpha+%2F+3+%2B+%5Cdelta%7D\" alt=\"T^{2\u002F3 + \\alpha \u002F 3 + \\delta}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":12,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":0,"contributions":[{"id":21625989,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习 92】Non-stationary MAB - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F80252625 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_bullet_gui-3","expPrefix":"vd_bullet_gui","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-2","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"没弹幕？你是第一个"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F80252625","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F80252625","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>