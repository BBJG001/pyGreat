<!doctype html>
<html lang="zh" data-hairline="true" data-theme="light"><head><meta charSet="utf-8"/><title data-react-helmet="true">【强化学习入门 1】从零开始认识强化学习 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"/><meta name="renderer" content="webkit"/><meta name="force-rendering" content="webkit"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"/><meta data-react-helmet="true" name="keywords" content="强化学习 (Reinforcement Learning)"/><meta data-react-helmet="true" name="description" content="目录强化学习基本概念马可夫决策模型价值函数Bellman算子Value Iteration和Policy Iteration动态规划方法蒙特卡洛方法引言强化学习学习的是一个策略，目前主要有三大类学习的框架，它们分别是策略迭代方法（policy…"/><meta data-react-helmet="true" property="og:title" content="【强化学习入门 1】从零开始认识强化学习"/><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/56045177"/><meta data-react-helmet="true" property="og:description" content="目录强化学习基本概念马可夫决策模型价值函数Bellman算子Value Iteration和Policy Iteration动态规划方法蒙特卡洛方法引言强化学习学习的是一个策略，目前主要有三大类学习的框架，它们分别是策略迭代方法（policy…"/><meta data-react-helmet="true" property="og:image" content=""/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"/><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"/><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"/><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"/><link rel="dns-prefetch" href="//static.zhimg.com"/><link rel="dns-prefetch" href="//pic1.zhimg.com"/><link rel="dns-prefetch" href="//pic2.zhimg.com"/><link rel="dns-prefetch" href="//pic3.zhimg.com"/><link rel="dns-prefetch" href="//pic4.zhimg.com"/><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="https://static.zhihu.com/heifetz/column.app.a9ced452148fee3a55c8.css" rel="stylesheet"/><script defer="" crossorigin="anonymous" src="https://unpkg.zhimg.com/@cfe/sentry-script@latest/dist/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;683-127b14ad&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#x27;t find variable: webkit&quot;,&quot;Can&#x27;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script></head><body class="WhiteBg-body"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;张楚珩&quot;,&quot;itemId&quot;:56045177,&quot;title&quot;:&quot;【强化学习入门 1】从零开始认识强化学习&quot;,&quot;type&quot;:&quot;article&quot;}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 200 91" fill="#0084FF" width="64" height="30"><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="/reinforcementlearning"><img class="Avatar Avatar--round" width="30" height="30" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_is.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_im.jpg 2x" alt="强化学习前沿"/></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="/reinforcementlearning">强化学习前沿</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">【强化学习入门 1】从零开始认识强化学习</h1><div class="Post-Author"><div class="AuthorInfo" itemProp="author" itemscope="" itemType="http://schema.org/Person"><meta itemProp="name" content="张楚珩"/><meta itemProp="image" content="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg"/><meta itemProp="url" content="https://www.zhihu.com/people/zhang-chu-heng"/><meta itemProp="zhihu:followerCount"/><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_xs.jpg" srcSet="https://pic4.zhimg.com/v2-2b7c11f27a7fd03282819889435815aa_l.jpg 2x" alt="张楚珩"/></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="//www.zhihu.com/people/zhang-chu-heng">张楚珩</a></div></div><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1pc1mic">.css-1pc1mic{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;width:18px;height:18px;margin-left:.3em;}</style><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-1pc1mic" data-tooltip="已认证的个人" aria-label="已认证的个人"><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><span class="css-18biwo">​<svg viewBox="0 0 24 24" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0084FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">清华大学 交叉信息院博士在读</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">50 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><h2><b>目录</b></h2><ul><li>强化学习基本概念</li><li>马可夫决策模型</li><li>价值函数</li><li>Bellman算子</li><li>Value Iteration和Policy Iteration</li><li>动态规划方法</li><li>蒙特卡洛方法</li></ul><h2><b>引言</b></h2><p>强化学习学习的是一个策略，目前主要有三大类学习的框架，它们分别是<b><i>策略迭代方法</i></b>（policy iteration method）、<b><i>策略梯度方法</i></b>（policy gradient method）和<b><i>无导数优化方法</i></b>（derivative-free optimization method）。我们在这一讲里面主要介绍强化学习的基本概念，在接下来的几讲中，我们将分别介绍这几大类方法以及其中具有代表性的算法。</p><h2><b>强化学习基本概念</b></h2><p>强化学习是机器学习领域的一类学习问题，它与常见的有监督学习、无监督学习等的最大不同之处在于，它是通过与环境之间的交互和反馈来学习的。正如一个新生的婴儿一样，它通过哭闹、吮吸、爬走等来对环境进行探索，并且慢慢地积累起对于环境的感知，从而一步步学习到环境的特性使得自己的行动能够尽快达成自己的愿望。再比如，这也同我们学习下围棋的模式类似，我们通过和不同的对手一盘一盘得对弈，慢慢积累起来我们对于每一步落子的判断，从而慢慢地提高自身的围棋水平。由DeepMind研发的AlphaGo围棋程序在训练学习的过程中就用到了强化学习的技术。</p><p>下面让我们来正式地定义一下强化学习问题。强化学习的基本模型就是个体-环境的交互。<b><i>个体/智能体</i></b>（agent）就是能够采取一系列行动并且期望获得较高收益或者达到某一目标的部分，比如我们前面例子中的新生婴儿或者在学习下围棋的玩家。而与此相关的另外的部分我们都统一称作<b><i>环境</i></b>（environment），比如前面例子中的婴儿的环境（比如包括其周围的房间以及婴儿的父母等）或者是你面前的棋盘以及对手。整个过程将其离散化为不同的时刻（time step）。在每个时刻环境和个体都会产生相应的交互。个体可以采取一定的<b><i>行动</i></b>（action），这样的行动是施加在环境中的。环境在接受到个体的行动之后，会反馈给个体环境目前的<b><i>状态</i></b>（state）以及由于上一个行动而产生的<b><i>奖励</i></b>（reward）。其中值得注意的一点是，这样个体-环境的划分并不一定是按照实体的临近关系划分，比如在动物行为学上，动物获得的奖励其实可能来自于其自身大脑中的化学物质的分泌，因此这时动物大脑中实现这一奖励机制的部分，也应该被划分为环境；而个体就仅仅只包括接受信号并且做出决定的部分。</p><p>上面所描述的个体-环境相互作用可以使用图一中的示意图表示。存在一连串的时刻 <img src="https://www.zhihu.com/equation?tex=t%3D1%2C+2%2C+3+%5Ccdots" alt="t=1, 2, 3 \cdots" eeimg="1"/> ，在每一个时刻中，个体都会接受到环境的一个状态信号 <img src="https://www.zhihu.com/equation?tex=S_t+%5Cin+%5Cmathcal%7BS%7D" alt="S_t \in \mathcal{S}" eeimg="1"/> ，在每一步中个体会从该状态允许的行动集中挑选一个来采取行动 <img src="https://www.zhihu.com/equation?tex=A_t+%5Cin+%5Cmathcal%7BA%7D%28S_t%29" alt="A_t \in \mathcal{A}(S_t)" eeimg="1"/> ，环境接受到这个行动信号之后，在下一个时刻环境会反馈给个体相应的状态信号 <img src="https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D+%5Cin+%5Cmathcal%7BS%7D%5E%2B" alt="S_{t+1} \in \mathcal{S}^+" eeimg="1"/> 和即时奖励 <img src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D%5Cin+%5Cmathcal%7BR%7D+%5Csubset+%5Cmathbb%7BR%7D" alt="R_{t+1}\in \mathcal{R} \subset \mathbb{R}" eeimg="1"/> 。其中，我们把所有的状态记做 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D%5E%2B" alt="\mathcal{S}^+" eeimg="1"/> ，把非终止状态记做 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D" alt="\mathcal{S}" eeimg="1"/> 。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-190bba94ea13917353464deb961b8b67_b.jpg" data-size="normal" data-rawwidth="3299" data-rawheight="899" class="origin_image zh-lightbox-thumb" width="3299" data-original="https://pic4.zhimg.com/v2-190bba94ea13917353464deb961b8b67_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3299&#39; height=&#39;899&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="3299" data-rawheight="899" class="origin_image zh-lightbox-thumb lazy" width="3299" data-original="https://pic4.zhimg.com/v2-190bba94ea13917353464deb961b8b67_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-190bba94ea13917353464deb961b8b67_b.jpg"/><figcaption>图一：个体环境相互作用</figcaption></figure><p>强化学习的目标是希望个体从环境中获得的总奖励最大，即我们的目标不是短期的某一步行动之后获得最大的奖励，而是希望长期地获得更多的奖励。比如一个婴儿可能短期的偷吃了零食，获得了身体上的愉悦（即，获取了较大的短期奖励），但是这一行为可能再某一段时间之后会导致父母的批评，从而降低了长期来的总奖励。在很多常见的任务中，比如下围棋，在一局棋未结束的时候奖励常常都是为零的，而仅当棋局结束的那一个时刻才会根据个体的输赢产生一个奖励值；而在另外一些任务中，环境给予奖励可能分布在几乎每个时刻中。对于像下围棋这样存在一个终止状态，并且所有的奖励会在这个终止状态及其之前结算清的任务我们称之为<b><i>回合制任务</i></b>（episodic task）；还存在另外的一类任务，它们并不存在一个终止状态，即原则上它们可以永久地运行下去，这类任务的奖励是分散地分布在这个连续的一连串的时刻中的，我们称这一类任务为<b><i>连续任务</i></b>（continuing task）。由于我们的目标是希望获得的总奖励最大，因此我们希望量化地定义这个总奖励，这里我们称之为<b><i>收益</i></b>（return）。对于回合制任务而言，我们可以很直接定义收益为</p><p><img src="https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+R_%7Bt%2B2%7D+%2B+%5Ccdots+%2B+R_T" alt="G_t = R_{t+1} + R_{t+2} + \cdots + R_T" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 为回合结束的时刻，即 <img src="https://www.zhihu.com/equation?tex=S_T" alt="S_T" eeimg="1"/> 属于终止状态。对于连续任务而言，不存在一个这样的终止状态，因此，这样的定义可能会在连续任务中发散。因此我们引入另外一种收益的计算方式，称之为衰减收益（discounted return）。</p><p><img src="https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%5Ccdots+%3D+%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D+%5Cgamma%5Ek+R_%7Bt%2Bk%2B1%7D" alt="G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}" eeimg="1"/> </p><p>其中衰减率（discount factor） <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"/> 满足 <img src="https://www.zhihu.com/equation?tex=0+%5Cle+%5Cgamma+%5Cle+1" alt="0 \le \gamma \le 1" eeimg="1"/> 。这样的定义也很好理解，相比于更远的收益，我们会更加偏好临近的收益，因此对于离得较近的收益权重更高。</p><p>强化学习的结果是就是这样一个行动决策，我们称<b><i>策略</i></b>（policy）。策略给出了在每个状态下我们应该采取的行动，我们可以把这个策略记做  <img src="https://www.zhihu.com/equation?tex=%5Cpi+%28a+%7C+s%29" alt="\pi (a | s)" eeimg="1"/>  ，它表示在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 下采取行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 的概率。</p><p>如果我们可以计算出每个状态或者采取某个行动之后收益，那么我们每次行动就只需要采取收益较大的行动或者采取能够到达收益较大状态的行动。这也是策略迭代方法中的一个主要思路。在策略迭代方法中，通过对于和环境的交互，迭代地估计出到达某个状态或者采取某个行动对应的收益，同时更新策略与此匹配；通过多次迭代，当期望收益估计的越来越准的时候，策略也慢慢变好。那么自然就产生一些问题，通过迭代，是否能把对应的期望收益估计准确？当期望收益估计准确的时候相应的策略是否为最优？这些问题将在下面引入更为正式的表述之后给出解答。</p><h2><b>马可夫决策过程</b></h2><p>我们从大家熟悉的马可夫链说起，对于一个离散有限的状态空间 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D%5E%2B" alt="\mathcal{S}^+" eeimg="1"/> 中的每一个状态 <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> ，它转移到其他状态的概率只取决于该状态本身，即所说的马可夫性，即</p><p><img src="https://www.zhihu.com/equation?tex=Pr%5BS_%7Bt%2B1%7D+%7C+S_0%2C+S_1%2C%5Ccdots+%2C+S_t%5D+%3D+Pr%5BS_%7Bt%2B1%7D+%7C+S_t%5D" alt="Pr[S_{t+1} | S_0, S_1,\cdots , S_t] = Pr[S_{t+1} | S_t]" eeimg="1"/> </p><p><b><i>马可夫决策过程</i></b>（Markov decision process, MDP）与马可夫链的区别就在于，在每个状态下，可以由个体从可能的行动空间 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BA%7D%28s%29" alt="\mathcal{A}(s)" eeimg="1"/> 中选择一个行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> ，紧接着的状态转移概率随着所选择行动的不同而不同。另外，我们再加入即时奖励，就可以得到MDP的动力学特征 <img src="https://www.zhihu.com/equation?tex=p%28s%27%2C+r+%7C+s%2C+a%29+%3D+Pr%5BS_%7Bt%2B1%7D+%3D+s%27%2C+R_%7Bt%2B1%7D+%3D+r+%7C+S_t%3Ds%2C+A_t%3Da%5D+%3D+Pr%5BS_%7Bt%2B1%7D+%3D+s%27%2C+R_%7Bt%2B1%7D+%3D+r+%7C+S_0%2C+A_0%2C+S_1%2C+A_1%2C+%5Ccdots%2C+S_t%2C+A_t%5C%7D" alt="p(s&#39;, r | s, a) = Pr[S_{t+1} = s&#39;, R_{t+1} = r | S_t=s, A_t=a] = Pr[S_{t+1} = s&#39;, R_{t+1} = r | S_0, A_0, S_1, A_1, \cdots, S_t, A_t\}" eeimg="1"/> </p><p>我们注意到，马可夫性使得我们可以仅仅利用当前状态来估计接下来的收益，即仅仅使用当前状态来估计的策略并不比使用所有历史的策略差。可以说马可夫性带给我们了极大的计算上的便利，我们不用每一步都去处理所有的历史步骤，而是只需要面对当前的状态来进行处理。同时注意到，可能有些信息并没有被完整的包含到模型的状态信号 <img src="https://www.zhihu.com/equation?tex=S_t" alt="S_t" eeimg="1"/> 中，这使得模型并不满足马可夫性。不过我们一般还是认为它们是<b><i>近似地</i></b>满足马可夫性的，因此仅仅使用当前状态的信息来做出对于未来收益的预测和行动的选择并不是很差的策略。同时，如果选取不仅仅是当前时刻的状态状态，而是之前的多个时刻的状态叠加在一起作为当前状态，一般可以增强马可夫性。</p><p>关于MDP更为形式化的定义是一个五元组 <img src="https://www.zhihu.com/equation?tex=%28%5Cmathcal%7BS%7D%2C+%5Cmathcal%7BA%7D%2C+P%2C+R%2C+%5Cgamma%29" alt="(\mathcal{S}, \mathcal{A}, P, R, \gamma)" eeimg="1"/> ，而提到的MDP动力学特征 <img src="https://www.zhihu.com/equation?tex=p%28s%27%2C+r+%7C+s%2C+a%29" alt="p(s&#39;, r | s, a)" eeimg="1"/> 反映了这个MDP问题中的所有环境因素。</p><ul><li><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D" alt="\mathcal{S}" eeimg="1"/> 表示状态空间，在<b><i>有限状态的马可夫决策过程</i></b>（finite MDP）中，该集合元素格式是有限的；</li><li><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BA%7D%28S%29" alt="\mathcal{A}(S)" eeimg="1"/> 表示处于状态 <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> 下时，可能的行动空间，在有限状态的马可夫决策过程中，每一个集合元素个数都是有限的；</li><li><img src="https://www.zhihu.com/equation?tex=P%28s+%27%7C+s%2C+a%29" alt="P(s &#39;| s, a)" eeimg="1"/> 表示在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 下，采取行动 <img src="https://www.zhihu.com/equation?tex=a%5Cin+%5Cmathcal%7BA%7D%28s%29" alt="a\in \mathcal{A}(s)" eeimg="1"/> 时跳转到状态 <img src="https://www.zhihu.com/equation?tex=s%27" alt="s&#39;" eeimg="1"/> 的概率，有关系$ <img src="https://www.zhihu.com/equation?tex=P%28s%27%7Cs%2C+a%29+%3D+Pr%5BS_%7Bt%2B1%7D%3Ds%27+%7C+S_t+%3D+s%2C+A_t+%3D+a%5D+%3D+%5Csum_%7Br%5Cin%5Cmathbb%7BR%7D%7D+p%28s%27%2C+r+%7C+s%2C+a%29" alt="P(s&#39;|s, a) = Pr[S_{t+1}=s&#39; | S_t = s, A_t = a] = \sum_{r\in\mathbb{R}} p(s&#39;, r | s, a)" eeimg="1"/> ；</li><li><img src="https://www.zhihu.com/equation?tex=R%28s%27%2C+s%2C+a%29" alt="R(s&#39;, s, a)" eeimg="1"/> 表示在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 下，采取行动 <img src="https://www.zhihu.com/equation?tex=a%5Cin+%5Cmathcal%7BA%7D%28s%29" alt="a\in \mathcal{A}(s)" eeimg="1"/> 并跳转到状态 <img src="https://www.zhihu.com/equation?tex=s%27" alt="s&#39;" eeimg="1"/> 时取得的即时奖励，有关系 <img src="https://www.zhihu.com/equation?tex=R%28s%27%2C+s%2C+a%29+%3D+%5Cmathbb%7BE%7D%5BR_%7Bt%2B1%7D+%7C+S_t+%3D+s%2C+A_t+%3D+a%2C+S_%7Bt%2B1%7D%3Ds%27%5D+%3D+%5Cdfrac%7B%5Csum_%7Br%5Cin%5Cmathbb%7BR%7D%7D+r+%5C%2C+p%28s%27%2C+r+%7Cs%2C+a%29%7D%7Bp%28s%27+%7Cs%2C+a%29%7D" alt="R(s&#39;, s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a, S_{t+1}=s&#39;] = \dfrac{\sum_{r\in\mathbb{R}} r \, p(s&#39;, r |s, a)}{p(s&#39; |s, a)}" eeimg="1"/> ；类似地，我们还可以得到在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 下，采取行动 <img src="https://www.zhihu.com/equation?tex=a%5Cin+%5Cmathcal%7BA%7D%28s%29" alt="a\in \mathcal{A}(s)" eeimg="1"/> 后的期望即时奖励， <img src="https://www.zhihu.com/equation?tex=R%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D%5BR_%7Bt%2B1%7D+%7C+S_t+%3D+s%2C+A_t+%3D+a%5D+%3D+%5Csum_%7Br%5Cin%5Cmathbb%7BR%7D%7D+r+%5Csum_%7Bs%27%5Cin%5Cmathcal%7BS%7D%7D+p%28s%27%2C+r+%7C+s%2C+a%29" alt="R(s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a] = \sum_{r\in\mathbb{R}} r \sum_{s&#39;\in\mathcal{S}} p(s&#39;, r | s, a)" eeimg="1"/> ；</li><li><img src="https://www.zhihu.com/equation?tex=%5Cgamma%5Cin+%5B0%2C1%5D" alt="\gamma\in [0,1]" eeimg="1"/> 表示衰减率，它指征了该MDP的目标为 <img src="https://www.zhihu.com/equation?tex=%5Cmax_%5Cpi+%5Cmathbb%7BE%7D_%7B%5Cpi%2C+P%2C+R%7D%5B%5Csum_%7Bt%3D1%7D%5ET+%5Cgamma%5Et+R_t%5D" alt="\max_\pi \mathbb{E}_{\pi, P, R}[\sum_{t=1}^T \gamma^t R_t]" eeimg="1"/> ，其中 <img src="https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%3D+R%28S_%7Bt%2B1%7D%2C+S_t%2C+A_t%29" alt="R_{t+1} = R(S_{t+1}, S_t, A_t)" eeimg="1"/> </li></ul><p>在下面的几个部分中，我们讨论的MDP都是有限状态的马可夫决策过程。</p><h2><b>价值函数</b></h2><p>接下来我们介绍强化学习领域非常重要的两个函数<b><i>状态价值函数</i></b>（state value function） <img src="https://www.zhihu.com/equation?tex=V_%5Cpi%28s%29" alt="V_\pi(s)" eeimg="1"/> 和<b><i>行动价值函数</i></b>（action value function） <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2C+a%29" alt="Q_\pi(s, a)" eeimg="1"/> ，其中函数 <img src="https://www.zhihu.com/equation?tex=V_%5Cpi%28s%29" alt="V_\pi(s)" eeimg="1"/> 表示的是当到达某个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 之后，如果接下来一直按着策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 来行动，能够获得的期望收益；函数 <img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2C+a%29" alt="Q_\pi(s, a)" eeimg="1"/> 表示的是当达到某个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"/> 之后，如果采取行动 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> ，接下来再按照策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 来行动，能获得的期望收益。它们的具体定义如下。</p><p><img src="https://www.zhihu.com/equation?tex=V_%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D_%5Cpi%5BG_0+%7C+S_0+%3D+s%5D+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B1%7D+%7C+S_0+%3D+s%5D" alt="V_\pi(s) = \mathbb{E}_\pi[G_0 | S_0 = s] = \mathbb{E}[\sum_{t=0}^\infty \gamma^t R_{t+1} | S_0 = s]" eeimg="1"/> </p><p><img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D_%5Cpi+%5BG_0+%7C+S_0+%3D+s%2C+A_0+%3D+a%5D+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B1%7D+%7C+S_0+%3D+s%2C+A_0+%3D+a%5D" alt="Q_\pi(s, a) = \mathbb{E}_\pi [G_0 | S_0 = s, A_0 = a] = \mathbb{E}[\sum_{t=0}^\infty \gamma^t R_{t+1} | S_0 = s, A_0 = a]" eeimg="1"/> </p><p>递归地展开上面这两个公式即可得到相应的两个<b><i>Bellman方程</i></b>（Bellman equation），比如对于上述关于 <img src="https://www.zhihu.com/equation?tex=V_%5Cpi%28s%29" alt="V_\pi(s)" eeimg="1"/> 的式子，有</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+V_%5Cpi%28s%29+%26+%3D+%5Cmathbb%7BE%7D_%5Cpi%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B1%7D+%7C+S_0+%3D+s%5D%5C%5C+%26%3D+%5Cmathbb%7BE%7D_%5Cpi%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B2%7D+%7C+S_1+%3D+s%5D+%5C%5C+%26%3D+%5Csum_a+%5Cpi%28a%7Cs%29+%5Csum_%7Bs%27%7D+%5Csum_r+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+%5Cmathbb%7BE%7D_%5Cpi%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B2%7D+%7C+S_%7B1%7D+%3D+s%27%5D%5D+%5C%5C+%26%3D+%5Csum_a+%5Cpi%28a%7Cs%29+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+V_%5Cpi%28s%27%29%5D+%5C%5C+%26+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} V_\pi(s) &amp; = \mathbb{E}_\pi[\sum_{t=0}^\infty \gamma^t R_{t+1} | S_0 = s]\\ &amp;= \mathbb{E}_\pi[R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^t R_{t+2} | S_1 = s] \\ &amp;= \sum_a \pi(a|s) \sum_{s&#39;} \sum_r p(s&#39;, r|s, a) [r + \gamma \mathbb{E}_\pi[\sum_{t=0}^\infty \gamma^t R_{t+2} | S_{1} = s&#39;]] \\ &amp;= \sum_a \pi(a|s) \sum_{s&#39;, r} p(s&#39;, r|s, a) [r + \gamma V_\pi(s&#39;)] \\ &amp; \forall s \in \mathcal{S} \end{aligned}" eeimg="1"/> </p><p>这就是关于状态价值函数的Bellman方程，它描述的是当前状态的价值函数和它后续状态价值函数之间的递归关系。同理可以得到关于行动价值函数的Bellman方程。</p><p><img src="https://www.zhihu.com/equation?tex=Q_%5Cpi%28s%2C+a%29+%3D+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+%5Csum_%7Ba%27%7D+%5Cpi%28a%27%7Cs%27%29+Q_%5Cpi%28s%27%2C+a%27%29%5D" alt="Q_\pi(s, a) = \sum_{s&#39;, r} p(s&#39;, r|s, a) [r + \gamma \sum_{a&#39;} \pi(a&#39;|s&#39;) Q_\pi(s&#39;, a&#39;)]" eeimg="1"/> </p><p>那么对于一个有限状态的马可夫决策过程来说，是否存在一个最优的策略呢？如果一个策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> 在任何状态 <img src="https://www.zhihu.com/equation?tex=s+%5Cin+%5Cmathcal%7BS%7D" alt="s \in \mathcal{S}" eeimg="1"/> 下都有 <img src="https://www.zhihu.com/equation?tex=V_%7B%5Cpi%7D%28s%29+%5Cle+V_%7B%5Cpi%E2%80%98%7D%28s%29" alt="V_{\pi}(s) \le V_{\pi‘}(s)" eeimg="1"/> ，那么我们称策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi%27" alt="\pi&#39;" eeimg="1"/> 优于 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 。对于所有的策略排序，我们总可以找到最优策略（有可能不止一个最优策略），我们记最优策略为 <img src="https://www.zhihu.com/equation?tex=%5Cpi%5E%2A" alt="\pi^*" eeimg="1"/> ，最优策略具有如下性质</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+V%5E%2A%28s%29+%26%3D+%5Cmax_%5Cpi+V_%5Cpi%28s%29+%5C%5C+Q%5E%2A%28s%2C+a%29+%26%3D+%5Cmax_%5Cpi+Q_%5Cpi%28s%2C+a%29+%5Cend%7Baligned%7D" alt="\begin{aligned} V^*(s) &amp;= \max_\pi V_\pi(s) \\ Q^*(s, a) &amp;= \max_\pi Q_\pi(s, a) \end{aligned}" eeimg="1"/> </p><p>使用上述类似的方法，我们可以得到<b><i>最优情况下的Bellman方程</i></b>（Bellman optimality equation）</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+V%5E%2A%28s%29+%26%3D+%5Cmax_%7Ba%5Cin%5Cmathcal%7BA%7D%28s%29%7D+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29%5Br+%2B+%5Cgamma+V%5E%2A%28s%27%29%5D+%5C%5C+Q%5E%2A%28s%2C+a%29+%26%3D+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+%5Cmax_%7Ba%27%7D+Q%5E%2A%28s%27%2C+a%27%29%5D+%5Cend%7Baligned%7D" alt="\begin{aligned} V^*(s) &amp;= \max_{a\in\mathcal{A}(s)} \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma V^*(s&#39;)] \\ Q^*(s, a) &amp;= \sum_{s&#39;, r} p(s&#39;, r|s, a) [r + \gamma \max_{a&#39;} Q^*(s&#39;, a&#39;)] \end{aligned}" eeimg="1"/> </p><p>显然地，状态价值函数和行动价值函数之间有关系 <img src="https://www.zhihu.com/equation?tex=V_%5Cpi%28s%29+%3D+%5Csum_%7Ba%7D+%5Cpi%28a%7Cs%29+Q_%5Cpi%28s%2C+a%29" alt="V_\pi(s) = \sum_{a} \pi(a|s) Q_\pi(s, a)" eeimg="1"/> ， <img src="https://www.zhihu.com/equation?tex=V%5E%2A%28s%29+%3D+%5Cmax_%7Ba%7D+Q%5E%2A%28s%2C+a%29" alt="V^*(s) = \max_{a} Q^*(s, a)" eeimg="1"/> 。</p><h2><b>Bellman算子</b></h2><p>在前一节中我们定义了两种价值函数，如果能够求得在某策略下的价值函数，我们就可以对该策略的好坏进行评估；如果能够求得最优情形下的价值函数，我们就可以自然得出最优策略。但我们还不知道要如何计算得到相应的价值函数数值，我们这里引入的<b><i>Bellman算子</i></b>（Bellman operator/Bellman backup operator）及其相关的性质，可以为相关算法的收敛性提供一些保证。</p><p>考虑有限的状态空间 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D%3D%5Bn%5D" alt="\mathcal{S}=[n]" eeimg="1"/> 和一个定义在状态集上的实值函数 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"/> ，状态空间给定的时候该实值函数也可以写成向量的形式 <img src="https://www.zhihu.com/equation?tex=V%5Cin%5Cmathbb%7BR%7D%5En" alt="V\in\mathbb{R}^n" eeimg="1"/> 。定义Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D%3A+%5Cmathbb%7BR%7D%5En%5Cto+%5Cmathbb%7BR%7D%5En" alt="\mathcal{T}: \mathbb{R}^n\to \mathbb{R}^n" eeimg="1"/> ，</p><p><img src="https://www.zhihu.com/equation?tex=%28%5Cmathcal%7BT%7D+V%29_i+%3D+%5Cmax_%7Ba%5Cin%5Cmathcal%7BA%7D%28i%29%7D+%5Csum_%7Bj%5Cin+%5Bn%5D%7D+P%28j%7Ci%2Ca%29+%28R%28j%2C+i%2C+a%29+%2B+%5Cgamma+V_j%29" alt="(\mathcal{T} V)_i = \max_{a\in\mathcal{A}(i)} \sum_{j\in [n]} P(j|i,a) (R(j, i, a) + \gamma V_j)" eeimg="1"/> </p><p>其中下标都表示向量的某个元素。对于一个策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 还可以定义Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D_%5Cpi%3A+%5Cmathbb%7BR%7D%5En%5Cto+%5Cmathbb%7BR%7D%5En" alt="\mathcal{T}_\pi: \mathbb{R}^n\to \mathbb{R}^n" eeimg="1"/> ，</p><p><img src="https://www.zhihu.com/equation?tex=%28%5Cmathcal%7BT%7D_%5Cpi+V%29_i+%3D+%5Csum_%7Bj%5Cin+%5Bn%5D%7D+P%28j%7Ci%2C%5Cpi%29+%28R%28j%2C+i%2C+%5Cpi%29+%2B+%5Cgamma+V_j%29" alt="(\mathcal{T}_\pi V)_i = \sum_{j\in [n]} P(j|i,\pi) (R(j, i, \pi) + \gamma V_j)" eeimg="1"/> </p><p>对于确定性的策略来说里面的 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 就是 <img src="https://www.zhihu.com/equation?tex=%5Cpi%28i%29" alt="\pi(i)" eeimg="1"/> ；对于随机策略来说，还需要在外面对策略给出的行动求期望，这里将其省略。</p><p>Bellman算子具有以下性质：</p><ul><li>Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D" alt="\mathcal{T}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D_%5Cpi" alt="\mathcal{T}_\pi" eeimg="1"/> 在max norm下都是按系数（modulus） <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"/> 收缩的（contraction），即 <img src="https://www.zhihu.com/equation?tex=%5ClVert+%5Cmathcal%7BT%7D+V+-+%5Cmathcal%7BT%7D+U%5CrVert_%7B%5Cinfty%7D+%5Cle+%5Cgamma+%5ClVert+V+-+U%5CrVert_%7B%5Cinfty%7D" alt="\lVert \mathcal{T} V - \mathcal{T} U\rVert_{\infty} \le \gamma \lVert V - U\rVert_{\infty}" eeimg="1"/>。 </li><li>Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D" alt="\mathcal{T}" eeimg="1"/> 具有唯一不动点 <img src="https://www.zhihu.com/equation?tex=V%5E%2A" alt="V^*" eeimg="1"/> ，即 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7DV%5E%2A+%3D+V%5E%2A" alt="\mathcal{T}V^* = V^*" eeimg="1"/> ；Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D_%5Cpi" alt="\mathcal{T}_\pi" eeimg="1"/> 也具有唯一不动点 <img src="https://www.zhihu.com/equation?tex=V_%5Cpi" alt="V_\pi" eeimg="1"/> ，即 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D_%5Cpi+V_%5Cpi+%3D+V_%5Cpi" alt="\mathcal{T}_\pi V_\pi = V_\pi" eeimg="1"/> ，<img src="https://www.zhihu.com/equation?tex=V%5E%2A" alt="V^*" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D_%5Cpi" alt="\mathcal{T}_\pi" eeimg="1"/> 的定义与前一节相同。前一个性质加上Banach fixed point theorem可以推导到此性质。</li><li>Bellman算子 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D" alt="\mathcal{T}" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D_%5Cpi" alt="\mathcal{T}_\pi" eeimg="1"/> 单调（monotonic），即 <img src="https://www.zhihu.com/equation?tex=V%5Cle+%5Cmathcal%7BT%7D+V+%5CRightarrow+V+%5Cle+%5Cmathcal%7BT%7D+V+%5Cle+%5Cmathcal%7BT%7D%5E2+V+%5Cle+%5Ccdots+%5Cle+V%5E%2A" alt="V\le \mathcal{T} V \Rightarrow V \le \mathcal{T} V \le \mathcal{T}^2 V \le \cdots \le V^*" eeimg="1"/> ，其中对于向量来说 <img src="https://www.zhihu.com/equation?tex=%5Cle" alt="\le" eeimg="1"/> 表示每一个元素都小于等于。</li><li>当 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7DV+%3D+V" alt="\mathcal{T}V = V" eeimg="1"/> 时， <img src="https://www.zhihu.com/equation?tex=V+%3D+V%5E%2A" alt="V = V^*" eeimg="1"/> ；当 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D_%5Cpi+V+%3D+V" alt="\mathcal{T}_\pi V = V" eeimg="1"/> 时， <img src="https://www.zhihu.com/equation?tex=V+%3D+V_%5Cpi" alt="V = V_\pi" eeimg="1"/> ；当 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7DV+%3D+%5Cmathcal%7BT%7D_%5Cpi+V+%3D+V" alt="\mathcal{T}V = \mathcal{T}_\pi V = V" eeimg="1"/> 时， <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 是最优策略。</li><li>对任意的 <img src="https://www.zhihu.com/equation?tex=V%5Cin+%5Cmathbb%7BR%7D%5En" alt="V\in \mathbb{R}^n" eeimg="1"/> ， <img src="https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Cto+%5Cinfty%7D+%5Cmathcal%7BT%7D%5Ek+V+%3D+V%5E%2A" alt="\lim_{k\to \infty} \mathcal{T}^k V = V^*" eeimg="1"/> 。</li></ul><h2><b>Value Iteration和Policy Iteration</b></h2><p>给出两种找到最优值函数 <img src="https://www.zhihu.com/equation?tex=V%5E%2A" alt="V^*" eeimg="1"/> 的算法，分别是<b><i>值迭代</i></b>（Value Iteration，VI）和<b><i>策略迭代</i></b>（Policy Iteration，PI），最后给出<b><i>改进的策略迭代</i></b>（Modified Policy Iteration）是这两者的统一。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%B8%80%EF%BC%9AValue+Iteration%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+V%5E%7B%280%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Bfor+%7D+t%3D0%2C1%2C%5Ccdots%2C+T+%5Ctext%7B+do%7D+%5C%5C+3%5Cquad+%26+%5Cquad+V%5E%7B%28t%2B1%29%7D+%5Cleftarrow+%5Cmathcal%7BT%7D+V%5E%7B%28t%29%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法一：Value Iteration} \\ 1\quad &amp; \text{initialize } V^{(0)} \\ 2\quad &amp; \text{for } t=0,1,\cdots, T \text{ do} \\ 3\quad &amp; \quad V^{(t+1)} \leftarrow \mathcal{T} V^{(t)} \end{aligned}" eeimg="1"/> </p><p>值迭代有如下性质：</p><ul><li>如果初始化的时候 <img src="https://www.zhihu.com/equation?tex=V%5E%7B%280%29%7D" alt="V^{(0)}" eeimg="1"/> 的每一个元素都置0，那么有 <img src="https://www.zhihu.com/equation?tex=V%5E%7B%280%29%7D+%5Cle+V%5E%7B%281%29%7D+%5Cle+%5Ccdots" alt="V^{(0)} \le V^{(1)} \le \cdots" eeimg="1"/> 。</li><li><img src="https://www.zhihu.com/equation?tex=%5ClVert+V%5E%7B%28n%29%7D+-+V%5E%2A%5CrVert+_%5Cinfty+%5Cle+%5Cgamma%5En+%5ClVert+R+%5CrVert+_%5Cinfty+%2F+%281-%5Cgamma%29" alt="\lVert V^{(n)} - V^*\rVert _\infty \le \gamma^n \lVert R \rVert _\infty / (1-\gamma)" eeimg="1"/> 。</li><li>在 <img src="https://www.zhihu.com/equation?tex=T%5Cto+%5Cinfty" alt="T\to \infty" eeimg="1"/> 的时候，能够得到 <img src="https://www.zhihu.com/equation?tex=V%3DV%5E%2A" alt="V=V^*" eeimg="1"/> 。</li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%BA%8C%EF%BC%9APolicy+Iteration%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+V%5E%7B%280%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Bfor+%7D+t%3D0%2C1%2C%5Ccdots%2C+T+%5Ctext%7B+do%7D+%5C%5C+3%5Cquad+%26+%5Cquad+%5Ctext%7Bfind+%7D+%5Cpi_t%2C+%5Ctext%7B+s.t.+%7D+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D+V%5E%7B%28t%29%7D+%3D+%5Cmathcal%7BT%7D+V%5E%7B%28t%29%7D+%5Ctext%7B+%28i.e+greedy+policy%29%7D+%5C%5C+4%5Cquad+%26+%5Cquad+V%5E%7B%28t%2B1%29%7D+%5Cleftarrow+%5Clim_%7Bk%5Cto+%5Cinfty%7D+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D%5Ek+V%5E%7B%28t%29%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法二：Policy Iteration} \\ 1\quad &amp; \text{initialize } V^{(0)} \\ 2\quad &amp; \text{for } t=0,1,\cdots, T \text{ do} \\ 3\quad &amp; \quad \text{find } \pi_t, \text{ s.t. } \mathcal{T}_{\pi_t} V^{(t)} = \mathcal{T} V^{(t)} \text{ (i.e greedy policy)} \\ 4\quad &amp; \quad V^{(t+1)} \leftarrow \lim_{k\to \infty} \mathcal{T}_{\pi_t}^k V^{(t)} \end{aligned}" eeimg="1"/> </p><p>策略迭代分为两个步骤交替进行：一步称作<b><i>策略改进</i></b>（policy improvement），即固定价值函数，并且找到在该价值函数下的最优策略（第3行）；另一步称作<b><i>策略估计</i></b>（policy evaluation），即固定策略，找出该策略下的价值函数（第4行）。这样的更新方式同样具有类似值循环的收敛和界的性质。这里出现的 <img src="https://www.zhihu.com/equation?tex=%5Clim_%7Bk%5Cto+%5Cinfty%7D" alt="\lim_{k\to \infty}" eeimg="1"/> 在实际操作中通常是循环直到 <img src="https://www.zhihu.com/equation?tex=%5ClVert+V%5CrVert_%5Cinfty" alt="\lVert V\rVert_\infty" eeimg="1"/> 小于某个设定的小量。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%B8%89%EF%BC%9AModified+Policy+Iteration%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+V%5E%7B%280%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Bfor+%7D+t%3D0%2C1%2C%5Ccdots%2C+T+%5Ctext%7B+do%7D+%5C%5C+3%5Cquad+%26+%5Cquad+%5Ctext%7Bfind+%7D+%5Cpi_t%2C+%5Ctext%7B+s.t.+%7D+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D+V%5E%7B%28t%29%7D+%3D+%5Cmathcal%7BT%7D+V%5E%7B%28t%29%7D+%5Ctext%7B+%28i.e+greedy+policy%29%7D+%5C%5C+4%5Cquad+%26+%5Cquad+V%5E%7B%28t%2B1%29%7D+%5Cleftarrow+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D%5E%7Bm_t%7D+V%5E%7B%28t%29%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法三：Modified Policy Iteration} \\ 1\quad &amp; \text{initialize } V^{(0)} \\ 2\quad &amp; \text{for } t=0,1,\cdots, T \text{ do} \\ 3\quad &amp; \quad \text{find } \pi_t, \text{ s.t. } \mathcal{T}_{\pi_t} V^{(t)} = \mathcal{T} V^{(t)} \text{ (i.e greedy policy)} \\ 4\quad &amp; \quad V^{(t+1)} \leftarrow \mathcal{T}_{\pi_t}^{m_t} V^{(t)} \end{aligned}" eeimg="1"/> </p><p>改进的策略迭代是对于上面两种算法的统一：当 <img src="https://www.zhihu.com/equation?tex=m_t%3D1" alt="m_t=1" eeimg="1"/> 时，该算法就是值循环；当 <img src="https://www.zhihu.com/equation?tex=m_t%3D%5Cinfty" alt="m_t=\infty" eeimg="1"/> 时，该算法就是策略迭代。该算法的收敛性能够由Bellman算子的性质保证。值迭代也可以看做策略迭代的策略评价步只进行了一次迭代，还没有完全收敛到该策略的价值函数的时候，又进行了下一步的策略改进步。实际上，这并不影响算法的收敛，并且常常会更快；更进一步，在第3、4行的循环中，甚至不需要每次都更新所有的状态（即 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"/> 的所有位置），该算法同样有效。这类交替甚至异步进行估计 <img src="https://www.zhihu.com/equation?tex=V_%5Cpi" alt="V_\pi" eeimg="1"/> 、改进 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 的算法思想，我们称之为<b><i>广义策略迭代</i></b>（generalized policy iteration）。</p><h2><b>动态规划方法</b></h2><p><b><i>动态规划</i></b>（dynamic programming, DP）算法其实就是直接从前面的值循环得到的，得到最优情形下的状态价值函数之后，输出相应的最优策略。下面给出动态规划算法。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E5%9B%9B%EF%BC%9ADynamic+Programming%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+array+V+arbitrarily+%28eg.+%7D+V%28s%29%3D0%2C+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%5E%2B+%5Ctext%7B%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Brepeat%7D+%5C%5C+3%5Cquad+%26+%5Cquad+%5CDelta+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+each+%7D+s+%5Cin+%5Cmathcal%7BS%7D+%5C%5C+5%5Cquad+%26+%5Cquad+%5Cquad+v+%5Cleftarrow+V%28s%29+%5C%5C+6%5Cquad+%26+%5Cquad+%5Cquad+V%28s%29+%5Cleftarrow+%5Cmax_a+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29%5Br+%2B+%5Cgamma+V%28s%27%29%5D+%5C%5C+7%5Cquad+%26+%5Cquad+%5Cquad+%5CDelta+%5Cleftarrow+%5Cmax%28%5CDelta%2C+%7Cv+-+V%28s%29%7C%29+%5C%5C+8%5Cquad+%26+%5Ctext%7Buntil+%7D+%5CDelta+%3C+%5Ctheta+%5Ctext%7B+%28a+small+positive+number%29%7D+%5C%5C+9%5Cquad+%26+%5Ctext%7Boutput+a+deterministic+policy%2C+%7D+%5Cpi+%5Capprox+%5Cpi_%2A+%5Ctext%7B%2C+such+that%7D+%5C%5C+10%5Cquad+%26+%5Cpi%28s%29+%3D+arg%5Cmax_a+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29%5Br+%2B+%5Cgamma+V%28s%27%29%5D+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法四：Dynamic Programming} \\ 1\quad &amp; \text{initialize array V arbitrarily (eg. } V(s)=0, \forall s \in \mathcal{S}^+ \text{)} \\ 2\quad &amp; \text{repeat} \\ 3\quad &amp; \quad \Delta \leftarrow 0 \\ 4\quad &amp; \quad \text{for each } s \in \mathcal{S} \\ 5\quad &amp; \quad \quad v \leftarrow V(s) \\ 6\quad &amp; \quad \quad V(s) \leftarrow \max_a \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma V(s&#39;)] \\ 7\quad &amp; \quad \quad \Delta \leftarrow \max(\Delta, |v - V(s)|) \\ 8\quad &amp; \text{until } \Delta &lt; \theta \text{ (a small positive number)} \\ 9\quad &amp; \text{output a deterministic policy, } \pi \approx \pi_* \text{, such that} \\ 10\quad &amp; \pi(s) = arg\max_a \sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \gamma V(s&#39;)] \end{aligned}" eeimg="1"/> </p><p>动态规划算法也可以看做把前面推导的Bellman方程化为了增量的形式进行计算，当算法收敛到不动点的时候，这时得到的状态价值函数就满足最优情形下的Bellman方程，对应的贪心策略就是最优策略。其中，核心的算法在第6行，它迭代地将状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 逼近最优状态价值函数 <img src="https://www.zhihu.com/equation?tex=V%5E%2A%28s%29" alt="V^*(s)" eeimg="1"/> 。算法的循环中没有显式地定义每一步对应的策略，我们可以对于每一步迭代的价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 都定义相应的贪心策略。利用以下定理，我们可以知道，经历每一步的迭代，之后策略都不比之前的策略差；从而当算法收敛的时候，我们可以得到最优策略。 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7BPolicy+Improvement+Theorem%7D+%5C%5C+%5Cquad+%26+%5Ctext%7B%E5%A6%82%E6%9E%9C%E5%AD%98%E5%9C%A8%E4%B8%80%E5%AF%B9%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E7%AD%96%E7%95%A5%7D+%5Cpi+%5Ctext%7B%E5%92%8C%7D+%5Cpi%27+%5Ctext%7B%EF%BC%8C%E4%BD%BF%E5%BE%97%7D+q_%5Cpi%28s%2C+%5Cpi%27%29+%5Cge+v_%5Cpi+%28s%29%2C+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D+%5Ctext%7B%EF%BC%8C%E9%82%A3%E4%B9%88%7D+%5Cpi%27+%5Ctext%7B%E4%B8%8D%E6%AF%94%7D+%5Cpi+%5Ctext%7B%E5%B7%AE%E3%80%82%7D%5C%5C+%5Cquad+%26+%5Ctext%7B%E5%A6%82%E6%9E%9C%E5%AD%98%E5%9C%A8%E4%B8%80%E5%AF%B9%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E7%AD%96%E7%95%A5%7D+%5Cpi+%5Ctext%7B%E5%92%8C%7D+%5Cpi%27+%5Ctext%7B%EF%BC%8C%E4%BD%BF%E5%BE%97%7D+v_%5Cpi%27%28s%29+%5Cge+v_%5Cpi+%28s%29%2C+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D+%5Ctext%7B%EF%BC%8C%E9%82%A3%E4%B9%88%7D+%5Cpi%27+%5Ctext%7B%E4%B8%8D%E6%AF%94%7D+%5Cpi+%5Ctext%7B%E5%B7%AE%E3%80%82%7D%5C%5C+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{Policy Improvement Theorem} \\ \quad &amp; \text{如果存在一对确定性的策略} \pi \text{和} \pi&#39; \text{，使得} q_\pi(s, \pi&#39;) \ge v_\pi (s), \forall s \in \mathcal{S} \text{，那么} \pi&#39; \text{不比} \pi \text{差。}\\ \quad &amp; \text{如果存在一对确定性的策略} \pi \text{和} \pi&#39; \text{，使得} v_\pi&#39;(s) \ge v_\pi (s), \forall s \in \mathcal{S} \text{，那么} \pi&#39; \text{不比} \pi \text{差。}\\ \end{aligned}" eeimg="1"/> </p><p>上述算法的第6行中可以看到， <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 是单调不减的，因此，其对应的策略也应该是单调提升的。</p><p>从第5行-第7行可以看出，在每一轮迭代中，都需要对于状态空间中所有的状态访问一次。当状态空间庞大的时候，这样的循环会十分消耗时间。可以采用异步DP的算法，每轮迭代仅更新一部分的状态，只要保证在多轮迭代中，每个状态都能被访问到，这样的异步DP算法一样能收敛到最优策略上。</p><p>DP算法对于每一个状态价值函数的估算都依赖于前一时刻各状态价值函数的数值，这种特性我们称之为<b><i>bootstrap</i></b>。这个单词本意是拔靴带，一个比较好的翻译是“自举”，参看下面这张图片来更好的理解这个词的含义。同时我们注意到，在DP算法中，我们认为反映环境的所有信息 <img src="https://www.zhihu.com/equation?tex=p%28s%27%2C+r%7Cs%2C+a%29" alt="p(s&#39;, r|s, a)" eeimg="1"/> 是已知的，即我们已经拥有了对于环境的建模，这种利用反映环境信息的模型来进行计算的特性我们称之为<b><i>model-based</i></b>。从某种意义上来说，DP方法本质上还并没有涉及到对于环境的“学习”过程，因为DP没有通过与环境的交互来获取关于环境的信息。</p><p class="ztext-empty-paragraph"><br/></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1f06d3ef3383adb4540428f322cf9a03_b.jpg" data-size="normal" data-rawwidth="362" data-rawheight="277" class="content_image" width="362"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;362&#39; height=&#39;277&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="362" data-rawheight="277" class="content_image lazy" width="362" data-actualsrc="https://pic4.zhimg.com/v2-1f06d3ef3383adb4540428f322cf9a03_b.jpg"/><figcaption>图：Bootstrap</figcaption></figure><h2><b>蒙特卡洛方法</b></h2><p>如果说动态规划类方法主要从迭代的角度出发，通过反复把Bellman算子作用到价值函数上期望收敛到最优情形下的价值函数；那么<b><i>蒙特卡洛方法</i></b>（Monte Carlo methods）则是直接从最优价值函数的定义出发，通过采样来直接对于最优价值函数进行无偏估计。</p><p>具体来说就是先进行采样，即随机地从一些状态出发，然后按照行动策略（behavior policy） <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 采取行动，直到达到终止状态；然后进行最有价值函数的估计，即回溯地利用探索到的这部分信息来更新目标策略（target policy） <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 下的价值函数。当每一轮的行动策略与目标策略一致的时候，我们称这样的方法为on-policy的；反之，我们称之为off-policy的。</p><p>我们先来学习on-policy的情形。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%BA%94%EF%BC%9AMonte+Carlo+Method+%28on-policy%29%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+Returns%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Bempty+list%7D+%5C%5C+4%5Cquad+%26+%5Cquad+%5Cpi%28a%7Cs%29+%5Cleftarrow+%5Ctext%7Barbitrary+%7D%5Cepsilon%5Ctext%7B-soft+policy%7D+%5C%5C+5%5Cquad+%26+%5Ctext%7Brepeat+forever%3A%7D+%5C%5C+6%5Cquad+%26+%5Cquad+%5Ctext%7Bgenerate+an+episode+using+%7D%5Cpi+%5C%5C+7%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+each+pair+%7D+s%2C+a+%5Ctext%7B+in+the+episode%3A%7D+%5C%5C+8%5Cquad+%26+%5Cquad+%5Cquad+G+%5Cleftarrow+%5Ctext%7Breturn+following+the+first+occurance+of+%7D+s%2C+a+%5C%5C+9%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bappend+%7D+G+%5Ctext%7B+to+%7D+Returns%28s%2C+a%29+%5C%5C+10%5Cquad+%26+%5Cquad+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Baverage%7D%28Returns%28s%2C+a%29%29+%5C%5C+11%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+each+%7D+s+%5Ctext%7B+in+the+episode%3A%7D+%5C%5C+12%5Cquad+%26+%5Cquad+%5Cquad+A%5E%2A+%5Cleftarrow+arg%5Cmax_a+Q%28s%2Ca%29+%5C%5C+13%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bfor+all+%7D+a%5Cin%5Cmathcal%7BA%7D%28s%29+%5C%5C+14%5Cquad+%26+%5Cquad+%5Cquad+%5Cquad+%5Cpi%28a%7Cs%29+%5Cleftarrow+%5Cbegin%7Bcases%7D+1-%5Cepsilon%2B%5Cepsilon%2F%7C%5Cmathcal%7BA%7D%28s%29%7C+%26+%5Ctext%7Bif+%7D+a+%3D+A%5E%2A+%5C%5C+%5Cepsilon%2F%7C%5Cmathcal%7BA%7D%28s%29%7C+%26+%5Ctext%7Bif+%7D+a+%5Cneq+A%5E%2A+%5Cend%7Bcases%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法五：Monte Carlo Method (on-policy)} \\ 1\quad &amp; \text{initialize } \forall s \in \mathcal{S}, a \in \mathcal{A}(s)\\ 2\quad &amp; \quad Q(s, a) \leftarrow \text{arbitrary}  \\ 3\quad &amp; \quad Returns(s, a) \leftarrow \text{empty list} \\ 4\quad &amp; \quad \pi(a|s) \leftarrow \text{arbitrary }\epsilon\text{-soft policy} \\ 5\quad &amp; \text{repeat forever:} \\ 6\quad &amp; \quad \text{generate an episode using }\pi \\ 7\quad &amp; \quad \text{for each pair } s, a \text{ in the episode:} \\ 8\quad &amp; \quad \quad G \leftarrow \text{return following the first occurance of } s, a \\ 9\quad &amp; \quad \quad \text{append } G \text{ to } Returns(s, a) \\ 10\quad &amp; \quad \quad Q(s, a) \leftarrow \text{average}(Returns(s, a)) \\ 11\quad &amp; \quad \text{for each } s \text{ in the episode:} \\ 12\quad &amp; \quad \quad A^* \leftarrow arg\max_a Q(s,a) \\ 13\quad &amp; \quad \quad \text{for all } a\in\mathcal{A}(s) \\ 14\quad &amp; \quad \quad \quad \pi(a|s) \leftarrow \begin{cases} 1-\epsilon+\epsilon/|\mathcal{A}(s)| &amp; \text{if } a = A^* \\ \epsilon/|\mathcal{A}(s)| &amp; \text{if } a \neq A^* \end{cases} \end{aligned}" eeimg="1"/> </p><p>对于MC方法，只有满足如下两个条件的时候才能够保证算法收敛到最优策略</p><ul><li>主循环被执行无穷多次</li><li>当循环次数趋向无穷的时候，每个状态被遍历的次数也趋向无穷</li></ul><p>对于第一点，我们只能在资源允许的情况下尽可能多地运行主循环。对于第二点来说，我们的解决方法主要有两个思路；要么在每次主循环开头时，保证选择任意起始状态的概率都大于零，这种方法称之为exploring start；要么在算法运行过程中保证我们行动策略 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 具有足够的随机性，即能始终以一个大于零的几率访问任意可能的行动。特别地， <img src="https://www.zhihu.com/equation?tex=%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D" alt="\forall s \in \mathcal{S}" eeimg="1"/> ，如果某个策略 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 能够保证对于每个行动的访问概率不低于 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon+%2F+%7C%5Cmathcal%7BA%7D%28s%29%7C" alt="\epsilon / |\mathcal{A}(s)|" eeimg="1"/> ，则称这样的策略是 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -soft的。</p><p>对于算法五第14行中定义的策略，我们称之为 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -greedy的，它是一种 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -soft策略。算法五中第10行是对于当前策略对应价值函数的估计，第14行是基于当前价值函数对于策略进行改进，由于探索使用的策略（第6行）和目标输出的策略（第14行）是同样的策略，因此这种方法是on-policy的。</p><p>行动策略也可以和目标策略不相同，即得到下面的off-policy的蒙特卡洛算法。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E5%85%AD%EF%BC%9AMonte+Carlo+Method+%28off-policy%29%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+C%28s%2C+a%29+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Cquad+%5Cpi%28s%29+%5Cleftarrow+%5Ctext%7Ba+deterministic+policy+that+is+greedy+w.r.t.%7D+Q%5C%5C+5%5Cquad+%26+%5Ctext%7Brepeat+forever%3A%7D+%5C%5C+6%5Cquad+%26+%5Cquad+%5Ctext%7Bgenerate+an+episode+using+any+soft+policy+%7D%5Cmu+%5C%5C+7%5Cquad+%26+%5Cquad+%5Cquad+S_0%2C+A_0%2C+R_1%2C+%5Ccdots%2C+S_%7BT-1%7D%2C+A_%7BT-1%7D%2C+R_T%2C+S_T+%5C%5C+8%5Cquad+%26+%5Cquad+G+%5Cleftarrow+0+%5C%5C+9%5Cquad+%26+%5Cquad+W+%5Cleftarrow+1+%5C%5C+10%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+%7D+t+%3D+T-1%2C+T-2%2C+%5Ccdots%2C++%5Ctext%7B+downto+%7D+0+%5Ctext%7B%3A%7D+%5C%5C+11%5Cquad+%26+%5Cquad+%5Cquad+G+%5Cleftarrow+%5Cgamma+G+%2B+R_%7Bt%2B1%7D+%5C%5C+12%5Cquad+%26+%5Cquad+%5Cquad+C%28S_t%2C+A_t%29+%5Cleftarrow+C%28S_t%2C+A_t%29+%2B+W+%5C%5C+13%5Cquad+%26+%5Cquad+%5Cquad+Q%28S_t%2C+A_t%29+%5Cleftarrow+Q%28S_t%2C+A_t%29+%2B+%5Cfrac%7BW%7D%7BC%28S_t%2C+A_t%29%7D+%28G+-+Q%28S_t%2C+A_t%29%29+%5C%5C+14%5Cquad+%26+%5Cquad+%5Cquad+%5Cpi%28S_t%29+%5Cleftarrow+arg%5Cmax_a+Q%28S_t%2C+a%29+%5C%5C+15%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bif+%7D+A_t+%5Cneq+%5Cpi%28S_t%29+%5Ctext%7B+then+exit+forloop%7D+%5C%5C+16%5Cquad+%26+%5Cquad+%5Cquad+W+%5Cleftarrow+W+%2F+%5Cmu%28A_t%7CS_t%29+%5C%5C+%5Cend%7Baligned%7D" alt="\begin{aligned} &amp; \text{算法六：Monte Carlo Method (off-policy)} \\ 1\quad &amp; \text{initialize } \forall s \in \mathcal{S}, a \in \mathcal{A}(s)\\ 2\quad &amp; \quad Q(s, a) \leftarrow \text{arbitrary}  \\ 3\quad &amp; \quad C(s, a) \leftarrow 0 \\ 4\quad &amp; \quad \pi(s) \leftarrow \text{a deterministic policy that is greedy w.r.t.} Q\\ 5\quad &amp; \text{repeat forever:} \\ 6\quad &amp; \quad \text{generate an episode using any soft policy }\mu \\ 7\quad &amp; \quad \quad S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_T, S_T \\ 8\quad &amp; \quad G \leftarrow 0 \\ 9\quad &amp; \quad W \leftarrow 1 \\ 10\quad &amp; \quad \text{for } t = T-1, T-2, \cdots,  \text{ downto } 0 \text{:} \\ 11\quad &amp; \quad \quad G \leftarrow \gamma G + R_{t+1} \\ 12\quad &amp; \quad \quad C(S_t, A_t) \leftarrow C(S_t, A_t) + W \\ 13\quad &amp; \quad \quad Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)} (G - Q(S_t, A_t)) \\ 14\quad &amp; \quad \quad \pi(S_t) \leftarrow arg\max_a Q(S_t, a) \\ 15\quad &amp; \quad \quad \text{if } A_t \neq \pi(S_t) \text{ then exit forloop} \\ 16\quad &amp; \quad \quad W \leftarrow W / \mu(A_t|S_t) \\ \end{aligned}" eeimg="1"/> </p><p>而算法三中，探索所使用的策略 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 是任意固定的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> -soft策略（第6行），比如它可以是在行动集中等概率选取行动的随机策略。而目标输出的策略 <img src="https://www.zhihu.com/equation?tex=%5Cpi" alt="\pi" eeimg="1"/> 是一个关于价值函数的贪心策略，因此这种方法是off-policy的。由于行动策略与目标策略不一致，因此在更新价值函数的时候我们需要把行动策略对应的收益投影到目标函数对应的收益上。直观地来说，如果行动策略选择了某个当前目标策略大概率会选择的路径，那么对应的价值函数的改变会更多一些；反之，应该尽可能少一些地改变价值函数。这里我们使用了重要性采样（importance sampling）的技术。即目标策略对应的价值函数 <img src="https://www.zhihu.com/equation?tex=V%28s%29" alt="V(s)" eeimg="1"/> 与行动策略得到的收益 <img src="https://www.zhihu.com/equation?tex=G_t" alt="G_t" eeimg="1"/> 应该满足如下关系。</p><p><img src="https://www.zhihu.com/equation?tex=V%28s%29+%3D+%5Cdfrac%7B%5Csum_%7Bt%7D+%5Crho_t%5E%7BT%7D+G_t%7D%7B%5Csum_%7Bt%7D+%5Crho_t%5E%7BT%7D%7D" alt="V(s) = \dfrac{\sum_{t} \rho_t^{T} G_t}{\sum_{t} \rho_t^{T}}" eeimg="1"/> </p><p>其中重要性采样率（importance-sampling ratio）</p><p><img src="https://www.zhihu.com/equation?tex=%5Crho_t%5E%7BT%7D+%3D+%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D+%5Cdfrac%7B%5Cpi%28A_k%7CS_k%29%7D%7B%5Cmu%28A_k%7CS_k%29%7D" alt="\rho_t^{T} = \prod_{k=t}^{T-1} \dfrac{\pi(A_k|S_k)}{\mu(A_k|S_k)}" eeimg="1"/> </p><p>其中， <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> 表示终止状态的时刻。算法三中的第12、13、16行所描述的就是这样的重要性采样的增量实现。</p><p>总体来说，蒙特卡洛方法能够通过与环境的交互来学习一个好的策略，它不需要对于环境的建模，即它是<b><i>model-free</i></b>的。不同于之前的动态规划算法，每一步中价值函数的更新并不依赖于之前的价值函数，而是直接更新到采样得到收益的平均值上，因此它不是bootstrap的。由于蒙特卡洛方法要求一直采样到最末尾，因此它只能用于回合制的任务。</p><h2><b>总结</b></h2><p>这一讲我们学习了强化学习的一些基本概念，包括</p><ul><li>强化学习的基础模型是马可夫决策模型</li><li>强化学习的目标是找到最大化收益的策略</li><li>找寻策略的一个重要途径是找到该马可夫决策模型上的价值函数</li><ul><li>直接使用采样的方法估计这个被定义出来的价值函数形成了蒙特卡洛方法</li><li>使用Bellman算子迭代计算这个价值函数形成了动态规划方法</li><li>在此两者的基础上还会发展出我们后面会讲到的<b><i>时间差分方法</i></b>（temporal-difference，TD）</li></ul></ul><p>强化学习的不同算法有不同的维度，我们可以总结出来下面这张表，在之后的学习之中，我们可以加入更多的行（算法）和列（维度）</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-57a4d448731936d8a189f9c75be21e23_b.png" data-caption="" data-size="normal" data-rawwidth="580" data-rawheight="83" class="origin_image zh-lightbox-thumb" width="580" data-original="https://pic4.zhimg.com/v2-57a4d448731936d8a189f9c75be21e23_r.jpg"/><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;580&#39; height=&#39;83&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="580" data-rawheight="83" class="origin_image zh-lightbox-thumb lazy" width="580" data-original="https://pic4.zhimg.com/v2-57a4d448731936d8a189f9c75be21e23_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-57a4d448731936d8a189f9c75be21e23_b.png"/></figure><h2><b>参考资料</b></h2><ol><li>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. Vol. 1. No. 1. Cambridge: MIT press, 1998. 一本经典的强化学习入门教材</li><li>Puterman, Martin L. Markov decision processes: discrete stochastic dynamic programming. John Wiley &amp; Sons, 2014. 一本讲马可夫决策过程的书，涉及更为坚实的数学基础</li><li><a href="https://link.zhihu.com/?target=http%3A//rail.eecs.berkeley.edu/deeprlcourse/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Berkeley CS294</a> 课件可以在 <a href="https://link.zhihu.com/?target=https%3A//github.com/sweta20/deep-rl-course" class=" wrap external" target="_blank" rel="nofollow noreferrer">这里</a> 找到</li><li>我导师开设的 <a href="https://link.zhihu.com/?target=http%3A//iiis.tsinghua.edu.cn/~jianli/courses/ATCS2018spring/ATCS-2018s.htm" class=" wrap external" target="_blank" rel="nofollow noreferrer">高等理论计算机课程</a> 的最后两讲</li></ol><p></p></div></div><div class="ContentItem-time">编辑于 2019-03-31</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-bottom"><div class="ContentItem-actions"><span><button aria-label="赞同 50 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 50</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>10 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_xs.jpg" srcSet="https://pic2.zhimg.com/v2-5d1f04a84f6be8896c07a32a05e1d6d1_l.jpg 2x" alt="强化学习前沿"/></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="/reinforcementlearning"><div class="Popover"><div id="null-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="null-content">强化学习前沿</div></div></a></h2><div class="ContentItem-meta">读呀读paper</div></div><div class="ContentItem-extra"><a href="/reinforcementlearning" type="button" class="Button">进入专栏</a></div></div></div></ul></div></div></div></main></div></div><script nonce="00f25254-4836-4e84-a00c-b9cd1df81117" async="" src="https://www.googletagmanager.com/gtag/js?id=UA-149949619-1"></script><script nonce="00f25254-4836-4e84-a00c-b9cd1df81117">function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-149949619-1");</script><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{}},"entities":{"users":{"zhang-chu-heng":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}}},"questions":{},"answers":{},"articles":{"56045177":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":56045177,"title":"【强化学习入门 1】从零开始认识强化学习","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56045177","imageUrl":"","titleImage":"","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-32f24dca20ba92051b6938309fa26d75_200x112.png\" data-caption=\"图一：个体环境相互作用\" data-size=\"normal\" data-rawwidth=\"3299\" data-rawheight=\"899\" data-watermark=\"watermark\" data-original-src=\"v2-32f24dca20ba92051b6938309fa26d75\" data-watermark-src=\"v2-190bba94ea13917353464deb961b8b67\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-32f24dca20ba92051b6938309fa26d75_r.png\"\u002F\u003E\u003Cb\u003E目录\u003C\u002Fb\u003E强化学习基本概念马可夫决策模型价值函数Bellman算子Value Iteration和Policy Iteration动态规划方法蒙特卡洛方法\u003Cb\u003E引言\u003C\u002Fb\u003E强化学习学习的是一个策略，目前主要有三大类学习的框架，它们分别是\u003Cb\u003E\u003Ci\u003E策略迭代方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy iteration method）、\u003Cb\u003E\u003Ci\u003E策略梯度方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（polic…","created":1548863069,"updated":1554042933,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"清华大学 交叉信息院博士在读"}],"exposedMedal":{"medalId":"1124316222665379841","medalName":"我的知乎 2019","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2592b0b52e1fac99f69b38e00252413b_r.png","miniAvatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-ad363cc3088dc8de7544fd08b1c4987a_is.png","description":"参与「我的知乎 2019」即可获得"}},"commentPermission":"all","state":"published","imageWidth":0,"imageHeight":0,"content":"\u003Ch2\u003E\u003Cb\u003E目录\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli\u003E强化学习基本概念\u003C\u002Fli\u003E\u003Cli\u003E马可夫决策模型\u003C\u002Fli\u003E\u003Cli\u003E价值函数\u003C\u002Fli\u003E\u003Cli\u003EBellman算子\u003C\u002Fli\u003E\u003Cli\u003EValue Iteration和Policy Iteration\u003C\u002Fli\u003E\u003Cli\u003E动态规划方法\u003C\u002Fli\u003E\u003Cli\u003E蒙特卡洛方法\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E引言\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E强化学习学习的是一个策略，目前主要有三大类学习的框架，它们分别是\u003Cb\u003E\u003Ci\u003E策略迭代方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy iteration method）、\u003Cb\u003E\u003Ci\u003E策略梯度方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy gradient method）和\u003Cb\u003E\u003Ci\u003E无导数优化方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（derivative-free optimization method）。我们在这一讲里面主要介绍强化学习的基本概念，在接下来的几讲中，我们将分别介绍这几大类方法以及其中具有代表性的算法。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E强化学习基本概念\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E强化学习是机器学习领域的一类学习问题，它与常见的有监督学习、无监督学习等的最大不同之处在于，它是通过与环境之间的交互和反馈来学习的。正如一个新生的婴儿一样，它通过哭闹、吮吸、爬走等来对环境进行探索，并且慢慢地积累起对于环境的感知，从而一步步学习到环境的特性使得自己的行动能够尽快达成自己的愿望。再比如，这也同我们学习下围棋的模式类似，我们通过和不同的对手一盘一盘得对弈，慢慢积累起来我们对于每一步落子的判断，从而慢慢地提高自身的围棋水平。由DeepMind研发的AlphaGo围棋程序在训练学习的过程中就用到了强化学习的技术。\u003C\u002Fp\u003E\u003Cp\u003E下面让我们来正式地定义一下强化学习问题。强化学习的基本模型就是个体-环境的交互。\u003Cb\u003E\u003Ci\u003E个体\u002F智能体\u003C\u002Fi\u003E\u003C\u002Fb\u003E（agent）就是能够采取一系列行动并且期望获得较高收益或者达到某一目标的部分，比如我们前面例子中的新生婴儿或者在学习下围棋的玩家。而与此相关的另外的部分我们都统一称作\u003Cb\u003E\u003Ci\u003E环境\u003C\u002Fi\u003E\u003C\u002Fb\u003E（environment），比如前面例子中的婴儿的环境（比如包括其周围的房间以及婴儿的父母等）或者是你面前的棋盘以及对手。整个过程将其离散化为不同的时刻（time step）。在每个时刻环境和个体都会产生相应的交互。个体可以采取一定的\u003Cb\u003E\u003Ci\u003E行动\u003C\u002Fi\u003E\u003C\u002Fb\u003E（action），这样的行动是施加在环境中的。环境在接受到个体的行动之后，会反馈给个体环境目前的\u003Cb\u003E\u003Ci\u003E状态\u003C\u002Fi\u003E\u003C\u002Fb\u003E（state）以及由于上一个行动而产生的\u003Cb\u003E\u003Ci\u003E奖励\u003C\u002Fi\u003E\u003C\u002Fb\u003E（reward）。其中值得注意的一点是，这样个体-环境的划分并不一定是按照实体的临近关系划分，比如在动物行为学上，动物获得的奖励其实可能来自于其自身大脑中的化学物质的分泌，因此这时动物大脑中实现这一奖励机制的部分，也应该被划分为环境；而个体就仅仅只包括接受信号并且做出决定的部分。\u003C\u002Fp\u003E\u003Cp\u003E上面所描述的个体-环境相互作用可以使用图一中的示意图表示。存在一连串的时刻 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t%3D1%2C+2%2C+3+%5Ccdots\" alt=\"t=1, 2, 3 \\cdots\" eeimg=\"1\"\u002F\u003E ，在每一个时刻中，个体都会接受到环境的一个状态信号 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t+%5Cin+%5Cmathcal%7BS%7D\" alt=\"S_t \\in \\mathcal{S}\" eeimg=\"1\"\u002F\u003E ，在每一步中个体会从该状态允许的行动集中挑选一个来采取行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_t+%5Cin+%5Cmathcal%7BA%7D%28S_t%29\" alt=\"A_t \\in \\mathcal{A}(S_t)\" eeimg=\"1\"\u002F\u003E ，环境接受到这个行动信号之后，在下一个时刻环境会反馈给个体相应的状态信号 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_%7Bt%2B1%7D+%5Cin+%5Cmathcal%7BS%7D%5E%2B\" alt=\"S_{t+1} \\in \\mathcal{S}^+\" eeimg=\"1\"\u002F\u003E 和即时奖励 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_%7Bt%2B1%7D%5Cin+%5Cmathcal%7BR%7D+%5Csubset+%5Cmathbb%7BR%7D\" alt=\"R_{t+1}\\in \\mathcal{R} \\subset \\mathbb{R}\" eeimg=\"1\"\u002F\u003E 。其中，我们把所有的状态记做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BS%7D%5E%2B\" alt=\"\\mathcal{S}^+\" eeimg=\"1\"\u002F\u003E ，把非终止状态记做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BS%7D\" alt=\"\\mathcal{S}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-190bba94ea13917353464deb961b8b67_b.jpg\" data-size=\"normal\" data-rawwidth=\"3299\" data-rawheight=\"899\" class=\"origin_image zh-lightbox-thumb\" width=\"3299\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-190bba94ea13917353464deb961b8b67_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;3299&#39; height=&#39;899&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"3299\" data-rawheight=\"899\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3299\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-190bba94ea13917353464deb961b8b67_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-190bba94ea13917353464deb961b8b67_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图一：个体环境相互作用\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E强化学习的目标是希望个体从环境中获得的总奖励最大，即我们的目标不是短期的某一步行动之后获得最大的奖励，而是希望长期地获得更多的奖励。比如一个婴儿可能短期的偷吃了零食，获得了身体上的愉悦（即，获取了较大的短期奖励），但是这一行为可能再某一段时间之后会导致父母的批评，从而降低了长期来的总奖励。在很多常见的任务中，比如下围棋，在一局棋未结束的时候奖励常常都是为零的，而仅当棋局结束的那一个时刻才会根据个体的输赢产生一个奖励值；而在另外一些任务中，环境给予奖励可能分布在几乎每个时刻中。对于像下围棋这样存在一个终止状态，并且所有的奖励会在这个终止状态及其之前结算清的任务我们称之为\u003Cb\u003E\u003Ci\u003E回合制任务\u003C\u002Fi\u003E\u003C\u002Fb\u003E（episodic task）；还存在另外的一类任务，它们并不存在一个终止状态，即原则上它们可以永久地运行下去，这类任务的奖励是分散地分布在这个连续的一连串的时刻中的，我们称这一类任务为\u003Cb\u003E\u003Ci\u003E连续任务\u003C\u002Fi\u003E\u003C\u002Fb\u003E（continuing task）。由于我们的目标是希望获得的总奖励最大，因此我们希望量化地定义这个总奖励，这里我们称之为\u003Cb\u003E\u003Ci\u003E收益\u003C\u002Fi\u003E\u003C\u002Fb\u003E（return）。对于回合制任务而言，我们可以很直接定义收益为\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+R_%7Bt%2B2%7D+%2B+%5Ccdots+%2B+R_T\" alt=\"G_t = R_{t+1} + R_{t+2} + \\cdots + R_T\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 为回合结束的时刻，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_T\" alt=\"S_T\" eeimg=\"1\"\u002F\u003E 属于终止状态。对于连续任务而言，不存在一个这样的终止状态，因此，这样的定义可能会在连续任务中发散。因此我们引入另外一种收益的计算方式，称之为衰减收益（discounted return）。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%5Ccdots+%3D+%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D+%5Cgamma%5Ek+R_%7Bt%2Bk%2B1%7D\" alt=\"G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中衰减率（discount factor） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 满足 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=0+%5Cle+%5Cgamma+%5Cle+1\" alt=\"0 \\le \\gamma \\le 1\" eeimg=\"1\"\u002F\u003E 。这样的定义也很好理解，相比于更远的收益，我们会更加偏好临近的收益，因此对于离得较近的收益权重更高。\u003C\u002Fp\u003E\u003Cp\u003E强化学习的结果是就是这样一个行动决策，我们称\u003Cb\u003E\u003Ci\u003E策略\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy）。策略给出了在每个状态下我们应该采取的行动，我们可以把这个策略记做  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi+%28a+%7C+s%29\" alt=\"\\pi (a | s)\" eeimg=\"1\"\u002F\u003E  ，它表示在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下采取行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 的概率。\u003C\u002Fp\u003E\u003Cp\u003E如果我们可以计算出每个状态或者采取某个行动之后收益，那么我们每次行动就只需要采取收益较大的行动或者采取能够到达收益较大状态的行动。这也是策略迭代方法中的一个主要思路。在策略迭代方法中，通过对于和环境的交互，迭代地估计出到达某个状态或者采取某个行动对应的收益，同时更新策略与此匹配；通过多次迭代，当期望收益估计的越来越准的时候，策略也慢慢变好。那么自然就产生一些问题，通过迭代，是否能把对应的期望收益估计准确？当期望收益估计准确的时候相应的策略是否为最优？这些问题将在下面引入更为正式的表述之后给出解答。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E马可夫决策过程\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E我们从大家熟悉的马可夫链说起，对于一个离散有限的状态空间 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BS%7D%5E%2B\" alt=\"\\mathcal{S}^+\" eeimg=\"1\"\u002F\u003E 中的每一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S\" alt=\"S\" eeimg=\"1\"\u002F\u003E ，它转移到其他状态的概率只取决于该状态本身，即所说的马可夫性，即\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Pr%5BS_%7Bt%2B1%7D+%7C+S_0%2C+S_1%2C%5Ccdots+%2C+S_t%5D+%3D+Pr%5BS_%7Bt%2B1%7D+%7C+S_t%5D\" alt=\"Pr[S_{t+1} | S_0, S_1,\\cdots , S_t] = Pr[S_{t+1} | S_t]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E马可夫决策过程\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Markov decision process, MDP）与马可夫链的区别就在于，在每个状态下，可以由个体从可能的行动空间 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BA%7D%28s%29\" alt=\"\\mathcal{A}(s)\" eeimg=\"1\"\u002F\u003E 中选择一个行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E ，紧接着的状态转移概率随着所选择行动的不同而不同。另外，我们再加入即时奖励，就可以得到MDP的动力学特征 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s%27%2C+r+%7C+s%2C+a%29+%3D+Pr%5BS_%7Bt%2B1%7D+%3D+s%27%2C+R_%7Bt%2B1%7D+%3D+r+%7C+S_t%3Ds%2C+A_t%3Da%5D+%3D+Pr%5BS_%7Bt%2B1%7D+%3D+s%27%2C+R_%7Bt%2B1%7D+%3D+r+%7C+S_0%2C+A_0%2C+S_1%2C+A_1%2C+%5Ccdots%2C+S_t%2C+A_t%5C%7D\" alt=\"p(s&#39;, r | s, a) = Pr[S_{t+1} = s&#39;, R_{t+1} = r | S_t=s, A_t=a] = Pr[S_{t+1} = s&#39;, R_{t+1} = r | S_0, A_0, S_1, A_1, \\cdots, S_t, A_t\\}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E我们注意到，马可夫性使得我们可以仅仅利用当前状态来估计接下来的收益，即仅仅使用当前状态来估计的策略并不比使用所有历史的策略差。可以说马可夫性带给我们了极大的计算上的便利，我们不用每一步都去处理所有的历史步骤，而是只需要面对当前的状态来进行处理。同时注意到，可能有些信息并没有被完整的包含到模型的状态信号 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"\u002F\u003E 中，这使得模型并不满足马可夫性。不过我们一般还是认为它们是\u003Cb\u003E\u003Ci\u003E近似地\u003C\u002Fi\u003E\u003C\u002Fb\u003E满足马可夫性的，因此仅仅使用当前状态的信息来做出对于未来收益的预测和行动的选择并不是很差的策略。同时，如果选取不仅仅是当前时刻的状态状态，而是之前的多个时刻的状态叠加在一起作为当前状态，一般可以增强马可夫性。\u003C\u002Fp\u003E\u003Cp\u003E关于MDP更为形式化的定义是一个五元组 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Cmathcal%7BS%7D%2C+%5Cmathcal%7BA%7D%2C+P%2C+R%2C+%5Cgamma%29\" alt=\"(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)\" eeimg=\"1\"\u002F\u003E ，而提到的MDP动力学特征 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s%27%2C+r+%7C+s%2C+a%29\" alt=\"p(s&#39;, r | s, a)\" eeimg=\"1\"\u002F\u003E 反映了这个MDP问题中的所有环境因素。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BS%7D\" alt=\"\\mathcal{S}\" eeimg=\"1\"\u002F\u003E 表示状态空间，在\u003Cb\u003E\u003Ci\u003E有限状态的马可夫决策过程\u003C\u002Fi\u003E\u003C\u002Fb\u003E（finite MDP）中，该集合元素格式是有限的；\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BA%7D%28S%29\" alt=\"\\mathcal{A}(S)\" eeimg=\"1\"\u002F\u003E 表示处于状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S\" alt=\"S\" eeimg=\"1\"\u002F\u003E 下时，可能的行动空间，在有限状态的马可夫决策过程中，每一个集合元素个数都是有限的；\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28s+%27%7C+s%2C+a%29\" alt=\"P(s &#39;| s, a)\" eeimg=\"1\"\u002F\u003E 表示在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下，采取行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a%5Cin+%5Cmathcal%7BA%7D%28s%29\" alt=\"a\\in \\mathcal{A}(s)\" eeimg=\"1\"\u002F\u003E 时跳转到状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s%27\" alt=\"s&#39;\" eeimg=\"1\"\u002F\u003E 的概率，有关系$ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28s%27%7Cs%2C+a%29+%3D+Pr%5BS_%7Bt%2B1%7D%3Ds%27+%7C+S_t+%3D+s%2C+A_t+%3D+a%5D+%3D+%5Csum_%7Br%5Cin%5Cmathbb%7BR%7D%7D+p%28s%27%2C+r+%7C+s%2C+a%29\" alt=\"P(s&#39;|s, a) = Pr[S_{t+1}=s&#39; | S_t = s, A_t = a] = \\sum_{r\\in\\mathbb{R}} p(s&#39;, r | s, a)\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28s%27%2C+s%2C+a%29\" alt=\"R(s&#39;, s, a)\" eeimg=\"1\"\u002F\u003E 表示在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下，采取行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a%5Cin+%5Cmathcal%7BA%7D%28s%29\" alt=\"a\\in \\mathcal{A}(s)\" eeimg=\"1\"\u002F\u003E 并跳转到状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s%27\" alt=\"s&#39;\" eeimg=\"1\"\u002F\u003E 时取得的即时奖励，有关系 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28s%27%2C+s%2C+a%29+%3D+%5Cmathbb%7BE%7D%5BR_%7Bt%2B1%7D+%7C+S_t+%3D+s%2C+A_t+%3D+a%2C+S_%7Bt%2B1%7D%3Ds%27%5D+%3D+%5Cdfrac%7B%5Csum_%7Br%5Cin%5Cmathbb%7BR%7D%7D+r+%5C%2C+p%28s%27%2C+r+%7Cs%2C+a%29%7D%7Bp%28s%27+%7Cs%2C+a%29%7D\" alt=\"R(s&#39;, s, a) = \\mathbb{E}[R_{t+1} | S_t = s, A_t = a, S_{t+1}=s&#39;] = \\dfrac{\\sum_{r\\in\\mathbb{R}} r \\, p(s&#39;, r |s, a)}{p(s&#39; |s, a)}\" eeimg=\"1\"\u002F\u003E ；类似地，我们还可以得到在状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 下，采取行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a%5Cin+%5Cmathcal%7BA%7D%28s%29\" alt=\"a\\in \\mathcal{A}(s)\" eeimg=\"1\"\u002F\u003E 后的期望即时奖励， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D%5BR_%7Bt%2B1%7D+%7C+S_t+%3D+s%2C+A_t+%3D+a%5D+%3D+%5Csum_%7Br%5Cin%5Cmathbb%7BR%7D%7D+r+%5Csum_%7Bs%27%5Cin%5Cmathcal%7BS%7D%7D+p%28s%27%2C+r+%7C+s%2C+a%29\" alt=\"R(s, a) = \\mathbb{E}[R_{t+1} | S_t = s, A_t = a] = \\sum_{r\\in\\mathbb{R}} r \\sum_{s&#39;\\in\\mathcal{S}} p(s&#39;, r | s, a)\" eeimg=\"1\"\u002F\u003E ；\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma%5Cin+%5B0%2C1%5D\" alt=\"\\gamma\\in [0,1]\" eeimg=\"1\"\u002F\u003E 表示衰减率，它指征了该MDP的目标为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax_%5Cpi+%5Cmathbb%7BE%7D_%7B%5Cpi%2C+P%2C+R%7D%5B%5Csum_%7Bt%3D1%7D%5ET+%5Cgamma%5Et+R_t%5D\" alt=\"\\max_\\pi \\mathbb{E}_{\\pi, P, R}[\\sum_{t=1}^T \\gamma^t R_t]\" eeimg=\"1\"\u002F\u003E ，其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_%7Bt%2B1%7D+%3D+R%28S_%7Bt%2B1%7D%2C+S_t%2C+A_t%29\" alt=\"R_{t+1} = R(S_{t+1}, S_t, A_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E在下面的几个部分中，我们讨论的MDP都是有限状态的马可夫决策过程。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E价值函数\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E接下来我们介绍强化学习领域非常重要的两个函数\u003Cb\u003E\u003Ci\u003E状态价值函数\u003C\u002Fi\u003E\u003C\u002Fb\u003E（state value function） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Cpi%28s%29\" alt=\"V_\\pi(s)\" eeimg=\"1\"\u002F\u003E 和\u003Cb\u003E\u003Ci\u003E行动价值函数\u003C\u002Fi\u003E\u003C\u002Fb\u003E（action value function） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2C+a%29\" alt=\"Q_\\pi(s, a)\" eeimg=\"1\"\u002F\u003E ，其中函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Cpi%28s%29\" alt=\"V_\\pi(s)\" eeimg=\"1\"\u002F\u003E 表示的是当到达某个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 之后，如果接下来一直按着策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 来行动，能够获得的期望收益；函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2C+a%29\" alt=\"Q_\\pi(s, a)\" eeimg=\"1\"\u002F\u003E 表示的是当达到某个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E 之后，如果采取行动 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E ，接下来再按照策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 来行动，能获得的期望收益。它们的具体定义如下。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Cpi%28s%29+%3D+%5Cmathbb%7BE%7D_%5Cpi%5BG_0+%7C+S_0+%3D+s%5D+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B1%7D+%7C+S_0+%3D+s%5D\" alt=\"V_\\pi(s) = \\mathbb{E}_\\pi[G_0 | S_0 = s] = \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t R_{t+1} | S_0 = s]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2C+a%29+%3D+%5Cmathbb%7BE%7D_%5Cpi+%5BG_0+%7C+S_0+%3D+s%2C+A_0+%3D+a%5D+%3D+%5Cmathbb%7BE%7D%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B1%7D+%7C+S_0+%3D+s%2C+A_0+%3D+a%5D\" alt=\"Q_\\pi(s, a) = \\mathbb{E}_\\pi [G_0 | S_0 = s, A_0 = a] = \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t R_{t+1} | S_0 = s, A_0 = a]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E递归地展开上面这两个公式即可得到相应的两个\u003Cb\u003E\u003Ci\u003EBellman方程\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Bellman equation），比如对于上述关于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Cpi%28s%29\" alt=\"V_\\pi(s)\" eeimg=\"1\"\u002F\u003E 的式子，有\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+V_%5Cpi%28s%29+%26+%3D+%5Cmathbb%7BE%7D_%5Cpi%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B1%7D+%7C+S_0+%3D+s%5D%5C%5C+%26%3D+%5Cmathbb%7BE%7D_%5Cpi%5BR_%7Bt%2B1%7D+%2B+%5Cgamma+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B2%7D+%7C+S_1+%3D+s%5D+%5C%5C+%26%3D+%5Csum_a+%5Cpi%28a%7Cs%29+%5Csum_%7Bs%27%7D+%5Csum_r+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+%5Cmathbb%7BE%7D_%5Cpi%5B%5Csum_%7Bt%3D0%7D%5E%5Cinfty+%5Cgamma%5Et+R_%7Bt%2B2%7D+%7C+S_%7B1%7D+%3D+s%27%5D%5D+%5C%5C+%26%3D+%5Csum_a+%5Cpi%28a%7Cs%29+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+V_%5Cpi%28s%27%29%5D+%5C%5C+%26+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} V_\\pi(s) &amp; = \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t R_{t+1} | S_0 = s]\\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^t R_{t+2} | S_1 = s] \\\\ &amp;= \\sum_a \\pi(a|s) \\sum_{s&#39;} \\sum_r p(s&#39;, r|s, a) [r + \\gamma \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t R_{t+2} | S_{1} = s&#39;]] \\\\ &amp;= \\sum_a \\pi(a|s) \\sum_{s&#39;, r} p(s&#39;, r|s, a) [r + \\gamma V_\\pi(s&#39;)] \\\\ &amp; \\forall s \\in \\mathcal{S} \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E这就是关于状态价值函数的Bellman方程，它描述的是当前状态的价值函数和它后续状态价值函数之间的递归关系。同理可以得到关于行动价值函数的Bellman方程。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%5Cpi%28s%2C+a%29+%3D+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+%5Csum_%7Ba%27%7D+%5Cpi%28a%27%7Cs%27%29+Q_%5Cpi%28s%27%2C+a%27%29%5D\" alt=\"Q_\\pi(s, a) = \\sum_{s&#39;, r} p(s&#39;, r|s, a) [r + \\gamma \\sum_{a&#39;} \\pi(a&#39;|s&#39;) Q_\\pi(s&#39;, a&#39;)]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E那么对于一个有限状态的马可夫决策过程来说，是否存在一个最优的策略呢？如果一个策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E 在任何状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s+%5Cin+%5Cmathcal%7BS%7D\" alt=\"s \\in \\mathcal{S}\" eeimg=\"1\"\u002F\u003E 下都有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%7B%5Cpi%7D%28s%29+%5Cle+V_%7B%5Cpi%E2%80%98%7D%28s%29\" alt=\"V_{\\pi}(s) \\le V_{\\pi‘}(s)\" eeimg=\"1\"\u002F\u003E ，那么我们称策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%27\" alt=\"\\pi&#39;\" eeimg=\"1\"\u002F\u003E 优于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 。对于所有的策略排序，我们总可以找到最优策略（有可能不止一个最优策略），我们记最优策略为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%5E%2A\" alt=\"\\pi^*\" eeimg=\"1\"\u002F\u003E ，最优策略具有如下性质\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+V%5E%2A%28s%29+%26%3D+%5Cmax_%5Cpi+V_%5Cpi%28s%29+%5C%5C+Q%5E%2A%28s%2C+a%29+%26%3D+%5Cmax_%5Cpi+Q_%5Cpi%28s%2C+a%29+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} V^*(s) &amp;= \\max_\\pi V_\\pi(s) \\\\ Q^*(s, a) &amp;= \\max_\\pi Q_\\pi(s, a) \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E使用上述类似的方法，我们可以得到\u003Cb\u003E\u003Ci\u003E最优情况下的Bellman方程\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Bellman optimality equation）\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+V%5E%2A%28s%29+%26%3D+%5Cmax_%7Ba%5Cin%5Cmathcal%7BA%7D%28s%29%7D+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29%5Br+%2B+%5Cgamma+V%5E%2A%28s%27%29%5D+%5C%5C+Q%5E%2A%28s%2C+a%29+%26%3D+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29+%5Br+%2B+%5Cgamma+%5Cmax_%7Ba%27%7D+Q%5E%2A%28s%27%2C+a%27%29%5D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} V^*(s) &amp;= \\max_{a\\in\\mathcal{A}(s)} \\sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \\gamma V^*(s&#39;)] \\\\ Q^*(s, a) &amp;= \\sum_{s&#39;, r} p(s&#39;, r|s, a) [r + \\gamma \\max_{a&#39;} Q^*(s&#39;, a&#39;)] \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E显然地，状态价值函数和行动价值函数之间有关系 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Cpi%28s%29+%3D+%5Csum_%7Ba%7D+%5Cpi%28a%7Cs%29+Q_%5Cpi%28s%2C+a%29\" alt=\"V_\\pi(s) = \\sum_{a} \\pi(a|s) Q_\\pi(s, a)\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%2A%28s%29+%3D+%5Cmax_%7Ba%7D+Q%5E%2A%28s%2C+a%29\" alt=\"V^*(s) = \\max_{a} Q^*(s, a)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003EBellman算子\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E在前一节中我们定义了两种价值函数，如果能够求得在某策略下的价值函数，我们就可以对该策略的好坏进行评估；如果能够求得最优情形下的价值函数，我们就可以自然得出最优策略。但我们还不知道要如何计算得到相应的价值函数数值，我们这里引入的\u003Cb\u003E\u003Ci\u003EBellman算子\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Bellman operator\u002FBellman backup operator）及其相关的性质，可以为相关算法的收敛性提供一些保证。\u003C\u002Fp\u003E\u003Cp\u003E考虑有限的状态空间 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BS%7D%3D%5Bn%5D\" alt=\"\\mathcal{S}=[n]\" eeimg=\"1\"\u002F\u003E 和一个定义在状态集上的实值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E ，状态空间给定的时候该实值函数也可以写成向量的形式 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5Cin%5Cmathbb%7BR%7D%5En\" alt=\"V\\in\\mathbb{R}^n\" eeimg=\"1\"\u002F\u003E 。定义Bellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D%3A+%5Cmathbb%7BR%7D%5En%5Cto+%5Cmathbb%7BR%7D%5En\" alt=\"\\mathcal{T}: \\mathbb{R}^n\\to \\mathbb{R}^n\" eeimg=\"1\"\u002F\u003E ，\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Cmathcal%7BT%7D+V%29_i+%3D+%5Cmax_%7Ba%5Cin%5Cmathcal%7BA%7D%28i%29%7D+%5Csum_%7Bj%5Cin+%5Bn%5D%7D+P%28j%7Ci%2Ca%29+%28R%28j%2C+i%2C+a%29+%2B+%5Cgamma+V_j%29\" alt=\"(\\mathcal{T} V)_i = \\max_{a\\in\\mathcal{A}(i)} \\sum_{j\\in [n]} P(j|i,a) (R(j, i, a) + \\gamma V_j)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中下标都表示向量的某个元素。对于一个策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 还可以定义Bellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D_%5Cpi%3A+%5Cmathbb%7BR%7D%5En%5Cto+%5Cmathbb%7BR%7D%5En\" alt=\"\\mathcal{T}_\\pi: \\mathbb{R}^n\\to \\mathbb{R}^n\" eeimg=\"1\"\u002F\u003E ，\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Cmathcal%7BT%7D_%5Cpi+V%29_i+%3D+%5Csum_%7Bj%5Cin+%5Bn%5D%7D+P%28j%7Ci%2C%5Cpi%29+%28R%28j%2C+i%2C+%5Cpi%29+%2B+%5Cgamma+V_j%29\" alt=\"(\\mathcal{T}_\\pi V)_i = \\sum_{j\\in [n]} P(j|i,\\pi) (R(j, i, \\pi) + \\gamma V_j)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E对于确定性的策略来说里面的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 就是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%28i%29\" alt=\"\\pi(i)\" eeimg=\"1\"\u002F\u003E ；对于随机策略来说，还需要在外面对策略给出的行动求期望，这里将其省略。\u003C\u002Fp\u003E\u003Cp\u003EBellman算子具有以下性质：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003EBellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D\" alt=\"\\mathcal{T}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D_%5Cpi\" alt=\"\\mathcal{T}_\\pi\" eeimg=\"1\"\u002F\u003E 在max norm下都是按系数（modulus） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 收缩的（contraction），即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5ClVert+%5Cmathcal%7BT%7D+V+-+%5Cmathcal%7BT%7D+U%5CrVert_%7B%5Cinfty%7D+%5Cle+%5Cgamma+%5ClVert+V+-+U%5CrVert_%7B%5Cinfty%7D\" alt=\"\\lVert \\mathcal{T} V - \\mathcal{T} U\\rVert_{\\infty} \\le \\gamma \\lVert V - U\\rVert_{\\infty}\" eeimg=\"1\"\u002F\u003E。 \u003C\u002Fli\u003E\u003Cli\u003EBellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D\" alt=\"\\mathcal{T}\" eeimg=\"1\"\u002F\u003E 具有唯一不动点 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%2A\" alt=\"V^*\" eeimg=\"1\"\u002F\u003E ，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7DV%5E%2A+%3D+V%5E%2A\" alt=\"\\mathcal{T}V^* = V^*\" eeimg=\"1\"\u002F\u003E ；Bellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D_%5Cpi\" alt=\"\\mathcal{T}_\\pi\" eeimg=\"1\"\u002F\u003E 也具有唯一不动点 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Cpi\" alt=\"V_\\pi\" eeimg=\"1\"\u002F\u003E ，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D_%5Cpi+V_%5Cpi+%3D+V_%5Cpi\" alt=\"\\mathcal{T}_\\pi V_\\pi = V_\\pi\" eeimg=\"1\"\u002F\u003E ，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%2A\" alt=\"V^*\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D_%5Cpi\" alt=\"\\mathcal{T}_\\pi\" eeimg=\"1\"\u002F\u003E 的定义与前一节相同。前一个性质加上Banach fixed point theorem可以推导到此性质。\u003C\u002Fli\u003E\u003Cli\u003EBellman算子 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D\" alt=\"\\mathcal{T}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D_%5Cpi\" alt=\"\\mathcal{T}_\\pi\" eeimg=\"1\"\u002F\u003E 单调（monotonic），即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5Cle+%5Cmathcal%7BT%7D+V+%5CRightarrow+V+%5Cle+%5Cmathcal%7BT%7D+V+%5Cle+%5Cmathcal%7BT%7D%5E2+V+%5Cle+%5Ccdots+%5Cle+V%5E%2A\" alt=\"V\\le \\mathcal{T} V \\Rightarrow V \\le \\mathcal{T} V \\le \\mathcal{T}^2 V \\le \\cdots \\le V^*\" eeimg=\"1\"\u002F\u003E ，其中对于向量来说 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cle\" alt=\"\\le\" eeimg=\"1\"\u002F\u003E 表示每一个元素都小于等于。\u003C\u002Fli\u003E\u003Cli\u003E当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7DV+%3D+V\" alt=\"\\mathcal{T}V = V\" eeimg=\"1\"\u002F\u003E 时， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V+%3D+V%5E%2A\" alt=\"V = V^*\" eeimg=\"1\"\u002F\u003E ；当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7D_%5Cpi+V+%3D+V\" alt=\"\\mathcal{T}_\\pi V = V\" eeimg=\"1\"\u002F\u003E 时， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V+%3D+V_%5Cpi\" alt=\"V = V_\\pi\" eeimg=\"1\"\u002F\u003E ；当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BT%7DV+%3D+%5Cmathcal%7BT%7D_%5Cpi+V+%3D+V\" alt=\"\\mathcal{T}V = \\mathcal{T}_\\pi V = V\" eeimg=\"1\"\u002F\u003E 时， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 是最优策略。\u003C\u002Fli\u003E\u003Cli\u003E对任意的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5Cin+%5Cmathbb%7BR%7D%5En\" alt=\"V\\in \\mathbb{R}^n\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clim_%7Bk%5Cto+%5Cinfty%7D+%5Cmathcal%7BT%7D%5Ek+V+%3D+V%5E%2A\" alt=\"\\lim_{k\\to \\infty} \\mathcal{T}^k V = V^*\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003EValue Iteration和Policy Iteration\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E给出两种找到最优值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%2A\" alt=\"V^*\" eeimg=\"1\"\u002F\u003E 的算法，分别是\u003Cb\u003E\u003Ci\u003E值迭代\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Value Iteration，VI）和\u003Cb\u003E\u003Ci\u003E策略迭代\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Policy Iteration，PI），最后给出\u003Cb\u003E\u003Ci\u003E改进的策略迭代\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Modified Policy Iteration）是这两者的统一。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%B8%80%EF%BC%9AValue+Iteration%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+V%5E%7B%280%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Bfor+%7D+t%3D0%2C1%2C%5Ccdots%2C+T+%5Ctext%7B+do%7D+%5C%5C+3%5Cquad+%26+%5Cquad+V%5E%7B%28t%2B1%29%7D+%5Cleftarrow+%5Cmathcal%7BT%7D+V%5E%7B%28t%29%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法一：Value Iteration} \\\\ 1\\quad &amp; \\text{initialize } V^{(0)} \\\\ 2\\quad &amp; \\text{for } t=0,1,\\cdots, T \\text{ do} \\\\ 3\\quad &amp; \\quad V^{(t+1)} \\leftarrow \\mathcal{T} V^{(t)} \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E值迭代有如下性质：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E如果初始化的时候 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%7B%280%29%7D\" alt=\"V^{(0)}\" eeimg=\"1\"\u002F\u003E 的每一个元素都置0，那么有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%7B%280%29%7D+%5Cle+V%5E%7B%281%29%7D+%5Cle+%5Ccdots\" alt=\"V^{(0)} \\le V^{(1)} \\le \\cdots\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5ClVert+V%5E%7B%28n%29%7D+-+V%5E%2A%5CrVert+_%5Cinfty+%5Cle+%5Cgamma%5En+%5ClVert+R+%5CrVert+_%5Cinfty+%2F+%281-%5Cgamma%29\" alt=\"\\lVert V^{(n)} - V^*\\rVert _\\infty \\le \\gamma^n \\lVert R \\rVert _\\infty \u002F (1-\\gamma)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003Cli\u003E在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T%5Cto+%5Cinfty\" alt=\"T\\to \\infty\" eeimg=\"1\"\u002F\u003E 的时候，能够得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%3DV%5E%2A\" alt=\"V=V^*\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%BA%8C%EF%BC%9APolicy+Iteration%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+V%5E%7B%280%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Bfor+%7D+t%3D0%2C1%2C%5Ccdots%2C+T+%5Ctext%7B+do%7D+%5C%5C+3%5Cquad+%26+%5Cquad+%5Ctext%7Bfind+%7D+%5Cpi_t%2C+%5Ctext%7B+s.t.+%7D+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D+V%5E%7B%28t%29%7D+%3D+%5Cmathcal%7BT%7D+V%5E%7B%28t%29%7D+%5Ctext%7B+%28i.e+greedy+policy%29%7D+%5C%5C+4%5Cquad+%26+%5Cquad+V%5E%7B%28t%2B1%29%7D+%5Cleftarrow+%5Clim_%7Bk%5Cto+%5Cinfty%7D+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D%5Ek+V%5E%7B%28t%29%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法二：Policy Iteration} \\\\ 1\\quad &amp; \\text{initialize } V^{(0)} \\\\ 2\\quad &amp; \\text{for } t=0,1,\\cdots, T \\text{ do} \\\\ 3\\quad &amp; \\quad \\text{find } \\pi_t, \\text{ s.t. } \\mathcal{T}_{\\pi_t} V^{(t)} = \\mathcal{T} V^{(t)} \\text{ (i.e greedy policy)} \\\\ 4\\quad &amp; \\quad V^{(t+1)} \\leftarrow \\lim_{k\\to \\infty} \\mathcal{T}_{\\pi_t}^k V^{(t)} \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E策略迭代分为两个步骤交替进行：一步称作\u003Cb\u003E\u003Ci\u003E策略改进\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy improvement），即固定价值函数，并且找到在该价值函数下的最优策略（第3行）；另一步称作\u003Cb\u003E\u003Ci\u003E策略估计\u003C\u002Fi\u003E\u003C\u002Fb\u003E（policy evaluation），即固定策略，找出该策略下的价值函数（第4行）。这样的更新方式同样具有类似值循环的收敛和界的性质。这里出现的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clim_%7Bk%5Cto+%5Cinfty%7D\" alt=\"\\lim_{k\\to \\infty}\" eeimg=\"1\"\u002F\u003E 在实际操作中通常是循环直到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5ClVert+V%5CrVert_%5Cinfty\" alt=\"\\lVert V\\rVert_\\infty\" eeimg=\"1\"\u002F\u003E 小于某个设定的小量。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%B8%89%EF%BC%9AModified+Policy+Iteration%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+V%5E%7B%280%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Bfor+%7D+t%3D0%2C1%2C%5Ccdots%2C+T+%5Ctext%7B+do%7D+%5C%5C+3%5Cquad+%26+%5Cquad+%5Ctext%7Bfind+%7D+%5Cpi_t%2C+%5Ctext%7B+s.t.+%7D+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D+V%5E%7B%28t%29%7D+%3D+%5Cmathcal%7BT%7D+V%5E%7B%28t%29%7D+%5Ctext%7B+%28i.e+greedy+policy%29%7D+%5C%5C+4%5Cquad+%26+%5Cquad+V%5E%7B%28t%2B1%29%7D+%5Cleftarrow+%5Cmathcal%7BT%7D_%7B%5Cpi_t%7D%5E%7Bm_t%7D+V%5E%7B%28t%29%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法三：Modified Policy Iteration} \\\\ 1\\quad &amp; \\text{initialize } V^{(0)} \\\\ 2\\quad &amp; \\text{for } t=0,1,\\cdots, T \\text{ do} \\\\ 3\\quad &amp; \\quad \\text{find } \\pi_t, \\text{ s.t. } \\mathcal{T}_{\\pi_t} V^{(t)} = \\mathcal{T} V^{(t)} \\text{ (i.e greedy policy)} \\\\ 4\\quad &amp; \\quad V^{(t+1)} \\leftarrow \\mathcal{T}_{\\pi_t}^{m_t} V^{(t)} \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E改进的策略迭代是对于上面两种算法的统一：当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m_t%3D1\" alt=\"m_t=1\" eeimg=\"1\"\u002F\u003E 时，该算法就是值循环；当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m_t%3D%5Cinfty\" alt=\"m_t=\\infty\" eeimg=\"1\"\u002F\u003E 时，该算法就是策略迭代。该算法的收敛性能够由Bellman算子的性质保证。值迭代也可以看做策略迭代的策略评价步只进行了一次迭代，还没有完全收敛到该策略的价值函数的时候，又进行了下一步的策略改进步。实际上，这并不影响算法的收敛，并且常常会更快；更进一步，在第3、4行的循环中，甚至不需要每次都更新所有的状态（即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E 的所有位置），该算法同样有效。这类交替甚至异步进行估计 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_%5Cpi\" alt=\"V_\\pi\" eeimg=\"1\"\u002F\u003E 、改进 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 的算法思想，我们称之为\u003Cb\u003E\u003Ci\u003E广义策略迭代\u003C\u002Fi\u003E\u003C\u002Fb\u003E（generalized policy iteration）。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E动态规划方法\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E\u003Cb\u003E\u003Ci\u003E动态规划\u003C\u002Fi\u003E\u003C\u002Fb\u003E（dynamic programming, DP）算法其实就是直接从前面的值循环得到的，得到最优情形下的状态价值函数之后，输出相应的最优策略。下面给出动态规划算法。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E5%9B%9B%EF%BC%9ADynamic+Programming%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+array+V+arbitrarily+%28eg.+%7D+V%28s%29%3D0%2C+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%5E%2B+%5Ctext%7B%29%7D+%5C%5C+2%5Cquad+%26+%5Ctext%7Brepeat%7D+%5C%5C+3%5Cquad+%26+%5Cquad+%5CDelta+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+each+%7D+s+%5Cin+%5Cmathcal%7BS%7D+%5C%5C+5%5Cquad+%26+%5Cquad+%5Cquad+v+%5Cleftarrow+V%28s%29+%5C%5C+6%5Cquad+%26+%5Cquad+%5Cquad+V%28s%29+%5Cleftarrow+%5Cmax_a+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29%5Br+%2B+%5Cgamma+V%28s%27%29%5D+%5C%5C+7%5Cquad+%26+%5Cquad+%5Cquad+%5CDelta+%5Cleftarrow+%5Cmax%28%5CDelta%2C+%7Cv+-+V%28s%29%7C%29+%5C%5C+8%5Cquad+%26+%5Ctext%7Buntil+%7D+%5CDelta+%3C+%5Ctheta+%5Ctext%7B+%28a+small+positive+number%29%7D+%5C%5C+9%5Cquad+%26+%5Ctext%7Boutput+a+deterministic+policy%2C+%7D+%5Cpi+%5Capprox+%5Cpi_%2A+%5Ctext%7B%2C+such+that%7D+%5C%5C+10%5Cquad+%26+%5Cpi%28s%29+%3D+arg%5Cmax_a+%5Csum_%7Bs%27%2C+r%7D+p%28s%27%2C+r%7Cs%2C+a%29%5Br+%2B+%5Cgamma+V%28s%27%29%5D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法四：Dynamic Programming} \\\\ 1\\quad &amp; \\text{initialize array V arbitrarily (eg. } V(s)=0, \\forall s \\in \\mathcal{S}^+ \\text{)} \\\\ 2\\quad &amp; \\text{repeat} \\\\ 3\\quad &amp; \\quad \\Delta \\leftarrow 0 \\\\ 4\\quad &amp; \\quad \\text{for each } s \\in \\mathcal{S} \\\\ 5\\quad &amp; \\quad \\quad v \\leftarrow V(s) \\\\ 6\\quad &amp; \\quad \\quad V(s) \\leftarrow \\max_a \\sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \\gamma V(s&#39;)] \\\\ 7\\quad &amp; \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v - V(s)|) \\\\ 8\\quad &amp; \\text{until } \\Delta &lt; \\theta \\text{ (a small positive number)} \\\\ 9\\quad &amp; \\text{output a deterministic policy, } \\pi \\approx \\pi_* \\text{, such that} \\\\ 10\\quad &amp; \\pi(s) = arg\\max_a \\sum_{s&#39;, r} p(s&#39;, r|s, a)[r + \\gamma V(s&#39;)] \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E动态规划算法也可以看做把前面推导的Bellman方程化为了增量的形式进行计算，当算法收敛到不动点的时候，这时得到的状态价值函数就满足最优情形下的Bellman方程，对应的贪心策略就是最优策略。其中，核心的算法在第6行，它迭代地将状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 逼近最优状态价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%5E%2A%28s%29\" alt=\"V^*(s)\" eeimg=\"1\"\u002F\u003E 。算法的循环中没有显式地定义每一步对应的策略，我们可以对于每一步迭代的价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 都定义相应的贪心策略。利用以下定理，我们可以知道，经历每一步的迭代，之后策略都不比之前的策略差；从而当算法收敛的时候，我们可以得到最优策略。 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7BPolicy+Improvement+Theorem%7D+%5C%5C+%5Cquad+%26+%5Ctext%7B%E5%A6%82%E6%9E%9C%E5%AD%98%E5%9C%A8%E4%B8%80%E5%AF%B9%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E7%AD%96%E7%95%A5%7D+%5Cpi+%5Ctext%7B%E5%92%8C%7D+%5Cpi%27+%5Ctext%7B%EF%BC%8C%E4%BD%BF%E5%BE%97%7D+q_%5Cpi%28s%2C+%5Cpi%27%29+%5Cge+v_%5Cpi+%28s%29%2C+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D+%5Ctext%7B%EF%BC%8C%E9%82%A3%E4%B9%88%7D+%5Cpi%27+%5Ctext%7B%E4%B8%8D%E6%AF%94%7D+%5Cpi+%5Ctext%7B%E5%B7%AE%E3%80%82%7D%5C%5C+%5Cquad+%26+%5Ctext%7B%E5%A6%82%E6%9E%9C%E5%AD%98%E5%9C%A8%E4%B8%80%E5%AF%B9%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E7%AD%96%E7%95%A5%7D+%5Cpi+%5Ctext%7B%E5%92%8C%7D+%5Cpi%27+%5Ctext%7B%EF%BC%8C%E4%BD%BF%E5%BE%97%7D+v_%5Cpi%27%28s%29+%5Cge+v_%5Cpi+%28s%29%2C+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D+%5Ctext%7B%EF%BC%8C%E9%82%A3%E4%B9%88%7D+%5Cpi%27+%5Ctext%7B%E4%B8%8D%E6%AF%94%7D+%5Cpi+%5Ctext%7B%E5%B7%AE%E3%80%82%7D%5C%5C+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{Policy Improvement Theorem} \\\\ \\quad &amp; \\text{如果存在一对确定性的策略} \\pi \\text{和} \\pi&#39; \\text{，使得} q_\\pi(s, \\pi&#39;) \\ge v_\\pi (s), \\forall s \\in \\mathcal{S} \\text{，那么} \\pi&#39; \\text{不比} \\pi \\text{差。}\\\\ \\quad &amp; \\text{如果存在一对确定性的策略} \\pi \\text{和} \\pi&#39; \\text{，使得} v_\\pi&#39;(s) \\ge v_\\pi (s), \\forall s \\in \\mathcal{S} \\text{，那么} \\pi&#39; \\text{不比} \\pi \\text{差。}\\\\ \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E上述算法的第6行中可以看到， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 是单调不减的，因此，其对应的策略也应该是单调提升的。\u003C\u002Fp\u003E\u003Cp\u003E从第5行-第7行可以看出，在每一轮迭代中，都需要对于状态空间中所有的状态访问一次。当状态空间庞大的时候，这样的循环会十分消耗时间。可以采用异步DP的算法，每轮迭代仅更新一部分的状态，只要保证在多轮迭代中，每个状态都能被访问到，这样的异步DP算法一样能收敛到最优策略上。\u003C\u002Fp\u003E\u003Cp\u003EDP算法对于每一个状态价值函数的估算都依赖于前一时刻各状态价值函数的数值，这种特性我们称之为\u003Cb\u003E\u003Ci\u003Ebootstrap\u003C\u002Fi\u003E\u003C\u002Fb\u003E。这个单词本意是拔靴带，一个比较好的翻译是“自举”，参看下面这张图片来更好的理解这个词的含义。同时我们注意到，在DP算法中，我们认为反映环境的所有信息 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28s%27%2C+r%7Cs%2C+a%29\" alt=\"p(s&#39;, r|s, a)\" eeimg=\"1\"\u002F\u003E 是已知的，即我们已经拥有了对于环境的建模，这种利用反映环境信息的模型来进行计算的特性我们称之为\u003Cb\u003E\u003Ci\u003Emodel-based\u003C\u002Fi\u003E\u003C\u002Fb\u003E。从某种意义上来说，DP方法本质上还并没有涉及到对于环境的“学习”过程，因为DP没有通过与环境的交互来获取关于环境的信息。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1f06d3ef3383adb4540428f322cf9a03_b.jpg\" data-size=\"normal\" data-rawwidth=\"362\" data-rawheight=\"277\" class=\"content_image\" width=\"362\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;362&#39; height=&#39;277&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"362\" data-rawheight=\"277\" class=\"content_image lazy\" width=\"362\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1f06d3ef3383adb4540428f322cf9a03_b.jpg\"\u002F\u003E\u003Cfigcaption\u003E图：Bootstrap\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E蒙特卡洛方法\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E如果说动态规划类方法主要从迭代的角度出发，通过反复把Bellman算子作用到价值函数上期望收敛到最优情形下的价值函数；那么\u003Cb\u003E\u003Ci\u003E蒙特卡洛方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（Monte Carlo methods）则是直接从最优价值函数的定义出发，通过采样来直接对于最优价值函数进行无偏估计。\u003C\u002Fp\u003E\u003Cp\u003E具体来说就是先进行采样，即随机地从一些状态出发，然后按照行动策略（behavior policy） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 采取行动，直到达到终止状态；然后进行最有价值函数的估计，即回溯地利用探索到的这部分信息来更新目标策略（target policy） \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 下的价值函数。当每一轮的行动策略与目标策略一致的时候，我们称这样的方法为on-policy的；反之，我们称之为off-policy的。\u003C\u002Fp\u003E\u003Cp\u003E我们先来学习on-policy的情形。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E4%BA%94%EF%BC%9AMonte+Carlo+Method+%28on-policy%29%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+Returns%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Bempty+list%7D+%5C%5C+4%5Cquad+%26+%5Cquad+%5Cpi%28a%7Cs%29+%5Cleftarrow+%5Ctext%7Barbitrary+%7D%5Cepsilon%5Ctext%7B-soft+policy%7D+%5C%5C+5%5Cquad+%26+%5Ctext%7Brepeat+forever%3A%7D+%5C%5C+6%5Cquad+%26+%5Cquad+%5Ctext%7Bgenerate+an+episode+using+%7D%5Cpi+%5C%5C+7%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+each+pair+%7D+s%2C+a+%5Ctext%7B+in+the+episode%3A%7D+%5C%5C+8%5Cquad+%26+%5Cquad+%5Cquad+G+%5Cleftarrow+%5Ctext%7Breturn+following+the+first+occurance+of+%7D+s%2C+a+%5C%5C+9%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bappend+%7D+G+%5Ctext%7B+to+%7D+Returns%28s%2C+a%29+%5C%5C+10%5Cquad+%26+%5Cquad+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Baverage%7D%28Returns%28s%2C+a%29%29+%5C%5C+11%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+each+%7D+s+%5Ctext%7B+in+the+episode%3A%7D+%5C%5C+12%5Cquad+%26+%5Cquad+%5Cquad+A%5E%2A+%5Cleftarrow+arg%5Cmax_a+Q%28s%2Ca%29+%5C%5C+13%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bfor+all+%7D+a%5Cin%5Cmathcal%7BA%7D%28s%29+%5C%5C+14%5Cquad+%26+%5Cquad+%5Cquad+%5Cquad+%5Cpi%28a%7Cs%29+%5Cleftarrow+%5Cbegin%7Bcases%7D+1-%5Cepsilon%2B%5Cepsilon%2F%7C%5Cmathcal%7BA%7D%28s%29%7C+%26+%5Ctext%7Bif+%7D+a+%3D+A%5E%2A+%5C%5C+%5Cepsilon%2F%7C%5Cmathcal%7BA%7D%28s%29%7C+%26+%5Ctext%7Bif+%7D+a+%5Cneq+A%5E%2A+%5Cend%7Bcases%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法五：Monte Carlo Method (on-policy)} \\\\ 1\\quad &amp; \\text{initialize } \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\\\ 2\\quad &amp; \\quad Q(s, a) \\leftarrow \\text{arbitrary}  \\\\ 3\\quad &amp; \\quad Returns(s, a) \\leftarrow \\text{empty list} \\\\ 4\\quad &amp; \\quad \\pi(a|s) \\leftarrow \\text{arbitrary }\\epsilon\\text{-soft policy} \\\\ 5\\quad &amp; \\text{repeat forever:} \\\\ 6\\quad &amp; \\quad \\text{generate an episode using }\\pi \\\\ 7\\quad &amp; \\quad \\text{for each pair } s, a \\text{ in the episode:} \\\\ 8\\quad &amp; \\quad \\quad G \\leftarrow \\text{return following the first occurance of } s, a \\\\ 9\\quad &amp; \\quad \\quad \\text{append } G \\text{ to } Returns(s, a) \\\\ 10\\quad &amp; \\quad \\quad Q(s, a) \\leftarrow \\text{average}(Returns(s, a)) \\\\ 11\\quad &amp; \\quad \\text{for each } s \\text{ in the episode:} \\\\ 12\\quad &amp; \\quad \\quad A^* \\leftarrow arg\\max_a Q(s,a) \\\\ 13\\quad &amp; \\quad \\quad \\text{for all } a\\in\\mathcal{A}(s) \\\\ 14\\quad &amp; \\quad \\quad \\quad \\pi(a|s) \\leftarrow \\begin{cases} 1-\\epsilon+\\epsilon\u002F|\\mathcal{A}(s)| &amp; \\text{if } a = A^* \\\\ \\epsilon\u002F|\\mathcal{A}(s)| &amp; \\text{if } a \\neq A^* \\end{cases} \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E对于MC方法，只有满足如下两个条件的时候才能够保证算法收敛到最优策略\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E主循环被执行无穷多次\u003C\u002Fli\u003E\u003Cli\u003E当循环次数趋向无穷的时候，每个状态被遍历的次数也趋向无穷\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E对于第一点，我们只能在资源允许的情况下尽可能多地运行主循环。对于第二点来说，我们的解决方法主要有两个思路；要么在每次主循环开头时，保证选择任意起始状态的概率都大于零，这种方法称之为exploring start；要么在算法运行过程中保证我们行动策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 具有足够的随机性，即能始终以一个大于零的几率访问任意可能的行动。特别地， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D\" alt=\"\\forall s \\in \\mathcal{S}\" eeimg=\"1\"\u002F\u003E ，如果某个策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 能够保证对于每个行动的访问概率不低于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon+%2F+%7C%5Cmathcal%7BA%7D%28s%29%7C\" alt=\"\\epsilon \u002F |\\mathcal{A}(s)|\" eeimg=\"1\"\u002F\u003E ，则称这样的策略是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -soft的。\u003C\u002Fp\u003E\u003Cp\u003E对于算法五第14行中定义的策略，我们称之为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy的，它是一种 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -soft策略。算法五中第10行是对于当前策略对应价值函数的估计，第14行是基于当前价值函数对于策略进行改进，由于探索使用的策略（第6行）和目标输出的策略（第14行）是同样的策略，因此这种方法是on-policy的。\u003C\u002Fp\u003E\u003Cp\u003E行动策略也可以和目标策略不相同，即得到下面的off-policy的蒙特卡洛算法。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7B%E7%AE%97%E6%B3%95%E5%85%AD%EF%BC%9AMonte+Carlo+Method+%28off-policy%29%7D+%5C%5C+1%5Cquad+%26+%5Ctext%7Binitialize+%7D+%5Cforall+s+%5Cin+%5Cmathcal%7BS%7D%2C+a+%5Cin+%5Cmathcal%7BA%7D%28s%29%5C%5C+2%5Cquad+%26+%5Cquad+Q%28s%2C+a%29+%5Cleftarrow+%5Ctext%7Barbitrary%7D++%5C%5C+3%5Cquad+%26+%5Cquad+C%28s%2C+a%29+%5Cleftarrow+0+%5C%5C+4%5Cquad+%26+%5Cquad+%5Cpi%28s%29+%5Cleftarrow+%5Ctext%7Ba+deterministic+policy+that+is+greedy+w.r.t.%7D+Q%5C%5C+5%5Cquad+%26+%5Ctext%7Brepeat+forever%3A%7D+%5C%5C+6%5Cquad+%26+%5Cquad+%5Ctext%7Bgenerate+an+episode+using+any+soft+policy+%7D%5Cmu+%5C%5C+7%5Cquad+%26+%5Cquad+%5Cquad+S_0%2C+A_0%2C+R_1%2C+%5Ccdots%2C+S_%7BT-1%7D%2C+A_%7BT-1%7D%2C+R_T%2C+S_T+%5C%5C+8%5Cquad+%26+%5Cquad+G+%5Cleftarrow+0+%5C%5C+9%5Cquad+%26+%5Cquad+W+%5Cleftarrow+1+%5C%5C+10%5Cquad+%26+%5Cquad+%5Ctext%7Bfor+%7D+t+%3D+T-1%2C+T-2%2C+%5Ccdots%2C++%5Ctext%7B+downto+%7D+0+%5Ctext%7B%3A%7D+%5C%5C+11%5Cquad+%26+%5Cquad+%5Cquad+G+%5Cleftarrow+%5Cgamma+G+%2B+R_%7Bt%2B1%7D+%5C%5C+12%5Cquad+%26+%5Cquad+%5Cquad+C%28S_t%2C+A_t%29+%5Cleftarrow+C%28S_t%2C+A_t%29+%2B+W+%5C%5C+13%5Cquad+%26+%5Cquad+%5Cquad+Q%28S_t%2C+A_t%29+%5Cleftarrow+Q%28S_t%2C+A_t%29+%2B+%5Cfrac%7BW%7D%7BC%28S_t%2C+A_t%29%7D+%28G+-+Q%28S_t%2C+A_t%29%29+%5C%5C+14%5Cquad+%26+%5Cquad+%5Cquad+%5Cpi%28S_t%29+%5Cleftarrow+arg%5Cmax_a+Q%28S_t%2C+a%29+%5C%5C+15%5Cquad+%26+%5Cquad+%5Cquad+%5Ctext%7Bif+%7D+A_t+%5Cneq+%5Cpi%28S_t%29+%5Ctext%7B+then+exit+forloop%7D+%5C%5C+16%5Cquad+%26+%5Cquad+%5Cquad+W+%5Cleftarrow+W+%2F+%5Cmu%28A_t%7CS_t%29+%5C%5C+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} &amp; \\text{算法六：Monte Carlo Method (off-policy)} \\\\ 1\\quad &amp; \\text{initialize } \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\\\ 2\\quad &amp; \\quad Q(s, a) \\leftarrow \\text{arbitrary}  \\\\ 3\\quad &amp; \\quad C(s, a) \\leftarrow 0 \\\\ 4\\quad &amp; \\quad \\pi(s) \\leftarrow \\text{a deterministic policy that is greedy w.r.t.} Q\\\\ 5\\quad &amp; \\text{repeat forever:} \\\\ 6\\quad &amp; \\quad \\text{generate an episode using any soft policy }\\mu \\\\ 7\\quad &amp; \\quad \\quad S_0, A_0, R_1, \\cdots, S_{T-1}, A_{T-1}, R_T, S_T \\\\ 8\\quad &amp; \\quad G \\leftarrow 0 \\\\ 9\\quad &amp; \\quad W \\leftarrow 1 \\\\ 10\\quad &amp; \\quad \\text{for } t = T-1, T-2, \\cdots,  \\text{ downto } 0 \\text{:} \\\\ 11\\quad &amp; \\quad \\quad G \\leftarrow \\gamma G + R_{t+1} \\\\ 12\\quad &amp; \\quad \\quad C(S_t, A_t) \\leftarrow C(S_t, A_t) + W \\\\ 13\\quad &amp; \\quad \\quad Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\frac{W}{C(S_t, A_t)} (G - Q(S_t, A_t)) \\\\ 14\\quad &amp; \\quad \\quad \\pi(S_t) \\leftarrow arg\\max_a Q(S_t, a) \\\\ 15\\quad &amp; \\quad \\quad \\text{if } A_t \\neq \\pi(S_t) \\text{ then exit forloop} \\\\ 16\\quad &amp; \\quad \\quad W \\leftarrow W \u002F \\mu(A_t|S_t) \\\\ \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E而算法三中，探索所使用的策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 是任意固定的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -soft策略（第6行），比如它可以是在行动集中等概率选取行动的随机策略。而目标输出的策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 是一个关于价值函数的贪心策略，因此这种方法是off-policy的。由于行动策略与目标策略不一致，因此在更新价值函数的时候我们需要把行动策略对应的收益投影到目标函数对应的收益上。直观地来说，如果行动策略选择了某个当前目标策略大概率会选择的路径，那么对应的价值函数的改变会更多一些；反之，应该尽可能少一些地改变价值函数。这里我们使用了重要性采样（importance sampling）的技术。即目标策略对应的价值函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29\" alt=\"V(s)\" eeimg=\"1\"\u002F\u003E 与行动策略得到的收益 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"\u002F\u003E 应该满足如下关系。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%28s%29+%3D+%5Cdfrac%7B%5Csum_%7Bt%7D+%5Crho_t%5E%7BT%7D+G_t%7D%7B%5Csum_%7Bt%7D+%5Crho_t%5E%7BT%7D%7D\" alt=\"V(s) = \\dfrac{\\sum_{t} \\rho_t^{T} G_t}{\\sum_{t} \\rho_t^{T}}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中重要性采样率（importance-sampling ratio）\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Crho_t%5E%7BT%7D+%3D+%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D+%5Cdfrac%7B%5Cpi%28A_k%7CS_k%29%7D%7B%5Cmu%28A_k%7CS_k%29%7D\" alt=\"\\rho_t^{T} = \\prod_{k=t}^{T-1} \\dfrac{\\pi(A_k|S_k)}{\\mu(A_k|S_k)}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T\" alt=\"T\" eeimg=\"1\"\u002F\u003E 表示终止状态的时刻。算法三中的第12、13、16行所描述的就是这样的重要性采样的增量实现。\u003C\u002Fp\u003E\u003Cp\u003E总体来说，蒙特卡洛方法能够通过与环境的交互来学习一个好的策略，它不需要对于环境的建模，即它是\u003Cb\u003E\u003Ci\u003Emodel-free\u003C\u002Fi\u003E\u003C\u002Fb\u003E的。不同于之前的动态规划算法，每一步中价值函数的更新并不依赖于之前的价值函数，而是直接更新到采样得到收益的平均值上，因此它不是bootstrap的。由于蒙特卡洛方法要求一直采样到最末尾，因此它只能用于回合制的任务。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E总结\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E这一讲我们学习了强化学习的一些基本概念，包括\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E强化学习的基础模型是马可夫决策模型\u003C\u002Fli\u003E\u003Cli\u003E强化学习的目标是找到最大化收益的策略\u003C\u002Fli\u003E\u003Cli\u003E找寻策略的一个重要途径是找到该马可夫决策模型上的价值函数\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli\u003E直接使用采样的方法估计这个被定义出来的价值函数形成了蒙特卡洛方法\u003C\u002Fli\u003E\u003Cli\u003E使用Bellman算子迭代计算这个价值函数形成了动态规划方法\u003C\u002Fli\u003E\u003Cli\u003E在此两者的基础上还会发展出我们后面会讲到的\u003Cb\u003E\u003Ci\u003E时间差分方法\u003C\u002Fi\u003E\u003C\u002Fb\u003E（temporal-difference，TD）\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Cp\u003E强化学习的不同算法有不同的维度，我们可以总结出来下面这张表，在之后的学习之中，我们可以加入更多的行（算法）和列（维度）\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57a4d448731936d8a189f9c75be21e23_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"580\" data-rawheight=\"83\" class=\"origin_image zh-lightbox-thumb\" width=\"580\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57a4d448731936d8a189f9c75be21e23_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;580&#39; height=&#39;83&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"580\" data-rawheight=\"83\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"580\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57a4d448731936d8a189f9c75be21e23_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-57a4d448731936d8a189f9c75be21e23_b.png\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E参考资料\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Col\u003E\u003Cli\u003ESutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. Vol. 1. No. 1. Cambridge: MIT press, 1998. 一本经典的强化学习入门教材\u003C\u002Fli\u003E\u003Cli\u003EPuterman, Martin L. Markov decision processes: discrete stochastic dynamic programming. John Wiley &amp; Sons, 2014. 一本讲马可夫决策过程的书，涉及更为坚实的数学基础\u003C\u002Fli\u003E\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Frail.eecs.berkeley.edu\u002Fdeeprlcourse\u002Findex.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBerkeley CS294\u003C\u002Fa\u003E 课件可以在 \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002Fsweta20\u002Fdeep-rl-course\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E这里\u003C\u002Fa\u003E 找到\u003C\u002Fli\u003E\u003Cli\u003E我导师开设的 \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fiiis.tsinghua.edu.cn\u002F~jianli\u002Fcourses\u002FATCS2018spring\u002FATCS-2018s.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E高等理论计算机课程\u003C\u002Fa\u003E 的最后两讲\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","type":"topic","id":"20039099","name":"强化学习 (Reinforcement Learning)"}],"voteupCount":50,"voting":0,"column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"},"commentCount":10,"contributions":[{"id":20207533,"state":"accepted","type":"first_publish","column":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"【强化学习入门 1】从零开始认识强化学习 - 来自知乎专栏「强化学习前沿」，作者: 张楚珩 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56045177 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false}},"columns":{"reinforcementlearning":{"description":"读呀读paper","canManage":false,"intro":"我们的目标是通用人工智能！","isFollowing":false,"urlToken":"reinforcementlearning","id":"reinforcementlearning","articlesCount":137,"acceptSubmission":true,"title":"强化学习前沿","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Freinforcementlearning","commentPermission":"all","created":1537150763,"updated":1541142463,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5d1f04a84f6be8896c07a32a05e1d6d1_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_{size}.jpg","uid":"35077274730496","userType":"people","isFollowing":false,"urlToken":"zhang-chu-heng","id":"db39e3e0528520071b0a6e5f6240cfea","description":"","name":"张楚珩","isAdvertiser":false,"headline":"强化学习 量化投资","gender":1,"url":"\u002Fpeople\u002Fdb39e3e0528520071b0a6e5f6240cfea","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b7c11f27a7fd03282819889435815aa_l.jpg","isOrg":false,"type":"people"},"followers":4344,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-gw_mweb_rec-1","expPrefix":"gw_mweb_rec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_hdpimg-2","expPrefix":"qa_hdpimg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_bullet_second-2","expPrefix":"vd_bullet_second","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_topicfeed-2","expPrefix":"se_topicfeed","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"li_education_box","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zr_training_boost","type":"String","value":"false","chainId":"_all_"},{"id":"tp_club_android_feed","type":"String","value":"old","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"soc_authormore","type":"String","value":"2","chainId":"_all_"},{"id":"tp_score_1","type":"String","value":"a","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"zr_update_merge_size","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"1","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"0","chainId":"_all_"},{"id":"se_pek_test","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"se_sug_term","type":"String","value":"0","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_presearch_ab","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"gue_self_censoring","type":"String","value":"1"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adsort","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"gue_bullet_second","type":"String","value":"1"},{"id":"web_mweb_launch","type":"String","value":"0"},{"id":"web_hdpimg","type":"String","value":"1"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosintimacy","type":"String","value":"2","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"se_hotsearch_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover_copy","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zvideo_link","type":"String","value":"1"},{"id":"se_article_icon","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"li_yxzl_new_style_a","type":"String","value":"1","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_voted","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"se_highlight","type":"String","value":"0","chainId":"_all_"},{"id":"tp_topic_rec","type":"String","value":"1","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"qap_article_like","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_across","type":"String","value":"1","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"1"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"zr_slot_training","type":"String","value":"1","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_content0","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pk","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"2","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"soc_userrec","type":"String","value":"0","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iossort","type":"String","value":"0","chainId":"_all_"},{"id":"li_catalog_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tp_discover","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_edu","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch_num","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst4","type":"String","value":"3","chainId":"_all_"},{"id":"soc_wonderuser_recom","type":"String","value":"2","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_videobillboard","type":"String","value":"1","chainId":"_all_"},{"id":"se_ctx_rerank","type":"String","value":"1","chainId":"_all_"},{"id":"se_searchwiki","type":"String","value":"0","chainId":"_all_"},{"id":"se_specialbutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"web_mweb_rec_length","type":"String","value":"1"},{"id":"li_answers_link","type":"String","value":"0","chainId":"_all_"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_"},{"id":"soc_newfeed","type":"String","value":"2","chainId":"_all_"},{"id":"web_creator_route","type":"String","value":"1"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_cate_l3","type":"String","value":"0","chainId":"_all_"},{"id":"soc_adreadline","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"li_svip_tab_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_collect","type":"String","value":"0"},{"id":"soc_authormore2","type":"String","value":"2","chainId":"_all_"},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_"},{"id":"se_rf_w","type":"String","value":"0","chainId":"_all_"},{"id":"se_relation_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"2","chainId":"_all_"},{"id":"tp_club_discover","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_autoplay","type":"String","value":"0"},{"id":"li_answer_card","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_search_paid","type":"String","value":"0","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_relationship","type":"String","value":"1","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"zr_search_sim","type":"String","value":"0","chainId":"_all_"},{"id":"se_click_club","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"tp_club_android_join","type":"String","value":"1","chainId":"_all_"},{"id":"soc_adpinweight","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"0","chainId":"_all_"},{"id":"se_prf","type":"String","value":"0","chainId":"_all_"},{"id":"se_sug_entrance","type":"String","value":"1","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"soc_iosreadline","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"6","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_discovery_ab_1","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"web_sec672","type":"String","value":"0"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_read","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_gen_search","type":"String","value":"0","chainId":"_all_"},{"id":"li_answer_label","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iosreadfilter","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_merger","type":"String","value":"1","chainId":"_all_"},{"id":"se_pek_test2","type":"String","value":"1","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"soc_feed_intimacy","type":"String","value":"2","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"se_timebox_up","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"web_mini_review","type":"String","value":"0"},{"id":"li_svip_cardshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_p","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"tp_club_feed","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_tab_feed","type":"String","value":"0","chainId":"_all_"},{"id":"soc_iospinweight","type":"String","value":"0","chainId":"_all_"},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"soc_cardheight","type":"String","value":"2","chainId":"_all_"},{"id":"gue_bullet_guide","type":"String","value":"发个弹幕聊聊…"},{"id":"li_assessment_show","type":"String","value":"1","chainId":"_all_"},{"id":"li_sku_bottom_bar_re","type":"String","value":"0","chainId":"_all_"},{"id":"web_ask","type":"String","value":"0"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_ltr_video","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_entry","type":"String","value":"0","chainId":"_all_"},{"id":"zr_training_first","type":"String","value":"false","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"1","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_dnn_mt_v2","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_pic","type":"String","value":"0.6","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"1","chainId":"_all_"},{"id":"web_audit_01","type":"String","value":"case1"},{"id":"se_related_index","type":"String","value":"3","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"web_upload","type":"String","value":"1"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebok_chap","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"tp_topic_tab","type":"String","value":"0","chainId":"_all_"},{"id":"soc_brdcst3","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_label","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_pic_swiper","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"ls_recommend_test","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_pek_test3","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_join","type":"String","value":"0","chainId":"_all_"},{"id":"gue_video_replay","type":"String","value":"0"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"tp_topic_tab_new","type":"String","value":"0-0-0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"1","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"web_unfriendly_comm","type":"String","value":"0"},{"id":"li_salt_hot","type":"String","value":"1","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_cbert_index","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F80.0.3987.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F56045177","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F56045177","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"德州","countryName":"中国","regionName":"山东","countryCode":"CN"},"logged":false},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"reinforcementlearning"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="https://static.zhihu.com/heifetz/vendor.7b36fae46082fd30a0db.js"></script><script src="https://static.zhihu.com/heifetz/column.app.76e02b16e87eec249f44.js"></script></body><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script src="https://zz.bdstatic.com/linksubmit/push.js" async=""></script></html>